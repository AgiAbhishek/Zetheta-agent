[
  {
    "text": "0\n\n\n1\nSPECIAL EDITION \nFOR ZETHETA ALGO\nRITHMS PRIVATE LIMITED \nMarch, 2024 \n\n2\nTABLE OF CONTENTS\n \n1 Preface 3 \n2 About the Authors 6 \n3 Copyright 8 \n4 Disclaimer 8 \n5 Chapter 0: Basic concept s before getting started 9 \n6 Chapter 1: Getting started 21 \n7 Chapter 2: Time Series Graphics 42 \n8 Chapter 3: Judgemental Forecasts 71 \n9 Chapter 4: Time Ser ies Features 114 \n10 Chapter 5: The Forecaster’s Toolbox 140 \n11 Chapter 6: Time Series Decomposition 206 \n12 Chapter 7: Exponent ial smoothing 246 \n13 Chapter 8: ARIMA Models 296 \n14 Chapter 9: Dynamic Regression Models 365 \n15 Chapter 10: Forecasting hier archical and grouped time series 3 9 3  \n16 Chapter 11: Advanced fo recasting methods 426 \n17 Chapter 12: Some practica l forecasting issues 459 \n18 Appendix – Using R 489 \n19 BIBLIOGRAPHY 496",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 1,
      "total_chunks": 873
    }
  },
  {
    "text": "1\nPreface \nThe methods of forecasting have undergone a big change in the r ecent past with \ndata becoming bigger and variable s becoming complicated and mul tiple. This has \nnecessitated a move in platforms u sed hitherto like Microsoft E xcel to programmes \nlike Python and R. In this book we integrate simple regression forecasting on \nMicrosoft Excel and then assist in learning how this could be d one in R language \nwith different methods. If you do not have any programming experience then you can \nlearn forecasting without doing any programming or you may like  to take the first \nstep in learning R language which has become indispensable for complicated \nstatistical problems the world tries to find solutions to. \nForecasting is both a science and an art. It is the process of making predictions \nabout future events based on past and present data with paramet ers that can \ninfluence in the future, aiming to reduce uncertainty and assis t decision-making.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 2,
      "total_chunks": 873
    }
  },
  {
    "text": "about future events based on past and present data with paramet ers that can \ninfluence in the future, aiming to reduce uncertainty and assis t decision-making. \nFrom weather forecasts to artificial intelligence based financi al projections, from \nsales predictions to demographic trends, forecasting permeates nearly every aspect \nof our lives and endeavours. \nIn this book, we delve into the fascinating world of forecastin g, exploring its \nprinciples, methods, and applications across various domains. W hether you are a \nseasoned professional seeking to  refine your forecasting skills  or a newcomer \ncurious about the mechanics behind predicting the future, this book is designed to \nserve as a comprehensive guide. \nOur journey begins by laying the groundwork, examining the fundamental concepts \nof forecasting and the underlying principles that govern it. We then progress to \nexplore a diverse array of forecasting techniques, ranging from  classical time series",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 3,
      "total_chunks": 873
    }
  },
  {
    "text": "of forecasting and the underlying principles that govern it. We then progress to \nexplore a diverse array of forecasting techniques, ranging from  classical time series \nanalysis to advanced machine learning algorithms. Through clear  explanations, \nillustrative examples, and practi cal insights, we aim to equip readers with the \nknowledge and tools necessary to tackle real-world forecasting challenges with \nconfidence. \nHowever, forecasting is not merely about crunching numbers or a pplying algorithms. \nIt also requires a deep understanding of the context, domain ex pertise, and human \njudgment. Throughout this book, we emphasize the importance of combining \nquantitative analysis with qualitat ive insights, embracing unce rtainty, and \ncontinuously refining our models in light of new information. \nMoreover, as the world becomes increasingly complex and inter-c onnected, the \nchallenges of forecasting grow more intricate. Rapid technologi cal advancements,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 4,
      "total_chunks": 873
    }
  },
  {
    "text": "Moreover, as the world becomes increasingly complex and inter-c onnected, the \nchallenges of forecasting grow more intricate. Rapid technologi cal advancements, \neconomic volatility, geopolitic al shifts, and societal changes all contribute to the \ndynamic landscape in which foreca sts are made. Thus, we must ad apt our \napproaches and strategies accordi ngly, embracing agility and in novation in our \nforecasting endeavours.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 5,
      "total_chunks": 873
    }
  },
  {
    "text": "2\n \nUltimately, the goal of this book is not merely to impart knowl edge, but to cultivate a \nmind-set that embraces uncertain ty as an opportunity, that harn esses the power of \ndata and technology while respecting the nuances of human judgm ent, and that \nrecognises the profound impact forecasting can have on shaping our future. \nAt the end of each chapter we provide a list of “further readin g”. In general, these \nlists comprise suggested textbooks that provide a more advanced  or detailed \ntreatment of the subject. Where there is no suitable textbook, we suggest journal \narticles that provide more information. \nWe use R throughout the book and we intend students to learn ho w to forecast with \nR. R is free and available on almost every operating system. It is a wonderful tool for \nall statistical analysis, not just for forecasting. See the Usi ng R appendix for \ninstructions on installing and using R. \nAll R examples in the book assume you have loaded the fpp3 package first:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 6,
      "total_chunks": 873
    }
  },
  {
    "text": "all statistical analysis, not just for forecasting. See the Usi ng R appendix for \ninstructions on installing and using R. \nAll R examples in the book assume you have loaded the fpp3 package first: \nlibrary(fpp3) \n#> ── Attaching packages ──────────────────────────────── fpp3 0.5 ──\n#> ✔ tibble 3.2.1 ✔ tsibble 1.1.4 \n#> ✔ dplyr 1.1.4 ✔ tsibbledata 0.4.1 \n#> ✔ tidyr 1.3.1 ✔ feasts 0.3.2 \n#> ✔ lubridate 1.9.3 ✔ fable 0.3.4 \n#> ✔ ggplot2 3.5.0 ✔ fabletools 0.4.1 \n#> ── Conflicts ──────────────────────────────────── fpp3_conflicts ──\n#> ✖ lubridate::date() masks base::date()  \n#> ✖ dplyr::filter() masks stats::filter()  \n#> ✖ tsibble::intersect() masks base::intersect()  \n#> ✖ tsibble::interval() masks lubridate::interval()  \n#> ✖ dplyr::lag() masks stats::lag() \n#> ✖ tsibble::setdiff() masks base::setdiff()  \n#> ✖ tsibble::union() masks base::union()  \nThis will \nload the relevant dat a sets, and attach several packa ges as listed above.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 7,
      "total_chunks": 873
    }
  },
  {
    "text": "#> ✖ tsibble::setdiff() masks base::setdiff()  \n#> ✖ tsibble::union() masks base::union()  \nThis will \nload the relevant dat a sets, and attach several packa ges as listed above. \nThese include several tidyverse packages, and packages to handle time series and \nforecasting in a “tidy” framework. The above output also shows the package versions",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 8,
      "total_chunks": 873
    }
  },
  {
    "text": "3\nwe have used in compiling this edition of the book. Some exampl es in the book will \nnot work with earlier versions of the packages. Finally, the ou tput lists some conflicts \nshowing which function will be preferenced when a function of t he same name is in \nmultiple packages. \nThe book is written for:  \n(1) people doing forecasting without formal training in the area; \n(2) undergraduate students studying business or engineering;  \n(3) MBA students doing a forecasting elective.  \nFor most sections, we only assume that readers are familiar wit h introductory \nstatistics, and with high-schoo l algebra. There are a couple of  sections that also \nrequire knowledge of matrices, but these are also learnt by most in high schools \ntoday. \nAs we embark on this exploration of forecasting, we invite you to approach it with \ncuriosity, humility, and a w illingness to engage with both the successes and \nlimitations of our predictive c apabilities. May this book serve as a beacon, guiding",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 9,
      "total_chunks": 873
    }
  },
  {
    "text": "curiosity, humility, and a w illingness to engage with both the successes and \nlimitations of our predictive c apabilities. May this book serve as a beacon, guiding \nyou through the intricate terrain  of forecasting and empowering  you to navigate the \nuncertain seas of tomorrow with clarity and insight.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 10,
      "total_chunks": 873
    }
  },
  {
    "text": "4\nAbout the Authors \nDr. Mayur Doshi \nDr. Mayur Doshi is a veteran in research & development industry including Artificial \nIntelligence with 35 years of experience. A mathematician by pr actice and Phd in \nOrganic Chemistry by qualification.  \nHe is a director on the board of directors of Falkonry Software  Private Limited, India \nwhose parent Company is in US (www.falkonry.com). Falkonry has created Time \nSeries AI - a GPU-scale breakth rough on real-tim e operational d ata (patented). By \nanalyzing terabytes of machine and sensor data, Falkonry AI app lications identify \ndeveloping faults earlier and better than would ever be possible with manual \nsystems. Today, Falkonry's time series AI solutions are used by companies – both \nlarge multinationals and regional manufacturers – to power thei r digital \ntransformation and achieve significant improvements in producti on uptime, quality, \nyield, and safety. Falkonry can discover insights hidden in operational data and",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 11,
      "total_chunks": 873
    }
  },
  {
    "text": "transformation and achieve significant improvements in producti on uptime, quality, \nyield, and safety. Falkonry can discover insights hidden in operational data and \ndeliver timely, actionable inte lligence. They empower our users –  p l a n t  p e r s o n n e l ,  \nprocess or maintenance engineers, line operators, and analysts – to make better \noperational decisions with evidence-based approaches. \nIn the bustling world of finance, where every tick of the clock carries the weight of \nfortunes and the risk of losses, a new voice has emerged—a firs t-time author whose \nground breaking book is poised to  assist the financial markets in redefining \nthemselves. Meet Dr. Mayur Doshi, a visionary mind and the auth or of the highly \nanticipated book, \"Application of  Artificial Intelligence in Fi nance\". This book on \nForecasting is essential prequel to the book on \"Application of Artificial Intelligence in \nFinance\".",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 12,
      "total_chunks": 873
    }
  },
  {
    "text": "anticipated book, \"Application of  Artificial Intelligence in Fi nance\". This book on \nForecasting is essential prequel to the book on \"Application of Artificial Intelligence in \nFinance\". \nDr. Mayur, a seasoned expert in artificial intelligence, brings  a fresh perspective to \nthe intersection of technology and finance. His journey into th e realm of AI applied to \nfinance began with a deep-seated curiosity about the untapped p otential of cutting-\nedge technologies and mathematical curiosity.  \nSuketu Sanghvi has a diploma in Computer Programming and is a rank holder \nChartered Accountant and also a  Company Secretary (ranked # 2nd  on all India \nbasis) and secured the first rank in computer systems in his ba chelor degree from \nthe Mumbai University. He has a work experience of 28 years and  started his career \nin investment banking in India and thereafter, moved to financi al centres of \nSingapore, Hong Kong and Dubai in the United Arab Emirates.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 13,
      "total_chunks": 873
    }
  },
  {
    "text": "in investment banking in India and thereafter, moved to financi al centres of \nSingapore, Hong Kong and Dubai in the United Arab Emirates. \nSuketu is an illustrious investment banker who has worked on de als and desks \nwhich have won the most prestigious awards globally such as Euromoney awards. In \nhis last employment he was a hedge fund manager at Essdar Capit al group in the \nUAE, managing money and providing  investment advisory. Couple o f his \ntransactions in middle east won the best Euromoney award deals globally. Where \nafter, he moved to India in 2014 to assist a large number of te ch start-ups in India on",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 14,
      "total_chunks": 873
    }
  },
  {
    "text": "5\na pro-bono basis. Suketu is a co-founder of multiple tech start -ups and other \nventures including Zetheta Algorithms Private Limited, Finarcad ia Holdings Private \nLimited, Nashvan Horticulture Industries Private Limited and Essdar Capital (UAE). \nHe worked for ABN AMRO Bank, India, Singapore & Hong Kong from Jan 2000 to \nMarch 2006. His primary assignment was to originate, structure,  underwrite and \ninvest in structured debt includi ng special situation, high yie ld type fixed income \ntransactions (for underwriting an d for proprietary investments in ABN AMRO Bank’s \nhold to maturity Asian ABCP co nduit book) with embedded derivat ives. Part of the \nmost successful DCM teams in Asia (including no # 1 in India) a nd received \nEuromoney awards for multiple years, Business India deal award in 2002 and \nFinance Asia deal award.  \nPrior to that he worked for UTI Bank (now Axis Bank) from 1997 to Dec 1999. He",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 15,
      "total_chunks": 873
    }
  },
  {
    "text": "Euromoney awards for multiple years, Business India deal award in 2002 and \nFinance Asia deal award.  \nPrior to that he worked for UTI Bank (now Axis Bank) from 1997 to Dec 1999. He \nwas part of the team that set-up the investment banking platform for the bank (which \ngrew from around 3 employees when joined to more than 1,000 emp loyees today \nand laid the foundation to take it  to the number # 1 investment  bank position in \nIndia). While being primarily responsible as credit underwriter and trader, he was \nalso involved in idea generation and setting up of new business es for the Bank. Prior \nto UTI Bank, he started his career in Mergers and Acquisitions,  Takeovers, Equity \nIPOs and Venture Capital Investments with marquee investment banking firms.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 16,
      "total_chunks": 873
    }
  },
  {
    "text": "6\nCopyright \nNo part of this book may be c opied, recorded, reproduced or dis tributed in any form \nor by any means, electronic, mechanical, photocopying, recordin g, or otherwise or \nstored in a database or retrieval system without the prior writ ten permission of the \nauthor.  \nThe program listing (if any) and cover page may be entered, sto red and executed in \na computer system, but they may not be reproduced for publication. \nThis book is published from India and any dispute, claim (inclu ding non-contractual \nclaims) arising out of or in connection with it or its contents shall be governed by, and \nconstrued in accordance with the laws of India and the courts i n India will have \nexclusive jurisdiction to settle any dispute or claim (includin g non-contractual claims) \narising out of or in connection with this book or any content in the book. \nDisclaimer \nNeither ZeTheta nor the authors guarantees the accuracy or comp leteness of any",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 17,
      "total_chunks": 873
    }
  },
  {
    "text": "arising out of or in connection with this book or any content in the book. \nDisclaimer \nNeither ZeTheta nor the authors guarantees the accuracy or comp leteness of any \ninformation published herein, and neither ZeTheta nor the autho rs shall be \nresponsible for any errors, omissions, or damages arising out of use of this \ninformation. This book is made available with the understanding  that ZeTheta and its \nauthors are providing information  to arouse intellectual curios ity/ research on the \nsubject matter but are not atte mpting to render any professiona l services, or give \nrecommendations, advice or opinions. If such services are requi red, the assistance \nof an appropriate professional should be sought. \nZeTheta is a technology platform and does not act or provide an y services of a \npublisher, broker, sub-broker, investment adviser, financial pr oduct distributor, \ninsurance agent, research analyst, portfolio manager, wealth ma nager, banker,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 18,
      "total_chunks": 873
    }
  },
  {
    "text": "publisher, broker, sub-broker, investment adviser, financial pr oduct distributor, \ninsurance agent, research analyst, portfolio manager, wealth ma nager, banker, \nfinancial adviser in any capacity which is regulated. ZeTheta a re not regulated by \nany Board/ Authority and do not ca rry out any business which re quires licensing/ \nregistration from any Board/ Authority. ZeTheta and the authors  are not financial \ninfluencer and do not make any recommendation to buy or sell an y securities or \nprovide any research on any listed security. \nNo representation or guarantee i s being made in this book as to t h e  f u t u r e  \nperformance/ outcome or forecasts. Any information, and in particular any forecast, \noutcome or opinion, contained in this book is not intended to p redict actual \nperformance, which will differ, and may differ substantially, f rom those illustrated in \nthe information in this book. In evaluating any illustrative pe rformance, outcome or",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 19,
      "total_chunks": 873
    }
  },
  {
    "text": "performance, which will differ, and may differ substantially, f rom those illustrated in \nthe information in this book. In evaluating any illustrative pe rformance, outcome or \nforecast contained herein or any other information provided in this book, you should \nunderstand that not all of the assumptions used herein are desc ribed in this book. \nConditions and events that are not accounted for or discussed i n this book may also \nhave a significant effect on the outcome. Projections are subject to much uncertainty \nbecause many of the events that shape the markets as well as fu ture developments \nin technologies, demographics and resources cannot be foreseen.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 20,
      "total_chunks": 873
    }
  },
  {
    "text": "7 \nNothing in this book should be construed as investment, legal, tax or financial \nadvice; nor any advice to purchas e any security, commodities, f utures or options. \nYou should carry out your own due diligence and place no reliance on ZeTheta or \nthe authors. ZeTheta and the authors do not, nor any person con nected with it, \naccept any liability arising from the use of anything in this b ook. The reader of this \nbook should rely on their own investigations and take their own professional advice.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 21,
      "total_chunks": 873
    }
  },
  {
    "text": "8\nChapter 0 Basic concepts before getting started \nHere we discuss general non-statistical and few simple statistical aspects of \nforecasting future cash flows of a business which can be done easily by everyone \nusing simply Microsoft Excel. \nThere are multiple types of fo recasting methods that financial analysts use to predict \nfuture revenues, expenses, and c apital costs for a business. Wh ile there is a wide \nrange of frequently used quantitative budget forecasting tools, the four main methods \nare:  (1) straight-line, (2) m oving average, (3) simple linear regression and (4) \nmultiple linear regressions. \nTechnique Use Data needed \nStraight line Constant growth \nrate \nHistorical data \nMoving \naverage \nRepeated \nforecasts \nHistorical data \nSimple linear \nregression \nCompare one \nindependent with \none dependent \nvariable \nA sample of \nrelevant \nobservations \nMultiple linear \nregression \nCompare more \nthan one \nindependent \nvariable with one \ndependent \nvariable \nA sample of",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 22,
      "total_chunks": 873
    }
  },
  {
    "text": "independent with \none dependent \nvariable \nA sample of \nrelevant \nobservations \nMultiple linear \nregression \nCompare more \nthan one \nindependent \nvariable with one \ndependent \nvariable \nA sample of \nrelevant \nobservations \nTop down and Bottom Up fore\ncasting \nThe expanding globalisation of business, the continuing move fr om push to pull \nmanufacturing, and the rise in consumer oriented economies and services industry \nwith disruptive tech start-ups, have led to a much more complex  forecasting and \nplanning world. Forecasters and planners are being asked to cre ate plans for \nexpanding geographies, increased numbers of sales channels, and broader, more \ndiverse, and shorter life cycle product lines. This complexity means that markets are \nmore dynamic and quantitatively based statistical forecast meth ods are becoming \nless effective in capturing al l that is happening in today’s ra pidly changing business \nenvironment.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 23,
      "total_chunks": 873
    }
  },
  {
    "text": "9\nMore market intelligence now needs to be incorporated during the development of \nforecasts. In this regard, each Sales and Operations Planning ( S&OP) team member \nmay have to generate, review, and revise demand forecasts that reflect the aspects \nof a business with which they are most familiar. This requires leveraging Top-Down \n& Bottom-Up forecasting in the process and also usage of other tools such as \nregression analysis. \nBottom-up forecasting is a metho d of estimating a company’s fut ure performance by \nstarting with low level company data and working “up” to revenu e. The opposite \napproach to bottom-up forecasting is called top-down forecastin g, which begins with \nbroad assumptions like Total Addressable Market (TAM) and assum ing the \ncompany’s market share to work “down” to revenue. \nTop-down forecasting is extremely useful for improving the accu racy of detailed \nforecasts. Aggregated demand is le ss volatile than its individual components, so on",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 24,
      "total_chunks": 873
    }
  },
  {
    "text": "Top-down forecasting is extremely useful for improving the accu racy of detailed \nforecasts. Aggregated demand is le ss volatile than its individual components, so on \na relative basis a forecast of the aggregate is more accurate t han the forecasts of its \nindividual components. This is  due to the phenomenon of compens ating errors \nwhere random errors and variations tend to cancel each other ou t. This is the \nprinciple behind the concept of Top-Down forecasting where, rather than forecasting \neach component separately, it is better to first forecast the a ggregated group and \nthen disaggregate the resulting forecast to derive the forecast s of the individual \ncomponents. The good news is that this principle can be leverag ed for any type of \naggregation, such as aggregations across companies in an indust ry, products, sales \nchannels (e.g., stores), geographies, and even time itself.  \nThe use of Bottom-Up forecasting is better for situations where  the individual",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 25,
      "total_chunks": 873
    }
  },
  {
    "text": "channels (e.g., stores), geographies, and even time itself.  \nThe use of Bottom-Up forecasting is better for situations where  the individual \ncomponents have different patterns of variation. Under the conc ept of Bottom-Up \nforecasting, one forecasts the individual components separately  and then adds the \nforecasts up to get the forecast for the aggregated group. \nGenerally, Top-Down or Bottom-Up used on an exclusive basis is not the best way to \nforecast. Often the aggregate group’s Bottom-Up forecast can be  improved by \nreplacing it with a Top-Down for ecast. The individual Bottom-Up  component \nforecasts can be then improved by adjusting each using correcti on factors derived \nfrom looking at the aggregated gr oup’s Bottom-Up versus its Top -Down forecast. \n(For example, if the Bottom-Up forecast predicts aggregate sales to remain flat, while \nthe Top-Down forecast predicts it to grow by 10%, then the corr ection factor to apply",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 26,
      "total_chunks": 873
    }
  },
  {
    "text": "(For example, if the Bottom-Up forecast predicts aggregate sales to remain flat, while \nthe Top-Down forecast predicts it to grow by 10%, then the corr ection factor to apply \nto the bottom level forecasts would be 1.1). Thus, TopDown in c onjunction with \nBottom-Up, and even Middle-Out is recommended. \nThere are two ways in which TopDown & Bottom-Up forecasting is useful. Cross-\nfunction teams comprised of members from the supply chain, oper ations, marketing, \nsales, and finance organisations meet to discuss their plans fo r generating and \nsatisfying customer demand. The process is driven by a baseline demand forecast \nthat reflects the demand expected from the marketing and sales plans, which in turn",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 27,
      "total_chunks": 873
    }
  },
  {
    "text": "10\ndrives the supply plans reflecting the future activities of the  operations, \nmanufacturing, logistics, and pr ocurement organisations. Thus, the first (obvious) \nway in which Top-Down & Bottom-Up forecasting is useful in the S&OP process is \nduring the development of the baseline forecast, in order to ta ke advantage of the \naccuracy that can be achieved from using both types in conjunct ion with each other. \nFor example, brand-level foreca sts may be most accurately gener ated at the brand \nlevel, and SKU-level forecasts might best be derived from disag gregating the brand-\nlevel forecasts using Top-Down forecasting. In turn, product group forecasts might \nbest be derived by aggregating the brand-level forecasts using Bottom-Up \nforecasting. \nThe S&OP process also involves refining the supply and demand p lans, as well as \nthe baseline-demand forecast. The resulting consensus-based sup ply and demand",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 28,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasting. \nThe S&OP process also involves refining the supply and demand p lans, as well as \nthe baseline-demand forecast. The resulting consensus-based sup ply and demand \nplans developed during the proc ess require accountability and c ommitment from \neach of the stakeholder organizations involved to ensure each w ill execute as close \nas possible to what is embodied in the plans. In order to get t his type of buy-in and \nincrease forecast accuracy, each organization needs to particip ate in the \ndevelopment of the forecasts in terms of reviewing and revising them as necessary. \nThis is best accomplished by translating and representing the d emand forecasts in a \nform in which each organization is used to dealing with. If mar keting’s approach to \nplanning, for example, focuses on revenues generated by product  groups and \nbrands rather than by unit-based Stock-Keeping Units (SKUs), th en any unit-based",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 29,
      "total_chunks": 873
    }
  },
  {
    "text": "planning, for example, focuses on revenues generated by product  groups and \nbrands rather than by unit-based Stock-Keeping Units (SKUs), th en any unit-based \nSKU demand forecasts needs to be aggregated to these product levels on a dollar \nbasis before Marketing could effectively review and revise the forecasts. Meanwhile, \nif Sales is most familiar with dealing with sales (in INR) by c ustomer accounts and/or \nsales districts and channels, then demand forecasts needs to be  aggregated, \ndisaggregated, and translated in to these account groupings befo r e  S a l e s  c a n  \nusefully play its role in the S&OP process. Similarly, Supply Chain managers are \nmost comfortable dealing with forecasts that reflect unit-based  SKU and case-level \ndemand, for example; while Finance relates best to forecasts th at are aggregated \ninto budgetary units in terms of revenues, costs, and margins.  \nThus to get the requisite accountability and commitment from al l the organizations",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 30,
      "total_chunks": 873
    }
  },
  {
    "text": "into budgetary units in terms of revenues, costs, and margins.  \nThus to get the requisite accountability and commitment from al l the organizations \ninvolved in the S&OP process requi res that forecasts be aggrega ted and \ndisaggregated (and possibly translated) to various levels to be  reviewed and revised \nby each one, in terms they best understand. This represents ano ther way in which \nTop-Down & Bottom Up forecasting is useful to the S&OP process.  For example, if \nan organization revises a dema nd forecast at an aggregated leve l, then the revision \nneeds to percolate up and down, using Top-Down, BottomUp, and M iddle-Out \nforecasting methods.  \nThere are several other forecast methods, in addition to top-do wn and bottom-up \nforecasting, such as regression analysis and Year-over-Year (Yo Y) analysis. In \nregression analysis, a financial analyst calculates how changes  in independent",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 31,
      "total_chunks": 873
    }
  },
  {
    "text": "11 \nvariables impact the dependent var iable (revenue). Year-over-Ye ar analysis is the \nsimplest method of forecasting where an analyst will look at hi storical growth rates \nand apply a growth rate percentage to historical revenue. \nIn regression, the premise is that changes in the value of a ma in variable (for \nexample, the sales of Product A) are closely associated with changes in some other \nvariable(s) (for example, the cost of Product B). So, if future  values of these other \nvariables (cost of Product B) can be estimated, it can be used to forecast the main \nvariable (sales of Product A). \nRegression analysis is a statistical technique for quantifying the relationship between \nvariables. In simple regression analysis, there is one dependen t variable (e.g. sales) \nto be forecast and one independent variable. The values of the independent variable \nare typically those assumed to \"cause\" or determine the values of the dependent",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 32,
      "total_chunks": 873
    }
  },
  {
    "text": "to be forecast and one independent variable. The values of the independent variable \nare typically those assumed to \"cause\" or determine the values of the dependent \nvariable. Thus, if we assume that the amount of advertising dollars spent on a \nproduct determines the amount of it s sales, we could use regres sion analysis to \nquantify the precise nature of the relationship between advertising and sales. For \nforecasting purposes, knowing the quantified relationship betwe en the variables \nallows us to provide forecasting estimates. \nThe simplest regression analysis models the relationship betwee n two variables \nusing the following equation: Y = a + bX, where Y is the dependent variable and X is \nthe independent variable. Notice that this simple equation deno tes a \"linear\" \nrelationship between X and Y. So this form would be appropriate if, when you plotted \na graph of Y and X, you tended to s ee the points roughly form along a straight line",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 33,
      "total_chunks": 873
    }
  },
  {
    "text": "relationship between X and Y. So this form would be appropriate if, when you plotted \na graph of Y and X, you tended to s ee the points roughly form along a straight line \n(as compared to having a curvilinear relationship). \nWhen you have several past concurrent observations of Y and X, regression \nanalysis provides a means to calculate the values of a and b, w hich are assumed to \nbe constant. Since you will then know a and b, if you can provi de an estimate of X in \nsome future period, you can calculate a future value of Y from the above equation. \nWhen using regression models for time series data, we need to d istinguish between \nthe different types of forecasts that can be produced, depending on what is assumed \nto be known when the forecasts are computed. \nEx-ante forecasts are those that are made using only the information that is available \nin advance. For example, ex-ant e forecasts for the percentage c hange in Indian",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 34,
      "total_chunks": 873
    }
  },
  {
    "text": "Ex-ante forecasts are those that are made using only the information that is available \nin advance. For example, ex-ant e forecasts for the percentage c hange in Indian \nconsumption for quarters followi ng the end of the sample, shoul d only use \ninformation that was available up to and including 2024 Q3. The se are genuine \nforecasts, made in advance using whatever information is availa ble at the time. \nTherefore in order to generate ex -ante forecasts, the model requires forecasts of the \npredictors. To obtain these we can use pure time series approac hes. Alternatively, \nforecasts from some other source, such as a government agency, may be available \nand can be used.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 35,
      "total_chunks": 873
    }
  },
  {
    "text": "12 \nEx-post forecasts are those that are made using later information on the predictors. \nFor example, ex-post forecasts o f consumption may use the actua l observations of \nthe predictors, once these have been observed. These are not ge nuine forecasts, \nbut are useful for studying the behaviour of forecasting models . The model from \nwhich ex-post forecasts are pr oduced should not be estimated using data from the \nforecast period. That is, ex-post forecasts can assume knowledge of the predictor \nvariables (the x variables), but  should not assume knowledge of  the data that are to \nbe forecast (the y variable). \nA comparative evaluation of ex- ante forecasts and ex-post forecasts can help to \nseparate out the sources of forec ast uncertainty. This will sho w whether forecast \nerrors have arisen due to poor forecasts of the predictor or due to a poor forecasting \nmodel. \nThe great advantage of regressi on models is that they can be us ed to capture",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 36,
      "total_chunks": 873
    }
  },
  {
    "text": "errors have arisen due to poor forecasts of the predictor or due to a poor forecasting \nmodel. \nThe great advantage of regressi on models is that they can be us ed to capture \nimportant relationships between t he forecast variable of intere st and the predictor \nvariables. A major challenge ho wever, is that in order to gener ate ex-ante forecasts, \nthe model require future values of each predictor. If scenario based forecasting is of \ninterest then these models are ex tremely useful. However, if ex -ante forecasting is \nthe main focus, obtaining forecasts of the predictors can be ch allenging (in many \ncases generating forecasts for the predictor variables can be m ore challenging than \nforecasting directly the forecast variable without using predictors). \nSimple Linear Regression - Example \nRegression analysis is a widely used tool for analysing the relationship between \nvariables for prediction purposes. In this example, we will loo k at the relationship",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 37,
      "total_chunks": 873
    }
  },
  {
    "text": "Regression analysis is a widely used tool for analysing the relationship between \nvariables for prediction purposes. In this example, we will loo k at the relationship \nbetween social media ads and revenue by running a regression an alysis on the two \nvariables on Microsoft Excel. \n \nColumn A: Month \nColumn B: Social Media Ads (independent variable x) \nColumn C: Revenues (dependent variable y) \nRevenues based on social media ads appear in rows 5 to 16. \nTwo formulae’s can be used in Microsoft Excel to forecast Reven ues based on \nproposed social media ads for forecast months: \n1. FORECAST function. \n2. SLOPE FUNCTION and INTERCEPT FUNCTION \nWe will also draw a scatter chart with the regression line. \nStep 1: Populate the data",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 38,
      "total_chunks": 873
    }
  },
  {
    "text": "13 \n \n \n \n \n \n \n \n \n \n \n \nStep 2: Use Microsoft Excel formulae’s \nForecast the revenue using the FORECAST function. For example, the company \nproposes to release 1125 social media ads in forecast month 1 and wants to forecast \nits revenue based on regression. In cell C22, use the formula =  \nFORECAST(B22,C5:C16,B5:B16). This formulae takes data from the Social Media \nads and regression data from the actual sales figures based on past performance of \nsocial media ads to generate a forecast. \nTraditional formulae’s of regressi on forecasting done from calc ulating slope first and \nthen the Y-Intercept is also available in Miscrosoft Excel. \nIn cell C18, the formulae is =SLOPE(C5:C16,B5:B16). \nIn cell C19, the formulae is=INTERCEPT(C5:C16,B5:B16). \n \nThus the formulae to compute rev enue forecast using Slope and the Y-intercept is \nproduct of proposed social media ads in a month multiplied by the slope and add the \nY-intercept value (if the value is negative then deduct).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 39,
      "total_chunks": 873
    }
  },
  {
    "text": "product of proposed social media ads in a month multiplied by the slope and add the \nY-intercept value (if the value is negative then deduct).  \nCell D29= A29*B29+C29",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 40,
      "total_chunks": 873
    }
  },
  {
    "text": "14 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nStep 3: Use Microsoft Excel chart for best fit line view \nSelect the Social Media ads and Revenue data in cell B5 to C16,  then go to Insert > \nChart > Scatter.  \nRight-click on the data points and select Format Data Series. U nder Marker Options, \nchange the colour to desired and choose no borderline.  \nRight-click on data points and s elect Add Trendline. Choose Lin ear line and check \nthe boxes for Display Equation on the chart and Display R-squar ed value on the \nchart. Make other display and clean up changes as desired.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 41,
      "total_chunks": 873
    }
  },
  {
    "text": "15 \n \n \nMultiple Linear Regression - Example \nMultiple linear regressions can be used to forecast revenues wh en two or more \nindependent variables are requir ed for a projection of a depend ent variable. In the \nexample below, we run a regression on direct marketing cost, so cial media \nadvertising cost and revenue to ide ntify the relationships betw een these variables \nusing Microsoft Excel. \nColumn A: Month \nColumn B: Direct Marketing cost (independent variable x1) \nColumn B: Social Media Ads (independent variable x2) \nColumn D: Revenues (dependent variable y) \nRevenues therefrom appear in column D rows 5 to 16. \nStep 1: Populate the data",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 42,
      "total_chunks": 873
    }
  },
  {
    "text": "16 \n \n \n \n \n \n \n \n \n \n \n \nStep 2: Run Regression Model \nGo to the menu tab>>Data>>Data Analysis and select Regression i n the drop down \nmenu items. \nSelect D4 to D16 for Input Y Range  \nSelect B4 to C16 for Input X Range \nCheck the box for Labels ( note row 4 is included in formul ae above to capture label \ntitles) \nSet Output Range at cell A33.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 43,
      "total_chunks": 873
    }
  },
  {
    "text": "17 \nStep 3: Result of the run on A33 cell \n \nThe relevant data is in cells B49, B50 and B51 which shall be u sed for computation \nof the forecasted sales.  \nStep 4: Result of the run on A33 cell \nCopy the co-efficients in the cells B48, B49, B50 and B51 from the summary output \nand paste it in cell A24, A25, A26 and A27 for ease of applying  formulae. We can \nnow forecast the revenue for each future month based on the pro posed budget of \nthe direct marketing costs and the social media ad cost.  \nWe can use the equation in cell D18 to forecast revenue: \n =$A$25+(B18*$A$26)+(C18*$A$27) for month 1. \nWe can use the equation in cell D19 to forecast revenue: \n =$A$25+(B19*$A$26)+(C19*$A$27) for month 2. \nWe can use the equation in cell D20 to forecast revenue: \n =$A$25+(B20*$A$26)+(C20*$A$27) for month 3. \nAnd so on.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 44,
      "total_chunks": 873
    }
  },
  {
    "text": "18 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nIn conclusion, while linear regre ssion models are simple and widely used, but they \ntoo have several drawbacks that one should be aware. These draw backs include \nlimited flexibility, susceptibi lity to outliers, assumptions of  linearity, overfitting, \nmulticollinearity, inability to h andle categorical variables, a nd assumptions of \nhomoscedasticity.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 45,
      "total_chunks": 873
    }
  },
  {
    "text": "19 \nCost forecasting \nForecasting revenues through any of the approaches above and it s relationship with \nvariable costs can be computed through statistical tools. Howev er, cost forecasting \nhas its own nuances. Firstly, there are overheads and fixed cos ts which may change \ndisproportionately to the change i n revenues and then there is inflation and other \nfactors which are external factors not in the control of the Company. \nThere are many factors that can affect the accuracy of cost for ecasting. Some of the \nmost important factors include: \n1. The level of detail required for the cost estimate. \n2. The complexity of the business. \n3. The timeframe involved in the forecast. \n4. The size and type of the project or product. \n5. The industry in which the project or product is being undertaken. \n6. The past performance is critical aspect but will it help for new products to be \nlaunched. \n7. Changes in economic conditions. \n8. Changes in technology.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 46,
      "total_chunks": 873
    }
  },
  {
    "text": "6. The past performance is critical aspect but will it help for new products to be \nlaunched. \n7. Changes in economic conditions. \n8. Changes in technology. \n9. Changes in customer demands. \n10. Unconventional or innovative methods or techniques being used in the project or \nproduct. \n11. The availability of qualified personnel. \n12. The availability of necessary resources including fund raising plans and its costs. \n13. The impact of changes in government regulations or policies  on the project or \nproduct. \n14. The availability of suitable consultants or advisors. \n15. Other factors that may affect cost estimates, such as politi c a l  a n d  s o c i a l  \nconditions, weather, etc. \nThere are a number of methods that can be used to forecast costs, including: \n1. Statistical tools – modelling  to simulate future events and calculate possible \noutcomes; \n2. Extrapolation - projecting results from known data;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 47,
      "total_chunks": 873
    }
  },
  {
    "text": "20\n3. Trend analysis - assessing whether a particular pattern is likely to continue;\n4. Scenario analysis - creating various possible future scenarios and assessing their\neffects on costs; \n5. Sensitivity analysis - identifying which param eters (element s) have the biggest\neffect on costs; and \n6. Benchmarking - comparing costs against those of similar projects or products to\ndetermine relative improvement/ decline (or stability) over time/space etc. \nIn order to improve accuracy, it is often helpful to use multip le methods to predict \ncosts, as different types of information can give different res ults (e.g., trend analysis \nvs scenario analysis). Additionally, it is important to regular ly review cost forecasts to \nensure that they remain accurate and up-to-date (e.g., by incor porating feedback \nfrom stakeholders). If a forecast does not meet expectations, i t is often necessary to \nreassess all assumptions underlying the forecast (e.g., level o f detail, complexity,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 48,
      "total_chunks": 873
    }
  },
  {
    "text": "from stakeholders). If a forecast does not meet expectations, i t is often necessary to \nreassess all assumptions underlying the forecast (e.g., level o f detail, complexity, \ntimeframe, etc.). \nOne material area on cost forecasting that valuers focus on is unstated expenses. \nFor example, the promoters or f ounders may decide to avoid full  salary or to \nwithdraw it gradually after reaching a particular goal. Under s uch circumstances, the \nbusiness forecast ought to account for such hidden costs. The v aluations of any \nenterprise will drop if such hidden costs are accounted for in the cash flow forecasts. \nForecasting and risk determinati on are very much at the heart o f practical valuation. \nAsset value bears on future, uncertain payoffs, so valuation re quires forecasting \nunder uncertainty, with both the forecast and the uncertainty priced in the valuation.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 49,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 1Chapter 1  Getting startedGetting started\nForecasting has fascinated people for thousands of years, sometimes being\nconsidered a sign of divine inspiration, and sometimes being seen as a criminal\nactivity. The Jewish prophet Isaiah wrote in about 700 BC\nOne hundred years later, in ancient Babylon, forecasters would foretell the future\nbased on the distribution of maggots in a rotten sheep’s liver. Around the same time,\npeople wanting forecasts would journey to Delphi in Greece to consult the Oracle,\nwho would provide her predictions while intoxicated by ethylene vapours.\nForecasters had a tougher time under the emperor Constantius, who issued a decree\nin AD357 forbidding anyone “to consult a soothsayer, a mathematician, or a\nforecaster  May curiosity to foretell the future be silenced forever.” A similar ban\non forecasting occurred in England in 1736 when it became an offence to defraud by\ncharging money for predictions. The punishment was three months’ imprisonment",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 50,
      "total_chunks": 873
    }
  },
  {
    "text": "on forecasting occurred in England in 1736 when it became an offence to defraud by\ncharging money for predictions. The punishment was three months’ imprisonment\nwith hard labour!\nThe varying fortunes of forecasters arise because good forecasts can seem almost\nmagical, while bad forecasts may be dangerous. Consider the following famous\npredictions about computing.\nI think there is a world market for maybe five computers.  (Chairman of IBM,\n1943)\nComputers in the future may weigh no more than 1.5 tons.  (Popular Mechanics,\n1949)\nThere is no reason anyone would want a computer in their home.  (President,\nDEC, 1977)\nThe last of these was made only three years before IBM produced the first personal\ncomputer. Not surprisingly, you can no longer buy a DEC computer. Forecasting is\nobviously a difficult activity, and businesses that do it well have a big advantage over\nthose whose forecasts fail.\nTell us what the future holds, so we may know that you are gods.\n(Isaiah 41:23)\n21",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 51,
      "total_chunks": 873
    }
  },
  {
    "text": "In this book, we will explore the most reliable methods for producing forecasts. The\nemphasis will be on methods that are replicable and testable, and have been shown\nto work.\n22",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 52,
      "total_chunks": 873
    }
  },
  {
    "text": "1.11.1  What can be forecast?What can be forecast?\nForecasting is required in many situations: deciding whether to build another power\ngeneration plant in the next five years requires forecasts of future demand;\nscheduling staff in a call centre next week requires forecasts of call volumes;\nstocking an inventory requires forecasts of stock requirements. Forecasts can be\nrequired several years in advance (for the case of capital investments), or only a few\nminutes beforehand (for telecommunication routing). Whatever the circumstances\nor time horizons involved, forecasting is an important aid to effective and efficient\nplanning.\nSome things are easier to forecast than others. The time of the sunrise tomorrow\nmorning can be forecast precisely. On the other hand, tomorrow’s lotto numbers\ncannot be forecast with any accuracy. The predictability of an event or a quantity\ndepends on several factors including:\n1. how well we understand the factors that contribute to it;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 53,
      "total_chunks": 873
    }
  },
  {
    "text": "cannot be forecast with any accuracy. The predictability of an event or a quantity\ndepends on several factors including:\n1. how well we understand the factors that contribute to it;\n2. how much data is available;\n3. whether the forecasts can affect the thing we are trying to forecast.\nFor example, forecasts of electricity demand can be highly accurate because all three\nconditions are usually satisfied. We have a good idea of the contributing factors:\nelectricity demand is driven largely by temperatures, with smaller effects for\ncalendar variation such as holidays, and economic conditions. Provided there is a\nsufficient history of data on electricity demand and weather conditions, and we have\nthe skills to develop a good model linking electricity demand and the key driver\nvariables, the forecasts can be remarkably accurate.\nOn the other hand, when forecasting currency exchange rates, only one of the\nconditions is satisfied: there is plenty of available data. However, we have a limited",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 54,
      "total_chunks": 873
    }
  },
  {
    "text": "On the other hand, when forecasting currency exchange rates, only one of the\nconditions is satisfied: there is plenty of available data. However, we have a limited\nunderstanding of the factors that affect exchange rates, and forecasts of the\nexchange rate have a direct effect on the rates themselves. If there are well-\npublicised forecasts that the exchange rate will increase, then people will\nimmediately adjust the price they are willing to pay and so the forecasts are self-\nfulfilling. In a sense, the exchange rates become their own forecasts. This is an\nexample of the “efficient market hypothesis”. Consequently, forecasting whether\nthe exchange rate will rise or fall tomorrow is about as predictable as forecasting\n23",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 55,
      "total_chunks": 873
    }
  },
  {
    "text": "whether a tossed coin will come down as a head or a tail. In both situations, you will\nbe correct about 50% of the time, whatever you forecast. In situations like this,\nforecasters need to be aware of their own limitations, and not claim more than is\npossible.\nOften in forecasting, a key step is knowing when something can be forecast\naccurately, and when forecasts will be no better than tossing a coin. Good forecasts\ncapture the genuine patterns and relationships which exist in the historical data, but\ndo not replicate past events that will not occur again. In this book, we will learn how\nto tell the difference between a random fluctuation in the past data that should be\nignored, and a genuine pattern that should be modelled and extrapolated.\nMany people wrongly assume that forecasts are not possible in a changing\nenvironment. Every environment is changing, and a good forecasting model captures\nthe way in which things are changing. Forecasts rarely assume that the environment",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 56,
      "total_chunks": 873
    }
  },
  {
    "text": "environment. Every environment is changing, and a good forecasting model captures\nthe way in which things are changing. Forecasts rarely assume that the environment\nis unchanging. What is normally assumed is that the way in which the environment\nis changing  will continue into the future. That is, a highly volatile environment will\ncontinue to be highly volatile; a business with fluctuating sales will continue to have\nfluctuating sales; and an economy that has gone through booms and busts will\ncontinue to go through booms and busts. A forecasting model is intended to capture\nthe way things move, not just where things are. As Abraham Lincoln said, “If we\ncould first know where we are and whither we are tending, we could better judge\nwhat to do and how to do it”.\nForecasting situations vary widely in their time horizons, factors determining actual\noutcomes, types of data patterns, and many other aspects. Forecasting methods can",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 57,
      "total_chunks": 873
    }
  },
  {
    "text": "what to do and how to do it”.\nForecasting situations vary widely in their time horizons, factors determining actual\noutcomes, types of data patterns, and many other aspects. Forecasting methods can\nbe simple, such as using the most recent observation as a forecast (which is called\nthe naïve methodnaïve method ), or highly complex, such as neural nets and econometric systems\nof simultaneous equations. Sometimes, there will be no data available at all. For\nexample, we may wish to forecast the sales of a new product in its first year, but\nthere are obviously no data to work with. In situations like this, we use judgmental\nforecasting, discussed in Chapter 4. The choice of method depends on what data are\navailable and the predictability of the quantity to be forecast.\n24",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 58,
      "total_chunks": 873
    }
  },
  {
    "text": "1.21.2  Forecasting, planning and goalsForecasting, planning and goals\nForecasting is a common statistical task in business, where it helps to inform\ndecisions about the scheduling of production, transportation and personnel, and\nprovides a guide to long-term strategic planning. However, business forecasting is\noften done poorly, and is frequently confused with planning and goals. They are\nthree different things.\nForecastingForecasting\nis about predicting the future as accurately as possible, given all of the\ninformation available, including historical data and knowledge of any future\nevents that might impact the forecasts.\nGoalsGoals\nare what you would like to have happen. Goals should be linked to forecasts and\nplans, but this does not always occur. Too often, goals are set without any plan\nfor how to achieve them, and no forecasts for whether they are realistic.\nPlanningPlanning\nis a response to forecasts and goals. Planning involves determining the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 59,
      "total_chunks": 873
    }
  },
  {
    "text": "for how to achieve them, and no forecasts for whether they are realistic.\nPlanningPlanning\nis a response to forecasts and goals. Planning involves determining the\nappropriate actions that are required to make your forecasts match your goals.\nForecasting should be an integral part of the decision-making activities of\nmanagement, as it can play an important role in many areas of a company. Modern\norganisations require short-term, medium-term and long-term forecasts,\ndepending on the specific application.\nShort-term forecastsShort-term forecasts\nare needed for the scheduling of personnel, production and transportation. As\npart of the scheduling process, forecasts of demand are often also required.\nMedium-term forecastsMedium-term forecasts\nare needed to determine future resource requirements, in order to purchase raw\nmaterials, hire personnel, or buy machinery and equipment.\nLong-term forecastsLong-term forecasts\nare used in strategic planning. Such decisions must take account of market",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 60,
      "total_chunks": 873
    }
  },
  {
    "text": "materials, hire personnel, or buy machinery and equipment.\nLong-term forecastsLong-term forecasts\nare used in strategic planning. Such decisions must take account of market\nopportunities, environmental factors and internal resources.\n25",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 61,
      "total_chunks": 873
    }
  },
  {
    "text": "An organisation needs to develop a forecasting system that involves several\napproaches to predicting uncertain events. Such forecasting systems require the\ndevelopment of expertise in identifying forecasting problems, applying a range of\nforecasting methods, selecting appropriate methods for each problem, and\nevaluating and refining forecasting methods over time. It is also important to have\nstrong organisational support for the use of formal forecasting methods if they are\nto be used successfully.\n26",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 62,
      "total_chunks": 873
    }
  },
  {
    "text": "1.31.3  Determining what to forecastDetermining what to forecast\nIn the early stages of a forecasting project, decisions need to be made about what\nshould be forecast. For example, if forecasts are required for items in a\nmanufacturing environment, it is necessary to ask whether forecasts are needed for:\n1. every product line, or for groups of products?\n2. every sales outlet, or for outlets grouped by region, or only for total sales?\n3. weekly data, monthly data or annual data?\nIt is also necessary to consider the forecasting horizon. Will forecasts be required for\none month in advance, for 6 months, or for ten years? Different types of models will\nbe necessary, depending on what forecast horizon is most important.\nHow frequently are forecasts required? Forecasts that need to be produced frequently\nare better done using an automated system than with methods that require careful\nmanual work.\nIt is worth spending time talking to the people who will use the forecasts to ensure",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 63,
      "total_chunks": 873
    }
  },
  {
    "text": "are better done using an automated system than with methods that require careful\nmanual work.\nIt is worth spending time talking to the people who will use the forecasts to ensure\nthat you understand their needs, and how the forecasts are to be used, before\nembarking on extensive work in producing the forecasts.\nOnce it has been determined what forecasts are required, it is then necessary to find\nor collect the data on which the forecasts will be based. The data required for\nforecasting may already exist. These days, a lot of data are recorded, and the\nforecaster’s task is often to identify where and how the required data are stored. The\ndata may include sales records of a company, the historical demand for a product, or\nthe unemployment rate for a geographic region. A large part of a forecaster’s time\ncan be spent in locating and collating the available data prior to developing suitable\nforecasting methods.\n27",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 64,
      "total_chunks": 873
    }
  },
  {
    "text": "1.41.4  Forecasting data and methodsForecasting data and methods\nThe appropriate forecasting methods depend largely on what data are available.\nIf there are no data available, or if the data available are not relevant to the forecasts,\nthen qualitative forecastingqualitative forecasting  methods must be used. These methods are not purely\nguesswork—there are well-developed structured approaches to obtaining good\nforecasts without using historical data. These methods are discussed in Chapter 4.\nQuantitative forecastingQuantitative forecasting  can be applied when two conditions are satisfied:\n1. numerical information about the past is available;\n2. it is reasonable to assume that some aspects of the past patterns will continue\ninto the future.\nThere is a wide range of quantitative forecasting methods, often developed within\nspecific disciplines for specific purposes. Each method has its own properties,\naccuracies, and costs that must be considered when choosing a specific method.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 65,
      "total_chunks": 873
    }
  },
  {
    "text": "specific disciplines for specific purposes. Each method has its own properties,\naccuracies, and costs that must be considered when choosing a specific method.\nMost quantitative prediction problems use either time series data (collected at\nregular intervals over time) or cross-sectional data (collected at a single point in\ntime). In this book we are concerned with forecasting future data, and we\nconcentrate on the time series domain.\nTime series forecastingTime series forecasting\nExamples of time series data include:\nDaily IBM stock prices\nMonthly rainfall\nQuarterly sales results for Amazon\nAnnual Google profits\nAnything that is observed sequentially over time is a time series. In this book, we will\nonly consider time series that are observed at regular intervals of time (e.g., hourly,\ndaily, weekly, monthly, quarterly, annually). Irregularly spaced time series can also\noccur, but are beyond the scope of this book.\n28",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 66,
      "total_chunks": 873
    }
  },
  {
    "text": "When forecasting time series data, the aim is to estimate how the sequence of\nobservations will continue into the future. Figure 1.1 shows the quarterly Australian\nbeer production from 1992 to the second quarter of 2010.\nFigure 1.1: Australian quarterly beer production: 1992Q1–2010Q2, with two years of\nforecasts.\nThe blue lines show forecasts for the next two years. Notice how the forecasts have\ncaptured the seasonal pattern seen in the historical data and replicated it for the next\ntwo years. The dark shaded region shows 80% prediction intervals. That is, each\nfuture value is expected to lie in the dark shaded region with a probability of 80%.\nThe light shaded region shows 95% prediction intervals. These prediction intervals\nare a useful way of displaying the uncertainty in forecasts. In this case the forecasts\nare expected to be accurate, and hence the prediction intervals are quite narrow.\nThe simplest time series forecasting methods use only information on the variable to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 67,
      "total_chunks": 873
    }
  },
  {
    "text": "are expected to be accurate, and hence the prediction intervals are quite narrow.\nThe simplest time series forecasting methods use only information on the variable to\nbe forecast, and make no attempt to discover the factors that affect its behaviour.\nTherefore they will extrapolate trend and seasonal patterns, but they ignore all other\ninformation such as marketing initiatives, competitor activity, changes in economic\nconditions, and so on.\nTime series models used for forecasting include decomposition models, exponential\nsmoothing models and ARIMA models. These models are discussed in Chapters 6, 7\nand 8, respectively.\n29",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 68,
      "total_chunks": 873
    }
  },
  {
    "text": "Predictor variables and time series forecastingPredictor variables and time series forecasting\nPredictor variables are often useful in time series forecasting. For example, suppose\nwe wish to forecast the hourly electricity demand (ED) of a hot region during the\nsummer period. A model with predictor variables might be of the form\nThe relationship is not exact — there will always be changes in electricity demand\nthat cannot be accounted for by the predictor variables. The “error” term on the\nright allows for random variation and the effects of relevant variables that are not\nincluded in the model. We call this an explanatory modelexplanatory model  because it helps explain\nwhat causes the variation in electricity demand.\nBecause the electricity demand data form a time series, we could also use a timetime\nseries modelseries model  for forecasting. In this case, a suitable time series forecasting equation\nis of the form",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 69,
      "total_chunks": 873
    }
  },
  {
    "text": "series modelseries model  for forecasting. In this case, a suitable time series forecasting equation\nis of the form\nwhere  is the present hour,  is the next hour,  is the previous hour,  is\ntwo hours ago, and so on. Here, prediction of the future is based on past values of a\nvariable, but not on external variables which may affect the system. Again, the\n“error” term on the right allows for random variation and the effects of relevant\nvariables that are not included in the model.\nThere is also a third type of model which combines the features of the above two\nmodels. For example, it might be given by\nThese types of mixed modelsmixed models  have been given various names in different disciplines.\nThey are known as dynamic regression models, panel data models, longitudinal\nmodels, transfer function models, and linear system models (assuming that  is\nlinear). These models are discussed in Chapter 9.\nAn explanatory model is useful because it incorporates information about other",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 70,
      "total_chunks": 873
    }
  },
  {
    "text": "linear). These models are discussed in Chapter 9.\nAn explanatory model is useful because it incorporates information about other\nvariables, rather than only historical values of the variable to be forecast. However,\nthere are several reasons a forecaster might select a time series model rather than an\nexplanatory or mixed model. First, the system may not be understood, and even if it\nwas understood it may be extremely difficult to measure the relationships that are\n30",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 71,
      "total_chunks": 873
    }
  },
  {
    "text": "assumed to govern its behaviour. Second, it is necessary to know or forecast the\nfuture values of the various predictors in order to be able to forecast the variable of\ninterest, and this may be too difficult. Third, the main concern may be only to predict\nwhat will happen, not to know why it happens. Finally, the time series model may\ngive more accurate forecasts than an explanatory or mixed model.\nThe model to be used in forecasting depends on the resources and data available, the\naccuracy of the competing models, and the way in which the forecasting model is to\nbe used.\n31",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 72,
      "total_chunks": 873
    }
  },
  {
    "text": "1.51.5  Some case studiesSome case studies\nThe following four cases are from our consulting practice and demonstrate different\ntypes of forecasting situations and the associated problems that often arise.\nCase 1Case 1\nThe client was a large company manufacturing disposable tableware such as napkins\nand paper plates. They needed forecasts of each of hundreds of items every month.\nThe time series data showed a range of patterns, some with trends, some seasonal,\nand some with neither. At the time, they were using their own software, written in-\nhouse, but it often produced forecasts that did not seem sensible. The methods that\nwere being used were the following:\n1. average of the last 12 months data;\n2. average of the last 6 months data;\n3. prediction from a straight line regression over the last 12 months;\n4. prediction from a straight line regression over the last 6 months;\n5. prediction obtained by a straight line through the last observation with slope",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 73,
      "total_chunks": 873
    }
  },
  {
    "text": "4. prediction from a straight line regression over the last 6 months;\n5. prediction obtained by a straight line through the last observation with slope\nequal to the average slope of the lines connecting last year’s and this year’s\nvalues;\n6. prediction obtained by a straight line through the last observation with slope\nequal to the average slope of the lines connecting last year’s and this year’s\nvalues, where the average is taken only over the last 6 months.\nThey required us to tell them what was going wrong and to modify the software to\nprovide more accurate forecasts. The software was written in COBOL, making it\ndifficult to do any sophisticated numerical computation.\nCase 2Case 2\nIn this case, the client was the Australian federal government, who needed to\nforecast the annual budget for the Pharmaceutical Benefit Scheme (PBS). The PBS\nprovides a subsidy for many pharmaceutical products sold in Australia, and the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 74,
      "total_chunks": 873
    }
  },
  {
    "text": "forecast the annual budget for the Pharmaceutical Benefit Scheme (PBS). The PBS\nprovides a subsidy for many pharmaceutical products sold in Australia, and the\nexpenditure depends on what people purchase during the year. The total expenditure\n32",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 75,
      "total_chunks": 873
    }
  },
  {
    "text": "was around A$7 billion in 2009, and had been underestimated by nearly $1 billion in\neach of the two years before we were asked to assist in developing a more accurate\nforecasting approach.\nIn order to forecast the total expenditure, it is necessary to forecast the sales\nvolumes of hundreds of groups of pharmaceutical products using monthly data.\nAlmost all of the groups have trends and seasonal patterns. The sales volumes for\nmany groups have sudden jumps up or down due to changes in what drugs are\nsubsidised. The expenditures for many groups also have sudden changes due to\ncheaper competitor drugs becoming available.\nThus we needed to find a forecasting method that allowed for trend and seasonality\nif they were present, and at the same time was robust to sudden changes in the\nunderlying patterns. It also needed to be able to be applied automatically to a large\nnumber of time series.\nCase 3Case 3\nA large car fleet company asked us to help them forecast vehicle re-sale values. They",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 76,
      "total_chunks": 873
    }
  },
  {
    "text": "number of time series.\nCase 3Case 3\nA large car fleet company asked us to help them forecast vehicle re-sale values. They\npurchase new vehicles, lease them out for three years, and then sell them. Better\nforecasts of vehicle sales values would mean better control of profits; understanding\nwhat affects resale values may allow leasing and sales policies to be developed in\norder to maximise profits.\nAt the time, the resale values were being forecast by a group of specialists.\nUnfortunately, they saw any statistical model as a threat to their jobs, and were\nuncooperative in providing information. Nevertheless, the company provided a large\namount of data on previous vehicles and their eventual resale values.\nCase 4Case 4\nIn this project, we needed to develop a model for forecasting weekly air passenger\ntraffic on major domestic routes for one of Australia’s leading airlines. The company\nrequired forecasts of passenger numbers for each major domestic route and for each",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 77,
      "total_chunks": 873
    }
  },
  {
    "text": "traffic on major domestic routes for one of Australia’s leading airlines. The company\nrequired forecasts of passenger numbers for each major domestic route and for each\nclass of passenger (economy class, business class and first class). The company\nprovided weekly traffic data from the previous six years.\nAir passenger numbers are affected by school holidays, major sporting events,\nadvertising campaigns, competition behaviour, etc. School holidays often do not\ncoincide in different Australian cities, and sporting events sometimes move from\n33",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 78,
      "total_chunks": 873
    }
  },
  {
    "text": "one city to another. During the period of the historical data, there was a major pilots’\nstrike during which there was no traffic for several months. A new cut-price airline\nalso launched and folded. Towards the end of the historical data, the airline had\ntrialled a redistribution of some economy class seats to business class, and some\nbusiness class seats to first class. After several months, however, the seat\nclassifications reverted to the original distribution.\n34",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 79,
      "total_chunks": 873
    }
  },
  {
    "text": "1.61.6  The basic steps in a forecasting taskThe basic steps in a forecasting task\nA forecasting task usually involves five basic steps.\nStep 1: Problem definition.Step 1: Problem definition.\nOften this is the most difficult part of forecasting. Defining the problem carefully\nrequires an understanding of the way the forecasts will be used, who requires the\nforecasts, and how the forecasting function fits within the organisation requiring\nthe forecasts. A forecaster needs to spend time talking to everyone who will be\ninvolved in collecting data, maintaining databases, and using the forecasts for\nfuture planning.\nStep 2: Gathering information.Step 2: Gathering information.\nThere are always at least two kinds of information required: (a) statistical data,\nand (b) the accumulated expertise of the people who collect the data and use the\nforecasts. Often, it will be difficult to obtain enough historical data to be able to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 80,
      "total_chunks": 873
    }
  },
  {
    "text": "and (b) the accumulated expertise of the people who collect the data and use the\nforecasts. Often, it will be difficult to obtain enough historical data to be able to\nfit a good statistical model. In that case, the judgmental forecasting methods of\nChapter 4 can be used. Occasionally, old data will be less useful due to structural\nchanges in the system being forecast; then we may choose to use only the most\nrecent data. However, remember that good statistical models will handle\nevolutionary changes in the system; don’t throw away good data unnecessarily.\nStep 3: Preliminary (exploratory) analysis.Step 3: Preliminary (exploratory) analysis.\nAlways start by graphing the data. Are there consistent patterns? Is there a\nsignificant trend? Is seasonality important? Is there evidence of the presence of\nbusiness cycles? Are there any outliers in the data that need to be explained by\nthose with expert knowledge? How strong are the relationships among the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 81,
      "total_chunks": 873
    }
  },
  {
    "text": "business cycles? Are there any outliers in the data that need to be explained by\nthose with expert knowledge? How strong are the relationships among the\nvariables available for analysis? Various tools have been developed to help with\nthis analysis. These are discussed in ChaptersÖ2 and 6.\nStep 4: Choosing and fitting models.Step 4: Choosing and fitting models.\nThe best model to use depends on the availability of historical data, the strength\nof relationships between the forecast variable and any explanatory variables, and\nthe way in which the forecasts are to be used. It is common to compare two or\nthree potential models. Each model is itself an artificial construct that is based on\na set of assumptions (explicit and implicit) and usually involves one or more\nparameters which must be estimated using the known historical data. We will\n35",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 82,
      "total_chunks": 873
    }
  },
  {
    "text": "discuss regression models (Chapter 5 ), exponential smoothing methods\n(ChapterÖ7), Box-Jenkins ARIMA models (ChapterÖ 8), Dynamic regression models\n(Chapter 9), Hierarchical forecasting (Chapter 10), and several advanced methods\nincluding neural networks and vector autoregression in ChapterÖ 11.\nStep 5: Using and evaluating a forecasting model.Step 5: Using and evaluating a forecasting model.\nOnce a model has been selected and its parameters estimated, the model is used\nto make forecasts. The performance of the model can only be properly evaluated\nafter the data for the forecast period have become available. A number of methods\nhave been developed to help in assessing the accuracy of forecasts. There are also\norganisational issues in using and acting on the forecasts. A brief discussion of\nsome of these issues is given in ChapterÖ3. When using a forecasting model in\npractice, numerous practical issues arise such as how to handle missing values",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 83,
      "total_chunks": 873
    }
  },
  {
    "text": "some of these issues is given in ChapterÖ3. When using a forecasting model in\npractice, numerous practical issues arise such as how to handle missing values\nand outliers, or how to deal with short time series. These are discussed in Chapter\n12.\n36",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 84,
      "total_chunks": 873
    }
  },
  {
    "text": "1.71.7  The statistical forecasting perspectiveThe statistical forecasting perspective\nThe thing we are trying to forecast is unknown (or we would not be forecasting it),\nand so we can think of it as a random variable . For example, the total sales for next\nmonth could take a range of possible values, and until we add up the actual sales at\nthe end of the month, we don’t know what the value will be. So until we know the\nsales for next month, it is a random quantity.\nBecause next month is relatively close, we usually have a good idea what the likely\nsales values could be. On the other hand, if we are forecasting the sales for the same\nmonth next year, the possible values it could take are much more variable. In most\nforecasting situations, the variation associated with the thing we are forecasting will\nshrink as the event approaches. In other words, the further ahead we forecast, the\nmore uncertain we are.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 85,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasting situations, the variation associated with the thing we are forecasting will\nshrink as the event approaches. In other words, the further ahead we forecast, the\nmore uncertain we are.\nWe can imagine many possible futures, each yielding a different value for the thing\nwe wish to forecast. Plotted in black in Figure 1.2 are the total international visitors\nto Australia from 1980 to 2015. Also shown are ten possible futures from 2016–2025.\nFigure 1.2: Total international visitors to Australia (1980-2015) along with ten possible\nfutures.\n37",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 86,
      "total_chunks": 873
    }
  },
  {
    "text": "When we obtain a forecast, we are estimating the middle of the range of possible\nvalues the random variable could take. Often, a forecast is accompanied by a\nprediction intervalprediction interval  giving a range of values the random variable could take with\nrelatively high probability. For example, a 95% prediction interval contains a range\nof values which should include the actual future value with probability 95%.\nRather than plotting individual possible futures as shown in Figure 1.2, we usually\nshow these prediction intervals instead. The plot below shows 80% and 95%\nintervals for the future Australian international visitors. The blue line is the average\nof the possible future values, which we call the point forecastspoint forecasts .\nFigure 1.3: Total international visitors to Australia (1980–2015) along with 10-year forecasts\nand 80% and 95% prediction intervals.\nWe will use the subscript  for time. For example,  will denote the observation at",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 87,
      "total_chunks": 873
    }
  },
  {
    "text": "and 80% and 95% prediction intervals.\nWe will use the subscript  for time. For example,  will denote the observation at\ntime . Suppose we denote all the information we have observed as  and we want to\nforecast . We then write  meaning “the random variable  given what we know\nin ”. The set of values that this random variable could take, along with their\nrelative probabilities, is known as the “probability distribution” of . In\nforecasting, we call this the forecast distributionforecast distribution .\nWhen we talk about the “forecast”, we usually mean the average value of the\nforecast distribution, and we put a “hat” over  to show this. Thus, we write the\nforecast of  as , meaning the average of the possible values that  could take\n38",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 88,
      "total_chunks": 873
    }
  },
  {
    "text": "given everything we know. Occasionally, we will use  to refer to the median (or\nmiddle value) of the forecast distribution instead.\nIt is often useful to specify exactly what information we have used in calculating the\nforecast. Then we will write, for example,  to mean the forecast of  taking\naccount of all previous observations . Similarly,  means the\nforecast of  taking account of  (i.e., an -step forecast taking account\nof all observations up to time ).\n/g3F/g35 /g30\n/g3F/g35 /g30/g5D/g30/gC3/g12 /g35 /g30\n/g9/g35 /g12 /gD/g9A/gD/g35 /g30/gC3/g12 /gA/g3F /g35 /g16 /gC /g24 /g5D/g16\n/g35 /g16 /gC /g24 /g35 /g12 /gD/g9A/gD/g35 /g16 /g24\n/g16\n39\n\n\n1.81.8  ExercisesExercises\n1. For cases 3 and 4 in SectionÖ 1.5, list the possible predictor variables that might be\nuseful, assuming that the relevant data are available.\n2. For case 3 in SectionÖ 1.5, describe the five steps of forecasting in the context of\nthis project.\n40",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 89,
      "total_chunks": 873
    }
  },
  {
    "text": "1.91.9  Further readingFurther reading\nArmstrong ( 2001) covers the whole field of forecasting, with each chapter\nwritten by different experts. It is highly opinionated at times (and we don’t agree\nwith everything in it), but it is full of excellent general advice on tackling\nforecasting problems.\nOrd, Fildes, & Kourentzes ( 2017) is a forecasting textbook covering some of the\nsame areas as this book, but with a different emphasis and not focused around\nany particular software environment. It is written by three highly respected\nforecasters, with many decades of experience between them.\nBibliography\nArmstrong, J. S. (Ed.). (2001). Principles of forecasting: A handbook for\nresearchers and practitioners . Kluwer Academic Publishers. [Amazon]\nOrd, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\n41",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 90,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 2Chapter 2  Time series graphicsTime series graphics\nThe first thing to do in any data analysis task is to plot the data. Graphs enable many\nfeatures of the data to be visualised, including patterns, unusual observations,\nchanges over time, and relationships between variables. The features that are seen in\nplots of the data must then be incorporated, as much as possible, into the forecasting\nmethods to be used. Just as the type of data determines what forecasting method to\nuse, it also determines what graphs are appropriate. But before we produce graphs,\nwe need to set up our time series in R.\n42",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 91,
      "total_chunks": 873
    }
  },
  {
    "text": "2.12.1 ĳtsĳ objects objects\nA time series can be thought of as a list of numbers, along with some information\nabout what times those numbers were recorded. This information can be stored as a\nıtsı object in R.\nSuppose you have annual observations for the last few years:\nYearYear ObservationObservation\n2012 123\n2013 39\n2014 78\n2015 52\n2016 110\nWe turn this into a ıtsı object using the ıts()ı function:\nIf you have annual data, with one observation per year, you only need to provide the\nstarting year (or the ending year).\nFor observations that are more frequent than once per year, you simply add a\nıfrequencyı argument. For example, if your monthly data is already stored as a\nnumerical vector ızı, then it can be converted to a ıtsı object like this:\nAlmost all of the data used in this book is already stored as ıtsı objects. But if you\nwant to work with your own data, you will need to use the ıts()ı function before\nproceeding with the analysis.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 92,
      "total_chunks": 873
    }
  },
  {
    "text": "Almost all of the data used in this book is already stored as ıtsı objects. But if you\nwant to work with your own data, you will need to use the ıts()ı function before\nproceeding with the analysis.\nFrequency of a time seriesFrequency of a time series\nThe “frequency” is the number of observations before the seasonal pattern repeats.\nWhen using the ıts()ı function in R, the following choices should be used.\ny <- ts(c(123,39,78,52,110), start=2012)\ny <- ts(z, start=2003, frequency=12)\n1\n43",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 93,
      "total_chunks": 873
    }
  },
  {
    "text": "DataData frequencyfrequency\nAnnual 1\nQuarterly 4\nMonthly 12\nWeekly 52\nActually, there are not  weeks in a year, but  on average,\nallowing for a leap year every fourth year. But most functions which use ıtsı objects\nrequire integer frequency.\nIf the frequency of observations is greater than once per week, then there is usually\nmore than one way of handling the frequency. For example, data with daily\nobservations might have a weekly seasonality (frequency ) or an annual\nseasonality (frequency ). Similarly, data that are observed every minute\nmight have an hourly seasonality (frequency ), a daily seasonality (frequency\n), a weekly seasonality (frequency ) and\nan annual seasonality (frequency ). If you want to use\na ıtsı object, then you need to decide which of these is the most important.\nIn chapter 11 we will look at handling these types of multiple seasonality, without\nhaving to choose just one of the frequencies.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 94,
      "total_chunks": 873
    }
  },
  {
    "text": "In chapter 11 we will look at handling these types of multiple seasonality, without\nhaving to choose just one of the frequencies.\n1. This is the opposite of the definition of frequency in physics, or in Fourier\nanalysis, where this would be called the “period”. ↩ \n44",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 95,
      "total_chunks": 873
    }
  },
  {
    "text": "2.22.2  Time plotsTime plots\nFor time series data, the obvious graph to start with is a time plot. That is, the\nobservations are plotted against the time of observation, with consecutive\nobservations joined by straight lines. Figure 2.1 below shows the weekly economy\npassenger load on Ansett Airlines between Australia’s two largest cities.\nFigure 2.1: Weekly economy passenger load on Ansett Airlines.\nWe will use the ıautoplot()ı command frequently. It automatically produces an\nappropriate plot of whatever you pass to it in the first argument. In this case, it\nrecognises ımelsyd[,\"Economy.Class\"] ı as a time series and produces a time plot.\nThe time plot immediately reveals some interesting features.\nThere was a period in 1989 when no passengers were carried — this was due to\nan industrial dispute.\nautoplot(melsyd[,\"Economy.Class\"]) +\n  ggtitle(\"Economy class passengers: Melbourne-Sydney\" ) +\n  xlab(\"Year\") +\n  ylab(\"Thousands\")\n45",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 96,
      "total_chunks": 873
    }
  },
  {
    "text": "There was a period of reduced load in 1992. This was due to a trial in which some\neconomy class seats were replaced by business class seats.\nA large increase in passenger load occurred in the second half of 1991.\nThere are some large dips in load around the start of each year. These are due to\nholiday effects.\nThere is a long-term fluctuation in the level of the series which increases during\n1987, decreases in 1989, and increases again through 1990 and 1991.\nThere are some periods of missing observations.\nAny model will need to take all these features into account in order to effectively\nforecast the passenger load into the future.\nA simpler time series is shown in Figure 2.2.\nFigure 2.2: Monthly sales of antidiabetic drugs in Australia.\nHere, there is a clear and increasing trend. There is also a strong seasonal pattern\nthat increases in size as the level of the series increases. The sudden drop at the start",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 97,
      "total_chunks": 873
    }
  },
  {
    "text": "Here, there is a clear and increasing trend. There is also a strong seasonal pattern\nthat increases in size as the level of the series increases. The sudden drop at the start\nof each year is caused by a government subsidisation scheme that makes it cost-\nautoplot(a10) +\n  ggtitle(\"Antidiabetic drug sales\" ) +\n  ylab(\"$ million\") +\n  xlab(\"Year\")\n46",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 98,
      "total_chunks": 873
    }
  },
  {
    "text": "effective for patients to stockpile drugs at the end of the calendar year. Any forecasts\nof this series would need to capture the seasonal pattern, and the fact that the trend\nis changing slowly.\n47",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 99,
      "total_chunks": 873
    }
  },
  {
    "text": "2.32.3  Time series patternsTime series patterns\nIn describing these time series, we have used words such as “trend” and “seasonal”\nwhich need to be defined more carefully.\nTrendTrend\nA trend exists when there is a long-term increase or decrease in the data. It does\nnot have to be linear. Sometimes we will refer to a trend as “changing direction”,\nwhen it might go from an increasing trend to a decreasing trend. There is a trend\nin the antidiabetic drug sales data shown in Figure 2.2.\nSeasonalSeasonal\nA seasonal pattern occurs when a time series is affected by seasonal factors such\nas the time of the year or the day of the week. Seasonality is always of a fixed and\nknown frequency. The monthly sales of antidiabetic drugs above shows\nseasonality which is induced partly by the change in the cost of the drugs at the\nend of the calendar year.\nCyclicCyclic\nA cycle occurs when the data exhibit rises and falls that are not of a fixed",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 100,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonality which is induced partly by the change in the cost of the drugs at the\nend of the calendar year.\nCyclicCyclic\nA cycle occurs when the data exhibit rises and falls that are not of a fixed\nfrequency. These fluctuations are usually due to economic conditions, and are\noften related to the “business cycle”. The duration of these fluctuations is usually\nat least 2 years.\nMany people confuse cyclic behaviour with seasonal behaviour, but they are really\nquite different. If the fluctuations are not of a fixed frequency then they are cyclic; if\nthe frequency is unchanging and associated with some aspect of the calendar, then\nthe pattern is seasonal. In general, the average length of cycles is longer than the\nlength of a seasonal pattern, and the magnitudes of cycles tend to be more variable\nthan the magnitudes of seasonal patterns.\nMany time series include trend, cycles and seasonality. When choosing a forecasting",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 101,
      "total_chunks": 873
    }
  },
  {
    "text": "than the magnitudes of seasonal patterns.\nMany time series include trend, cycles and seasonality. When choosing a forecasting\nmethod, we will first need to identify the time series patterns in the data, and then\nchoose a method that is able to capture the patterns properly.\nThe examples in Figure 2.3 show different combinations of the above components.\n48",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 102,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.3: Four examples of time series showing different patterns.\n1. The monthly housing sales (top left) show strong seasonality within each year,\nas well as some strong cyclic behaviour with a period of about 6–10 years. There\nis no apparent trend in the data over this period.\n2. The US treasury bill contracts (top right) show results from the Chicago market\nfor 100 consecutive trading days in 1981. Here there is no seasonality, but an\nobvious downward trend. Possibly, if we had a much longer series, we would see\nthat this downward trend is actually part of a long cycle, but when viewed over\nonly 100 days it appears to be a trend.\n3. The Australian quarterly electricity production (bottom left) shows a strong\nincreasing trend, with strong seasonality. There is no evidence of any cyclic\nbehaviour here.\n4. The daily change in the Google closing stock price (bottom right) has no trend,\nseasonality or cyclic behaviour. There are random fluctuations which do not",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 103,
      "total_chunks": 873
    }
  },
  {
    "text": "behaviour here.\n4. The daily change in the Google closing stock price (bottom right) has no trend,\nseasonality or cyclic behaviour. There are random fluctuations which do not\nappear to be very predictable, and no strong patterns that would help with\ndeveloping a forecasting model.\n49",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 104,
      "total_chunks": 873
    }
  },
  {
    "text": "2.42.4  Seasonal plotsSeasonal plots\nA seasonal plot is similar to a time plot except that the data are plotted against the\nindividual “seasons” in which the data were observed. An example is given below\nshowing the antidiabetic drug sales.\nFigure 2.4: Seasonal plot of monthly antidiabetic drug sales in Australia.\nThese are exactly the same data as were shown earlier, but now the data from each\nseason are overlapped. A seasonal plot allows the underlying seasonal pattern to be\nseen more clearly, and is especially useful in identifying years in which the pattern\nchanges.\nIn this case, it is clear that there is a large jump in sales in January each year.\nActually, these are probably sales in late December as customers stockpile before the\nend of the calendar year, but the sales are not registered with the government until a\nweek or two later. The graph also shows that there was an unusually small number of\nggseasonplot(a10, year.labels=TRUE , year.labels.left=TRUE ) +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 105,
      "total_chunks": 873
    }
  },
  {
    "text": "week or two later. The graph also shows that there was an unusually small number of\nggseasonplot(a10, year.labels=TRUE , year.labels.left=TRUE ) +\n  ylab(\"$ million\") +\n  ggtitle(\"Seasonal plot: antidiabetic drug sales\" )\n50",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 106,
      "total_chunks": 873
    }
  },
  {
    "text": "sales in March 2008 (most other years show an increase between February and\nMarch). The small number of sales in June 2008 is probably due to incomplete\ncounting of sales at the time the data were collected.\nA useful variation on the seasonal plot uses polar coordinates. Setting ıpolar=TRUEı\nmakes the time series axis circular rather than horizontal, as shown below.\nFigure 2.5: Polar seasonal plot of monthly antidiabetic drug sales in Australia.\nggseasonplot(a10, polar=TRUE) +\n  ylab(\"$ million\") +\n  ggtitle(\"Polar seasonal plot: antidiabetic drug sales\" )\n51",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 107,
      "total_chunks": 873
    }
  },
  {
    "text": "2.52.5  Seasonal subseries plotsSeasonal subseries plots\nAn alternative plot that emphasises the seasonal patterns is where the data for each\nseason are collected together in separate mini time plots.\nFigure 2.6: Seasonal subseries plot of monthly antidiabetic drug sales in Australia.\nThe horizontal lines indicate the means for each month. This form of plot enables\nthe underlying seasonal pattern to be seen clearly, and also shows the changes in\nseasonality over time. It is especially useful in identifying changes within particular\nseasons. In this example, the plot is not particularly revealing; but in some cases,\nthis is the most useful way of viewing seasonal changes over time.\nggsubseriesplot(a10) +\n  ylab(\"$ million\") +\n  ggtitle(\"Seasonal subseries plot: antidiabetic drug sales\" )\n52",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 108,
      "total_chunks": 873
    }
  },
  {
    "text": "2.62.6  ScatterplotsScatterplots\nThe graphs discussed so far are useful for visualising individual time series. It is also\nuseful to explore relationships between time series.\nFigure 2.7  shows two time series: half-hourly electricity demand (in Gigawatts) and\ntemperature (in degrees Celsius), for 2014 in Victoria, Australia. The temperatures\nare for Melbourne, the largest city in Victoria, while the demand values are for the\nentire state.\nFigure 2.7: Half hourly electricity demand and temperatures in Victoria, Australia, for 2014.\n(The actual code for this plot is a little more complicated than what is shown in order\nto include the months on the x-axis.)\nWe can study the relationship between demand and temperature by plotting one\nseries against the other.\nautoplot(elecdemand[,c (\"Demand\",\"Temperature\")], facets=TRUE) +\n  xlab(\"Year: 2014\") + ylab(\"\") +\n  ggtitle(\"Half-hourly electricity demand: Victoria, Australia\" )\n53",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 109,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.8: Half-hourly electricity demand plotted against temperature for 2014 in\nVictoria, Australia.\nThis scatterplot helps us to visualise the relationship between the variables. It is\nclear that high demand occurs when temperatures are high due to the effect of air-\nconditioning. But there is also a heating effect, where demand increases for very low\ntemperatures.\nCorrelationCorrelation\nIt is common to compute correlation coefficients  to measure the strength of the\nrelationship between two variables. The correlation between variables  and  is\ngiven by\nas.data.frame(elecdemand) |>\n  ggplot(aes(x=Temperature, y=Demand)) +\n  geom_point() +\n  ylab(\"Demand (GW)\") + xlab(\"Temperature (Celsius)\" )\n54",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 110,
      "total_chunks": 873
    }
  },
  {
    "text": "The value of  always lies between  and 1 with negative values indicating a\nnegative relationship and positive values indicating a positive relationship. The\ngraphs in Figure 2.9 show examples of data sets with varying levels of correlation.\nFigure 2.9: Examples of data sets with different levels of correlation.\nThe correlation coefficient only measures the strength of the linear relationship, and\ncan sometimes be misleading. For example, the correlation for the electricity\ndemand and temperature data shown in Figure 2.8 is 0.28, but the non-linear\nrelationship is stronger than that.\nThe plots in Figure 2.10 all have correlation coefficients of 0.82, but they have very\ndifferent relationships. This shows how important it is to look at the plots of the data\nand not simply rely on correlation values.\n/g2E /gC3/g12\n55",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 111,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.10: Each of these plots has a correlation coefficient of 0.82. Data from FJ\nAnscombe (1973) Graphs in statistical analysis. American Statistician , 2727, 17–21.\nScatterplot matricesScatterplot matrices\nWhen there are several potential predictor variables, it is useful to plot each variable\nagainst each other variable. Consider the five time series shown in Figure 2.11,\nshowing quarterly visitor numbers for five regions of New South Wales, Australia.\nautoplot(visnights[,1 :5], facets=TRUE) +\n  ylab(\"Number of visitor nights each quarter (millions)\" )\n56\n\n\nFigure 2.11: Quarterly visitor nights for various regions of NSW, Australia.\nTo see the relationships between these five time series, we can plot each time series\nagainst the others. These plots can be arranged in a scatterplot matrix, as shown in\nFigure 2.12. (This plot requires the ıGGallyı package to be installed.)\nGGally::ggpairs(as.data.frame(visnights[,1 :5]))\n57",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 112,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.12: A scatterplot matrix of the quarterly visitor nights in five regions of NSW,\nAustralia.\nFor each panel, the variable on the vertical axis is given by the variable name in that\nrow, and the variable on the horizontal axis is given by the variable name in that\ncolumn. There are many options available to produce different plots within each\npanel. In the default version, the correlations are shown in the upper right half of the\nplot, while the scatterplots are shown in the lower half. On the diagonal are shown\ndensity plots.\nThe value of the scatterplot matrix is that it enables a quick view of the relationships\nbetween all pairs of variables. In this example, the second column of plots shows\nthere is a strong positive relationship between visitors to the NSW north coast and\nvisitors to the NSW south coast, but no detectable relationship between visitors to\nthe NSW north coast and visitors to the NSW south inland. Outliers can also be seen.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 113,
      "total_chunks": 873
    }
  },
  {
    "text": "visitors to the NSW south coast, but no detectable relationship between visitors to\nthe NSW north coast and visitors to the NSW south inland. Outliers can also be seen.\nThere is one unusually high quarter for the NSW Metropolitan region, corresponding\n58",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 114,
      "total_chunks": 873
    }
  },
  {
    "text": "to the 2000 Sydney Olympics. This is most easily seen in the first two plots in the left\ncolumn of Figure 2.12, where the largest value for NSW Metro is separate from the\nmain cloud of observations.\n59\n\n\n2.72.7  Lag plotsLag plots\nFigure 2.13 displays scatterplots of quarterly Australian beer production, where the\nhorizontal axis shows lagged values of the time series. Each graph shows  plotted\nagainst  for different values of .\nFigure 2.13: Lagged scatterplots for quarterly beer production.\nbeer2 <- window(ausbeer, start=1992 )\ngglagplot(beer2)\n60",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 115,
      "total_chunks": 873
    }
  },
  {
    "text": "Here the colours indicate the quarter of the variable on the vertical axis. The lines\nconnect points in chronological order. The relationship is strongly positive at lags 4\nand 8, reflecting the strong seasonality in the data. The negative relationship seen\nfor lags 2 and 6 occurs because peaks (in Q4) are plotted against troughs (in Q2)\nThe ıwindow()ı function used here is very useful when extracting a portion of a time\nseries. In this case, we have extracted the data from ıausbeerı, beginning in 1992.\n61",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 116,
      "total_chunks": 873
    }
  },
  {
    "text": "2.82.8  AutocorrelationAutocorrelation\nJust as correlation measures the extent of a linear relationship between two\nvariables, autocorrelation measures the linear relationship between lagged values  of\na time series.\nThere are several autocorrelation coefficients, corresponding to each panel in the lag\nplot. For example,  measures the relationship between  and ,  measures the\nrelationship between  and , and so on.\nThe value of  can be written as\nwhere  is the length of the time series.\nThe first nine autocorrelation coefficients for the beer production data are given in\nthe following table.\n-0.102 -0.657 -0.060 0.869 -0.089 -0.635 -0.054 0.832 -0.108\nThese correspond to the nine scatterplots in Figure 2.13. The autocorrelation\ncoefficients are plotted to show the autocorrelation function  or ACF. The plot is also\nknown as a correlogram .\nggAcf(beer2)\n62",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 117,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.14: Autocorrelation function of quarterly beer production.\nIn this graph:\n is higher than for the other lags. This is due to the seasonal pattern in the\ndata: the peaks tend to be four quarters apart and the troughs tend to be four\nquarters apart.\n is more negative than for the other lags because troughs tend to be two\nquarters behind peaks.\nThe dashed blue lines indicate whether the correlations are significantly\ndifferent from zero. These are explained in Section 2.9.\nTrend and seasonality in ACF plotsTrend and seasonality in ACF plots\nWhen data have a trend, the autocorrelations for small lags tend to be large and\npositive because observations nearby in time are also nearby in size. So the ACF of\ntrended time series tend to have positive values that slowly decrease as the lags\nincrease.\nWhen data are seasonal, the autocorrelations will be larger for the seasonal lags (at\nmultiples of the seasonal frequency) than for other lags.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 118,
      "total_chunks": 873
    }
  },
  {
    "text": "increase.\nWhen data are seasonal, the autocorrelations will be larger for the seasonal lags (at\nmultiples of the seasonal frequency) than for other lags.\nWhen data are both trended and seasonal, you see a combination of these effects.\nThe monthly Australian electricity demand series plotted in Figure 2.15 shows both\ntrend and seasonality. Its ACF is shown in Figure 2.16 .\naelec <- window(elec, start=1980 )\nautoplot(aelec) + xlab(\"Year\") + ylab(\"GWh\")\n63",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 119,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.15: Monthly Australian electricity demand from 1980–1995.\nFigure 2.16: ACF of monthly Australian electricity demand.\nThe slow decrease in the ACF as the lags increase is due to the trend, while the\n“scalloped” shape is due to the seasonality.\nggAcf(aelec, lag=48)\n64\n\n\n2.92.9  White noiseWhite noise\nTime series that show no autocorrelation are called white noisewhite noise . Figure 2.17 gives an\nexample of a white noise series.\nFigure 2.17: A white noise time series.\nset.seed(30)\ny <- ts(rnorm(50))\nautoplot(y) + ggtitle(\"White noise\")\nggAcf(y)\n65",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 120,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 2.18: Autocorrelation function for the white noise series.\nFor white noise series, we expect each autocorrelation to be close to zero. Of course,\nthey will not be exactly equal to zero as there is some random variation. For a white\nnoise series, we expect 95% of the spikes in the ACF to lie within  where  is\nthe length of the time series. It is common to plot these bounds on a graph of the ACF\n(the blue dashed lines above). If one or more large spikes are outside these bounds,\nor if substantially more than 5% of spikes are outside these bounds, then the series\nis probably not white noise.\nIn this example,  and so the bounds are at . All of the\nautocorrelation coefficients lie within these limits, confirming that the data are\nwhite noise.\n/g65/g13/g10 /gCA /g16/g16\n/g16 /g1E /g16/g11 /g65/g13/g10 /gCA /g16/g11 /g1E /g65/g11/gF/g13/g19\n66",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 121,
      "total_chunks": 873
    }
  },
  {
    "text": "2.102.10  ExercisesExercises\n1. Use the help function to explore what the series ıgoldı, ıwoolyrnqı and ıgası\nrepresent.\na. Use ıautoplot()ı to plot each of these in separate plots.\nb. What is the frequency of each series? Hint: apply the ıfrequency()ı function.\nc. Use ıwhich.max()ı to spot the outlier in the ıgoldı series. Which observation\nwas it?\n2. Download the file ıtute1.csvı from the book website, open it in Excel (or some\nother spreadsheet application), and review its contents. You should find four\ncolumns of information. Columns B through D each contain a quarterly series,\nlabelled Sales, AdBudget and GDP. Sales contains the quarterly sales for a small\ncompany over the period 1981-2005. AdBudget is the advertising budget and GDP\nis the gross domestic product. All series have been adjusted for inflation.\na. You can read the data into R with the following script:\nb. Convert the data to time series\n(The ı[,-1]ı removes the first column which contains the quarters as we",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 122,
      "total_chunks": 873
    }
  },
  {
    "text": "a. You can read the data into R with the following script:\nb. Convert the data to time series\n(The ı[,-1]ı removes the first column which contains the quarters as we\ndon’t need them now.)\nc. Construct time series plots of each of the three series\nCheck what happens when you don’t include ıfacets=TRUEı.\n3. Download some monthly Australian retail data from the book website. These\nrepresent retail sales in various categories for different Australian states, and are\nstored in a MS-Excel file.\na. You can read the data into R with the following script:\ntute1 <- read.csv(\"tute1.csv\", header=TRUE )\nView(tute1)\nmytimeseries <- ts(tute1[,-1], start=1981, frequency=4)\nautoplot(mytimeseries, facets=TRUE )\n67",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 123,
      "total_chunks": 873
    }
  },
  {
    "text": "The second argument ( ıskip=1ı) is required because the Excel sheet has two\nheader rows.\nb. Select one of the time series as follows (but replace the column name with\nyour own chosen column):\nc. Explore your chosen retail time series using the following functions:\nıautoplot()ı, ıggseasonplot()ı, ıggsubseriesplot()ı, ıgglagplot()ı, ıggAcf()ı\nCan you spot any seasonality, cyclicity and trend? What do you learn about\nthe series?\n4. Create time plots of the following time series: ıbicoalı, ıchickenı, ıdoleı,\nıusdeathsı, ılynxı, ıgoogı, ıwritingı, ıfancyı, ıa10ı, ıh02ı.\nUse ıhelp()ı to find out about the data in each series.\nFor the ıgoogı plot, modify the axis labels and title.\n5. Use the ıggseasonplot()ı and ıggsubseriesplot()ı functions to explore the\nseasonal patterns in the following time series: ıwritingı, ıfancyı, ıa10ı, ıh02ı.\nWhat can you say about the seasonal patterns?\nCan you identify any unusual years?\n6. Use the following graphics functions: ıautoplot()ı, ıggseasonplot()ı,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 124,
      "total_chunks": 873
    }
  },
  {
    "text": "What can you say about the seasonal patterns?\nCan you identify any unusual years?\n6. Use the following graphics functions: ıautoplot()ı, ıggseasonplot()ı,\nıggsubseriesplot()ı, ıgglagplot()ı, ıggAcf()ı and explore features from the\nfollowing time series: ıhsalesı, ıusdeathsı, ıbricksqı, ısunspotareaı, ıgasolineı.\nCan you spot any seasonality, cyclicity and trend?\nWhat do you learn about the series?\n7. The ıarrivalsı data set comprises quarterly international arrivals (in thousands)\nto Australia from Japan, New Zealand, UK and the US.\nUse ıautoplot()ı, ıggseasonplot()ı and ıggsubseriesplot()ı to compare the\ndifferences between the arrivals from these four countries.\nCan you identify any unusual observations?\n8. The following time plots and ACF plots correspond to four different time series.\nYour task is to match each time plot in the first row with one of the ACF plots in\nthe second row.\nretaildata <- readxl::read_excel( \"retail.xlsx\", skip=1)\nmyts <- ts(retaildata[,\"A3349873A\"],",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 125,
      "total_chunks": 873
    }
  },
  {
    "text": "Your task is to match each time plot in the first row with one of the ACF plots in\nthe second row.\nretaildata <- readxl::read_excel( \"retail.xlsx\", skip=1)\nmyts <- ts(retaildata[,\"A3349873A\"],\n  frequency=12, start=c (1982,4))\n68",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 126,
      "total_chunks": 873
    }
  },
  {
    "text": "9. The ıpigsı data shows the monthly total number of pigs slaughtered in Victoria,\nAustralia, from Jan 1980 to Aug 1995. Use ımypigs <- window(pigs, start=1990) ı\nto select the data starting from 1990. Use ıautoplotı and ıggAcfı for ımypigsı\nseries and compare these to white noise plots from Figures 2.17 and 2.18.\n10. ıdjı contains 292 consecutive trading days of the Dow Jones Index. Use ıddj <-\ndiff(dj)ı to compute the daily changes in the index. Plot ıddjı and its ACF. Do\nthe changes in the Dow Jones Index look like white noise?\n69",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 127,
      "total_chunks": 873
    }
  },
  {
    "text": "2.112.11  Further readingFurther reading\nW. S. Cleveland (1993) is a classic book on the principles of visualisation for data\nanalysis. While it is more than 20 years old, the ideas are timeless.\nUnwin (2015) is a modern introduction to graphical data analysis using R. It does\nnot have much information on time series graphics, but plenty of excellent\ngeneral advice on using graphics for data analysis.\nBibliographyBibliography\nCleveland, W. S. (1993). Visualizing data . Hobart Press. [Amazon]\nUnwin, A. (2015). Graphical data analysis with R . Chapman; Hall/CRC. [Amazon]\n70",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 128,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 3Chapter 3  The forecaster’s toolboxThe forecaster’s toolbox\nIn this chapter, we discuss some general tools that are useful for many different\nforecasting situations. We will describe some benchmark forecasting methods, ways\nof making the forecasting task simpler using transformations and adjustments,\nmethods for checking whether a forecasting method has adequately utilised the\navailable information, and techniques for computing prediction intervals.\nEach of the tools discussed in this chapter will be used repeatedly in subsequent\nchapters as we develop and explore a range of forecasting methods.\n71",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 129,
      "total_chunks": 873
    }
  },
  {
    "text": "3.13.1  Some simple forecasting methodsSome simple forecasting methods\nSome forecasting methods are extremely simple and surprisingly effective. We will\nuse the following four forecasting methods as benchmarks throughout this book.\nAverage methodAverage method\nHere, the forecasts of all future values are equal to the average (or “mean”) of the\nhistorical data. If we let the historical data be denoted by , then we can\nwrite the forecasts as\nThe notation  is a short-hand for the estimate of  based on the data\n.\nNaïve methodNaïve method\nFor naïve forecasts, we simply set all forecasts to be the value of the last observation.\nThat is,\nThis method works remarkably well for many economic and financial time series.\nBecause a naïve forecast is optimal when data follow a random walk (see Section 8.1),\nthese are also called random walk forecastsrandom walk forecasts .\nmeanf(y, h)\n# y contains the time series\n# h is the forecast horizon\nnaive(y, h)\nrwf(y, h) # Equivalent alternative\n72",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 130,
      "total_chunks": 873
    }
  },
  {
    "text": "Seasonal naïve methodSeasonal naïve method\nA similar method is useful for highly seasonal data. In this case, we set each forecast\nto be equal to the last observed value from the same season (e.g., the same month of\nthe previous year). Formally, the forecast for time  is written as\nwhere  the seasonal period, and  is the integer part of  (i.e., the\nnumber of complete years in the forecast period prior to time ). This looks\nmore complicated than it really is. For example, with monthly data, the forecast for\nall future February values is equal to the last observed February value. With quarterly\ndata, the forecast of all future Q2 values is equal to the last observed Q2 value (where\nQ2 means the second quarter). Similar rules apply for other months and quarters,\nand for other seasonal periods.\nDrift methodDrift method\nA variation on the naïve method is to allow the forecasts to increase or decrease over",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 131,
      "total_chunks": 873
    }
  },
  {
    "text": "and for other seasonal periods.\nDrift methodDrift method\nA variation on the naïve method is to allow the forecasts to increase or decrease over\ntime, where the amount of change over time (called the driftdrift) is set to be the average\nchange seen in the historical data. Thus the forecast for time  is given by\nThis is equivalent to drawing a line between the first and last observations, and\nextrapolating it into the future.\nExamplesExamples\nFigure 3.1 shows the first three methods applied to the quarterly beer production\ndata.\nsnaive(y, h)\nrwf(y, h, drift=TRUE)\n73",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 132,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.1: Forecasts of Australian quarterly beer production.\nIn Figure 3.2, the non-seasonal methods are applied to a series of 200 days of the\nGoogle daily closing stock price.\n# Set training data from 1992 to 2007\nbeer2 <- window(ausbeer,start=1992 ,end=c(2007,4))\n# Plot some forecasts\nautoplot(beer2) +\n  autolayer(meanf(beer2, h=11),\n    series=\"Mean\", PI=FALSE ) +\n  autolayer(naive(beer2, h=11),\n    series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(beer2, h=11),\n    series=\"Seasonal naïve\", PI=FALSE ) +\n  ggtitle(\"Forecasts for quarterly beer production\" ) +\n  xlab(\"Year\") + ylab(\"Megalitres\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n74",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 133,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.2: Forecasts based on 200 days of the Google daily closing stock price.\nSometimes one of these simple methods will be the best forecasting method\navailable; but in many cases, these methods will serve as benchmarks rather than the\nmethod of choice. That is, any forecasting methods we develop will be compared to\nthese simple methods to ensure that the new method is better than these simple\nalternatives. If not, the new method is not worth considering.\nautoplot(goog200) +\n  autolayer(meanf(goog200, h=40),\n    series=\"Mean\", PI=FALSE ) +\n  autolayer(rwf(goog200, h=40),\n    series=\"Naïve\", PI=FALSE) +\n  autolayer(rwf(goog200, drift=TRUE , h=40),\n    series=\"Drift\", PI=FALSE ) +\n  ggtitle(\"Google stock (daily ending 6 Dec 2013)\" ) +\n  xlab(\"Day\") + ylab(\"Closing Price (US$)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n75",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 134,
      "total_chunks": 873
    }
  },
  {
    "text": "3.23.2  Transformations and adjustmentsTransformations and adjustments\nAdjusting the historical data can often lead to a simpler forecasting task. Here, we\ndeal with four kinds of adjustments: calendar adjustments, population adjustments,\ninflation adjustments and mathematical transformations. The purpose of these\nadjustments and transformations is to simplify the patterns in the historical data by\nremoving known sources of variation or by making the pattern more consistent\nacross the whole data set. Simpler patterns usually lead to more accurate forecasts.\nCalendar adjustmentsCalendar adjustments\nSome of the variation seen in seasonal data may be due to simple calendar effects. In\nsuch cases, it is usually much easier to remove the variation before fitting a\nforecasting model. The ımonthdays()ı function will compute the number of days in\neach month or quarter.\nFor example, if you are studying the monthly milk production on a farm, there will",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 135,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasting model. The ımonthdays()ı function will compute the number of days in\neach month or quarter.\nFor example, if you are studying the monthly milk production on a farm, there will\nbe variation between the months simply because of the different numbers of days in\neach month, in addition to the seasonal variation across the year.\ndframe <- cbind(Monthly = milk,\n                DailyAverage = milk /monthdays(milk))\n  autoplot(dframe, facet=TRUE ) +\n    xlab(\"Years\") + ylab(\"Pounds\") +\n    ggtitle(\"Milk production per cow\" )\n76",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 136,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.3: Monthly milk production per cow.\nNotice how much simpler the seasonal pattern is in the average daily production plot\ncompared to the total monthly production plot. By looking at the average daily\nproduction instead of the total monthly production, we effectively remove the\nvariation due to the different month lengths. Simpler patterns are usually easier to\nmodel and lead to more accurate forecasts.\nA similar adjustment can be done for sales data when the number of trading days in\neach month varies. In this case, the sales per trading day can be modelled instead of\nthe total sales for each month.\nPopulation adjustments\nAny data that are affected by population changes can be adjusted to give per-capita\ndata. That is, consider the data per person (or per thousand people, or per million\npeople) rather than the total. For example, if you are studying the number of hospital\nbeds in a particular region over time, the results are much easier to interpret if you",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 137,
      "total_chunks": 873
    }
  },
  {
    "text": "people) rather than the total. For example, if you are studying the number of hospital\nbeds in a particular region over time, the results are much easier to interpret if you\nremove the effects of population changes by considering the number of beds per\nthousand people. Then you can see whether there have been real increases in the\nnumber of beds, or whether the increases are due entirely to population increases. It\nis possible for the total number of beds to increase, but the number of beds per\n77",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 138,
      "total_chunks": 873
    }
  },
  {
    "text": "thousand people to decrease. This occurs when the population is increasing faster\nthan the number of hospital beds. For most data that are affected by population\nchanges, it is best to use per-capita data rather than the totals.\nInflation adjustmentsInflation adjustments\nData which are affected by the value of money are best adjusted before modelling.\nFor example, the average cost of a new house will have increased over the last few\ndecades due to inflation. A $200,000 house this year is not the same as a $200,000\nhouse twenty years ago. For this reason, financial time series are usually adjusted so\nthat all values are stated in dollar values from a particular year. For example, the\nhouse price data may be stated in year 2000 dollars.\nTo make these adjustments, a price index is used. If  denotes the price index and \ndenotes the original house price in year , then  gives the adjusted\nhouse price at year 2000 dollar values. Price indexes are often constructed by",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 139,
      "total_chunks": 873
    }
  },
  {
    "text": "denotes the original house price in year , then  gives the adjusted\nhouse price at year 2000 dollar values. Price indexes are often constructed by\ngovernment agencies. For consumer goods, a common price index is the Consumer\nPrice Index (or CPI).\nMathematical transformationsMathematical transformations\nIf the data show variation that increases or decreases with the level of the series,\nthen a transformation can be useful. For example, a logarithmic transformation is\noften useful. If we denote the original observations as  and the\ntransformed observations as , then . Logarithms are useful\nbecause they are interpretable: changes in a log value are relative (or percentage)\nchanges on the original scale. So if log base 10 is used, then an increase of 1 on the log\nscale corresponds to a multiplication of 10 on the original scale. Another useful\nfeature of log transformations is that they constrain the forecasts to stay positive on\nthe original scale.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 140,
      "total_chunks": 873
    }
  },
  {
    "text": "scale corresponds to a multiplication of 10 on the original scale. Another useful\nfeature of log transformations is that they constrain the forecasts to stay positive on\nthe original scale.\nSometimes other transformations are also used (although they are not so\ninterpretable). For example, square roots and cube roots can be used. These are\ncalled power transformationspower transformations  because they can be written in the form .\nA useful family of transformations, that includes both logarithms and power\ntransformations, is the family of Box-Cox transformationsBox-Cox transformations  (Box & Cox, 1964), which\ndepend on the parameter  and are defined as follows:\n78",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 141,
      "total_chunks": 873
    }
  },
  {
    "text": "The logarithm in a Box-Cox transformation is always a natural logarithm (i.e., to\nbase ). So if , natural logarithms are used, but if , a power\ntransformation is used, followed by some simple scaling.\nIf , then , so the transformed data is shifted downwards but there\nis no change in the shape of the time series. But for all other values of , the time\nseries will change shape.\nUse the slider below to see the effect of varying  to transform Australian monthly\nelectricity production:\nAustralian Monthly Electricity Production\nTransformation parameter: λ\n-1 21\n-1 -0.7 -0.4 -0.1 0.2 0.5 0.8 1.1 1.4 1.7 2\n/g33 /g30 /g1E /g8 /g4D/g50/g48/g9/g35 /g30 /gA /g4A/g47/g60/g4D /g1E/g11 /g1C\n/g9/g35 /g4D\n/g30 /gC3 /g12/gA/g10/g4D /g50/g55/g49/g46/g53/g58/g4A/g54/g46/gF\n/g21/g4D /g1E/g11 /g4D /gDC/g11\n/g4D /g1E/g12 /g33 /g30 /g1E /g35 /g30 /gC3/g12\n/g4D\n/g4D\n79",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 142,
      "total_chunks": 873
    }
  },
  {
    "text": "A good value of  is one which makes the size of the seasonal variation about the\nsame across the whole series, as that makes the forecasting model simpler. In this\ncase,  works quite well, although any value of  between 0 and 0.5 would\ngive similar results.\nThe ıBoxCox.lambda()ı function will choose a value of lambda for you.\nThe ıBoxCox()ı command actually implements a slight modification of the Box-Cox\ntransformation, discussed in Bickel & Doksum ( 1981), which allows for negative\nvalues of  when :\nFor positive values of , this is the same as the original Box-Cox transformation.\nHaving chosen a transformation, we need to forecast the transformed data. Then, we\nneed to reverse the transformation (or back-transform ) to obtain forecasts on the\noriginal scale. The reverse Box-Cox transformation is given by\n/g4D\n/g4D /g1E /g11/gF/g14/g11 /g4D\n(lambda <- BoxCox.lambda(elec))\n#> [1] 0.2654\nautoplot(BoxCox(elec,lambda))\n/g35 /g30 /g4D /g1F/g11",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 143,
      "total_chunks": 873
    }
  },
  {
    "text": "original scale. The reverse Box-Cox transformation is given by\n/g4D\n/g4D /g1E /g11/gF/g14/g11 /g4D\n(lambda <- BoxCox.lambda(elec))\n#> [1] 0.2654\nautoplot(BoxCox(elec,lambda))\n/g35 /g30 /g4D /g1F/g11\n/g33 /g30 /g1E /g8 /g4D/g50/g48/g9/g35 /g30 /gA /g4A/g47/g60/g4D /g1E/g11 /g1C\n/g54/g4A/g48/g4F/g9/g35 /g30 /gA/g9/g5D/g35 /g30 /g5D /g4D /gC3 /g12/gA/g10/g4D /g50/g55/g49/g46/g53/g58/g4A/g54/g46/gF\n/g35 /g30\n/g35 /g30 /g1E /g8\n/g46/g59/g51/g9/g33 /g30 /gA /g4A/g47/g60/g4D /g1E/g11 /g1C\n/g54/g4A/g48/g4F/g9/g4D/g33 /g30 /gC /g12/gA/g5D/g4D/g33 /g30 /gC/g12 /g5D /g12/g10/g4D /g50/g55/g49/g46/g53/g58/g4A/g54/g46/gF /g9/g14/gF/g12/gA\n80",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 144,
      "total_chunks": 873
    }
  },
  {
    "text": "Features of power transformationsFeatures of power transformations\nChoose a simple value of . It makes explanations easier.\nThe forecasting results are relatively insensitive to the value of .\nOften no transformation is needed.\nTransformations sometimes make little difference to the forecasts but have a\nlarge effect on prediction intervals.\nBias adjustmentsBias adjustments\nOne issue with using mathematical transformations such as Box-Cox\ntransformations is that the back-transformed point forecast will not be the mean of\nthe forecast distribution. In fact, it will usually be the median of the forecast\ndistribution (assuming that the distribution on the transformed space is symmetric).\nFor many purposes, this is acceptable, but occasionally the mean forecast is\nrequired. For example, you may wish to add up sales forecasts from various regions\nto form a forecast for the whole country. But medians do not add up, whereas means\ndo.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 145,
      "total_chunks": 873
    }
  },
  {
    "text": "required. For example, you may wish to add up sales forecasts from various regions\nto form a forecast for the whole country. But medians do not add up, whereas means\ndo.\nFor a Box-Cox transformation, the back-transformed mean is given by\nwhere  is the -step forecast variance. The larger the forecast variance, the bigger\nthe difference between the mean and the median.\nThe difference between the simple back-transformed forecast given by (3.1) and the\nmean given by (3.2) is called the biasbias. When we use the mean, rather than the\nmedian, we say the point forecasts have been bias-adjustedbias-adjusted .\nTo see how much difference this bias-adjustment makes, consider the following\nexample, where we forecast average annual price of eggs using the drift method with\na log transformation . The log transformation is useful in this case to ensure\nthe forecasts and the prediction intervals stay positive.\n81",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 146,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.4: Forecasts of egg prices using a random walk with drift applied to the logged\ndata.\nThe blue line in Figure 3.4 shows the forecast medians while the red line shows the\nforecast means. Notice how the skewed forecast distribution pulls up the point\nforecast when we use the bias adjustment.\nBias adjustment is not done by default in the forecastforecast package. If you want your\nforecasts to be means rather than medians, use the argument ıbiasadj=TRUEı when\nyou select your Box-Cox transformation parameter.\nBibliographyBibliography\nBickel, P. J., & Doksum, K. A. (1981). An analysis of transformations revisited.\nJournal of the American Statistical Association , 76(374), 296–311.\nfc <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80)\nfc2 <- rwf(eggs, drift=TRUE, lambda=0, h=50, level=80,\n  biasadj=TRUE)\nautoplot(eggs) +\n  autolayer(fc, series=\"Simple back transformation\") +\n  autolayer(fc2, series=\"Bias adjusted\", PI=FALSE ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n82",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 147,
      "total_chunks": 873
    }
  },
  {
    "text": "Box, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the\nRoyal Statistical Society. Series B, Statistical Methodology , 26(2), 211–252.\n[DOI]\n83",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 148,
      "total_chunks": 873
    }
  },
  {
    "text": "3.33.3  Residual diagnosticsResidual diagnostics\nFitted valuesFitted values\nEach observation in a time series can be forecast using all previous observations. We\ncall these fitted valuesfitted values  and they are denoted by , meaning the forecast of \nbased on observations  . We use these so often, we sometimes drop part\nof the subscript and just write  instead of . Fitted values always involve one-\nstep forecasts.\nActually, fitted values are often not true forecasts because any parameters involved\nin the forecasting method are estimated using all available observations in the time\nseries, including future observations. For example, if we use the average method, the\nfitted values are given by\nwhere  is the average computed over all available observations, including those at\ntimes after . Similarly, for the drift method, the drift parameter is estimated using\nall available observations. In this case, the fitted values are given by",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 149,
      "total_chunks": 873
    }
  },
  {
    "text": "times after . Similarly, for the drift method, the drift parameter is estimated using\nall available observations. In this case, the fitted values are given by\nwhere . In both cases, there is a parameter to be estimated\nfrom the data. The “hat” above the  reminds us that this is an estimate. When the\nestimate of  involves observations after time , the fitted values are not true\nforecasts. On the other hand, naïve or seasonal naïve forecasts do not involve any\nparameters, and so fitted values are true forecasts in such cases.\nResidualsResiduals\nThe “residuals” in a time series model are what is left over after fitting a model. For\nmany (but not all) time series models, the residuals are equal to the difference\nbetween the observations and the corresponding fitted values:\n84",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 150,
      "total_chunks": 873
    }
  },
  {
    "text": "Residuals are useful in checking whether a model has adequately captured the\ninformation in the data. A good forecasting method will yield residuals with the\nfollowing properties:\n1. The residuals are uncorrelated. If there are correlations between residuals, then\nthere is information left in the residuals which should be used in computing\nforecasts.\n2. The residuals have zero mean. If the residuals have a mean other than zero, then\nthe forecasts are biased.\nAny forecasting method that does not satisfy these properties can be improved.\nHowever, that does not mean that forecasting methods that satisfy these properties\ncannot be improved. It is possible to have several different forecasting methods for\nthe same data set, all of which satisfy these properties. Checking these properties is\nimportant in order to see whether a method is using all of the available information,\nbut it is not a good way to select a forecasting method.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 151,
      "total_chunks": 873
    }
  },
  {
    "text": "important in order to see whether a method is using all of the available information,\nbut it is not a good way to select a forecasting method.\nIf either of these properties is not satisfied, then the forecasting method can be\nmodified to give better forecasts. Adjusting for bias is easy: if the residuals have\nmean , then simply add  to all forecasts and the bias problem is solved. Fixing\nthe correlation problem is harder, and we will not address it until Chapter 9.\nIn addition to these essential properties, it is useful (but not necessary) for the\nresiduals to also have the following two properties.\n3. The residuals have constant variance.\n4. The residuals are normally distributed.\nThese two properties make the calculation of prediction intervals easier (see Section\n3.5 for an example). However, a forecasting method that does not satisfy these\nproperties cannot necessarily be improved. Sometimes applying a Box-Cox",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 152,
      "total_chunks": 873
    }
  },
  {
    "text": "3.5 for an example). However, a forecasting method that does not satisfy these\nproperties cannot necessarily be improved. Sometimes applying a Box-Cox\ntransformation may assist with these properties, but otherwise there is usually little\nthat you can do to ensure that your residuals have constant variance and a normal\ndistribution. Instead, an alternative approach to obtaining prediction intervals is\nnecessary. Again, we will not address how to do this until later in the book.\nExample: Forecasting the Google daily closing stock priceExample: Forecasting the Google daily closing stock price\nFor stock market prices and indexes, the best forecasting method is often the naïve\nmethod. That is, each forecast is simply equal to the last observed value, or \n. Hence, the residuals are simply equal to the difference between consecutive\n85",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 153,
      "total_chunks": 873
    }
  },
  {
    "text": "observations:\nThe following graph shows the Google daily closing stock price (GOOG). The large\njump at day 166 corresponds to 18 October 2013 when the price jumped 12% due to\nunexpectedly strong third quarter results.\nFigure 3.5: The daily Google stock price to 6 Dec 2013.\nThe residuals obtained from forecasting this series using the naïve Ömethod are\nshown in Figure 3.6. The large positive residual is a result of the unexpected price\njump at day 166.\n/g21 /g30 /g1E /g35 /g30 /gC3 /g3F/g35 /g30 /g1E /g35 /g30 /gC3 /g35 /g30/gC3/g12 /gF\nautoplot(goog200) +\n  xlab(\"Day\") + ylab(\"Closing Price (US$)\") +\n  ggtitle(\"Google Stock (daily ending 6 December 2013)\" )\nres <- residuals(naive(goog200))\nautoplot(res) + xlab(\"Day\") + ylab(\"\") +\n  ggtitle(\"Residuals from naïve method\" )\n86",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 154,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.6: Residuals from forecasting the Google stock price using the naïve method.\nFigure 3.7: Histogram of the residuals from the naïve ßmethod applied to the Google stock\nprice. The right tail seems a little too long for a normal distribution.\ngghistogram(res) + ggtitle(\"Histogram of residuals\" )\nggAcf(res) + ggtitle(\"ACF of residuals\")\n87",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 155,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.8: ACF of the residuals from the naïve method applied to the Google stock price.\nThe lack of correlation suggesting the forecasts are good.\nThese graphs show that the naïve method produces forecasts that appear to account\nfor all available information. The mean of the residuals is close to zero and there is\nno significant correlation in the residuals series. The time plot of the residuals shows\nthat the variation of the residuals stays much the same across the historical data,\napart from the one outlier, and therefore the residual variance can be treated as\nconstant. This can also be seen on the histogram of the residuals. The histogram\nsuggests that the residuals may not be normal — the right tail seems a little too\nlong, even when we ignore the outlier. Consequently, forecasts from this method will\nprobably be quite good, but prediction intervals that are computed assuming a\nnormal distribution may be inaccurate.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 156,
      "total_chunks": 873
    }
  },
  {
    "text": "probably be quite good, but prediction intervals that are computed assuming a\nnormal distribution may be inaccurate.\nPortmanteau tests for autocorrelationPortmanteau tests for autocorrelation\nIn addition to looking at the ACF plot, we can also do a more formal test for\nautocorrelation by considering a whole set of  values as a group, rather than\ntreating each one separately.\nRecall that  is the autocorrelation for lag . When we look at the ACF plot to see\nwhether each spike is within the required limits, we are implicitly carrying out\nmultiple hypothesis tests, each one with a small probability of giving a false positive.\nWhen enough of these tests are done, it is likely that at least one will give a false\npositive, and so we may conclude that the residuals have some remaining\nautocorrelation, when in fact they do not.\nIn order to overcome this problem, we test whether the first  autocorrelations are\nsignificantly different from what would be expected from a white noise process. A",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 157,
      "total_chunks": 873
    }
  },
  {
    "text": "In order to overcome this problem, we test whether the first  autocorrelations are\nsignificantly different from what would be expected from a white noise process. A\ntest for a group of autocorrelations is called a portmanteau test , from a French word\n88",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 158,
      "total_chunks": 873
    }
  },
  {
    "text": "describing a suitcase or coat rack carrying several items of clothing.\nOne such test is the Box-Pierce testBox-Pierce test , based on the following statistic\nwhere  is the maximum lag being considered and  is the number of observations. If\neach  is close to zero, then  will be small. If some  values are large (positive or\nnegative), then  will be large. We suggest using  for non-seasonal data and\n for seasonal data, where  is the period of seasonality. However, the test is\nnot good when  is large, so if these values are larger than , then use \nA related (and more accurate) test is the Ljung-Box test , based on\nAgain, large values of  suggest that the autocorrelations do not come from a white\nnoise series.\nHow large is too large? If the autocorrelations did come from a white noise series,\nthen both  and  would have a  distribution with  degrees of freedom. .\nIn the following code, ılagı .\n2\n# lag=h and fitdf=K\nBox.test(res, lag=10)\n#> \n#>  Box-Pierce test\n#> \n#> data:  res",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 159,
      "total_chunks": 873
    }
  },
  {
    "text": "then both  and  would have a  distribution with  degrees of freedom. .\nIn the following code, ılagı .\n2\n# lag=h and fitdf=K\nBox.test(res, lag=10)\n#> \n#>  Box-Pierce test\n#> \n#> data:  res\n#> X-squared = 11, df = 10, p-value = 0.4\nBox.test(res,lag=10, type=\"Lj\")\n#> \n#>  Box-Ljung test\n#> \n#> data:  res\n#> X-squared = 11, df = 10, p-value = 0.4\n89",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 160,
      "total_chunks": 873
    }
  },
  {
    "text": "For both  and , the results are not significant (i.e., the -values are relatively\nlarge). Thus, we can conclude that the residuals are not distinguishable from a white\nnoise series.\nAll of these methods for checking residuals are conveniently packaged into one R\nfunction ıcheckresiduals()ı, which will produce a time plot, ACF plot and histogram\nof the residuals (with an overlaid normal distribution for comparison), and do a\nLjung-Box test.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from Naive method\n#> Q* = 11, df = 10, p-value = 0.4\n#> \n#> Model df: 0.   Total lags used: 10\n2. For the ARIMA models discussed in chapters 8 and 9, the degrees of freedom is\nadjusted to give better results. ↩ \n/g13/g13 /gC7 /g2C\ncheckresiduals(naive(goog200))\n90\n\n\n91",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 161,
      "total_chunks": 873
    }
  },
  {
    "text": "3.43.4  Evaluating forecast accuracyEvaluating forecast accuracy\nTraining and test setsTraining and test sets\nIt is important to evaluate forecast accuracy using genuine forecasts. Consequently,\nthe size of the residuals is not a reliable indication of how large true forecast errors\nare likely to be. The accuracy of forecasts can only be determined by considering how\nwell a model performs on new data that were not used when fitting the model.\nWhen choosing models, it is common practice to separate the available data into two\nportions, trainingtraining and testtest data, where the training data is used to estimate any\nparameters of a forecasting method and the test data is used to evaluate its accuracy.\nBecause the test data is not used in determining the forecasts, it should provide a\nreliable indication of how well the model is likely to forecast on new data.\nThe size of the test set is typically about 20% of the total sample, although this value",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 162,
      "total_chunks": 873
    }
  },
  {
    "text": "reliable indication of how well the model is likely to forecast on new data.\nThe size of the test set is typically about 20% of the total sample, although this value\ndepends on how long the sample is and how far ahead you want to forecast. The test\nset should ideally be at least as large as the maximum forecast horizon required. The\nfollowing points should be noted.\nA model which fits the training data well will not necessarily forecast well.\nA perfect fit can always be obtained by using a model with enough parameters.\nOver-fitting a model to data is just as bad as failing to identify a systematic\npattern in the data.\nSome references describe the test set as the “hold-out set” because these data are\n“held out” of the data used for fitting. Other references call the training set the “in-\nsample data” and the test set the “out-of-sample data”. We prefer to use “training\ndata” and “test data” in this book.\n92",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 163,
      "total_chunks": 873
    }
  },
  {
    "text": "Functions to subset a time seriesFunctions to subset a time series\nThe ıwindow()ı function introduced in Chapter 2  is useful when extracting a portion\nof a time series, such as we need when creating training and test sets. In the\nıwindow()ı function, we specify the start and/or end of the portion of time series\nrequired using time values. For example,\nextracts all data from 1995 onward.\nAnother useful function is ısubset()ı which allows for more types of subsetting. A\ngreat advantage of this function is that it allows the use of indices to choose a subset.\nFor example,\nextracts the last 5 years of observations from ıausbeerı. It also allows extracting all\nvalues for a specific season. For example,\nextracts the first quarters for all years.\nFinally, ıheadı and ıtailı are useful for extracting the first few or last few\nobservations. For example, the last 5 years of ıausbeerı can also be obtained using\nForecast errorsForecast errors",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 164,
      "total_chunks": 873
    }
  },
  {
    "text": "Finally, ıheadı and ıtailı are useful for extracting the first few or last few\nobservations. For example, the last 5 years of ıausbeerı can also be obtained using\nForecast errorsForecast errors\nA forecast “error” is the difference between an observed value and its forecast. Here\n“error” does not mean a mistake, it means the unpredictable part of an observation.\nIt can be written as\nwhere the training data is given by  and the test data is given by\n.\nNote that forecast errors are different from residuals in two ways. First, residuals are\ncalculated on the training set while forecast errors are calculated on the test set.\nSecond, residuals are based on one-step forecasts while forecast errors can involve\nwindow(ausbeer, start=1995 )\nsubset(ausbeer, start=length(ausbeer)- 4*5)\nsubset(ausbeer, quarter = 1)\ntail(ausbeer, 4*5)\n93",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 165,
      "total_chunks": 873
    }
  },
  {
    "text": "multi-step  forecasts.\nWe can measure forecast accuracy by summarising the forecast errors in different\nways.\nScale-dependent errorsScale-dependent errors\nThe forecast errors are on the same scale as the data. Accuracy measures that are\nbased only on  are therefore scale-dependent and cannot be used to make\ncomparisons between series that involve different units.\nThe two most commonly used scale-dependent measures are based on the absolute\nerrors or squared errors:\nWhen comparing forecast methods applied to a single time series, or to several time\nseries with the same units, the MAE is popular as it is easy to both understand and\ncompute. A forecast method that minimises the MAE will lead to forecasts of the\nmedian, while minimising the RMSE will lead to forecasts of the mean.\nConsequently, the RMSE is also widely used, despite being more difficult to interpret.\nPercentage errorsPercentage errors\nThe percentage error is given by . Percentage errors have the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 166,
      "total_chunks": 873
    }
  },
  {
    "text": "Consequently, the RMSE is also widely used, despite being more difficult to interpret.\nPercentage errorsPercentage errors\nThe percentage error is given by . Percentage errors have the\nadvantage of being unit-free, and so are frequently used to compare forecast\nperformances between data sets. The most commonly used measure is:\nMeasures based on percentage errors have the disadvantage of being infinite or\nundefined if  for any  in the period of interest, and having extreme values if\nany  is close to zero. Another problem with percentage errors that is often\noverlooked is that they assume the unit of measurement has a meaningful zero.  For\nexample, a percentage error makes no sense when measuring the accuracy of\ntemperature forecasts on either the Fahrenheit or Celsius scales, because\ntemperature has an arbitrary zero point.\n3\n94",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 167,
      "total_chunks": 873
    }
  },
  {
    "text": "They also have the disadvantage that they put a heavier penalty on negative errors\nthan on positive errors. This observation led to the use of the so-called “symmetric”\nMAPE (sMAPE) proposed by Armstrong (1978, p. 348 ), which was used in the M3\nforecasting competition. It is defined by\nHowever, if  is close to zero,  is also likely to be close to zero. Thus, the measure\nstill involves division by a number close to zero, making the calculation unstable.\nAlso, the value of sMAPE can be negative, so it is not really a measure of “absolute\npercentage errors” at all.\nHyndman & Koehler ( 2006) recommend that the sMAPE not be used. It is included\nhere only because it is widely used, although we will not use it in this book.\nScaled errorsScaled errors\nScaled errors were proposed by Hyndman & Koehler ( 2006) as an alternative to using\npercentage errors when comparing forecast accuracy across series with different",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 168,
      "total_chunks": 873
    }
  },
  {
    "text": "Scaled errorsScaled errors\nScaled errors were proposed by Hyndman & Koehler ( 2006) as an alternative to using\npercentage errors when comparing forecast accuracy across series with different\nunits. They proposed scaling the errors based on the training MAE from a simple\nforecast method.\nFor a non-seasonal time series, a useful way to define a scaled error uses naïve\nforecasts:\nBecause the numerator and denominator both involve values on the scale of the\noriginal data,  is independent of the scale of the data. A scaled error is less than one\nif it arises from a better forecast than the average naïve forecast computed on the\ntraining data. Conversely, it is greater than one if the forecast is worse than the\naverage naïve forecast computed on the training data.\nFor seasonal time series, a scaled error can be defined using seasonal naïve\nforecasts:\n95",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 169,
      "total_chunks": 873
    }
  },
  {
    "text": "The mean absolute scaled error  is simply\nExamplesExamples\nFigure 3.9: Forecasts of Australian quarterly beer production using data up to the end of\n2007.\nFigure 3.9  shows three forecast methods applied to the quarterly Australian beer\nproduction using data only to the end of 2007. The actual values for the period\n2008–2010 are also shown. We compute the forecast accuracy measures for this\nperiod.\nbeer2 <- window(ausbeer,start=1992 ,end=c(2007,4))\nbeerfit1 <- meanf(beer2,h=10)\nbeerfit2 <- rwf(beer2,h=10)\nbeerfit3 <- snaive(beer2,h=10)\nautoplot(window(ausbeer, start=1992 )) +\n  autolayer(beerfit1, series=\"Mean\", PI=FALSE ) +\n  autolayer(beerfit2, series=\"Naïve\", PI=FALSE ) +\n  autolayer(beerfit3, series=\"Seasonal naïve\", PI=FALSE ) +\n  xlab(\"Year\") + ylab(\"Megalitres\") +\n  ggtitle(\"Forecasts for quarterly beer production\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n96",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 170,
      "total_chunks": 873
    }
  },
  {
    "text": "RMSERMSE MAEMAE MAPEMAPE MASEMASE\nMean method 38.45 34.83 8.28 2.44\nNaïve method 62.69 57.40 14.18 4.01\nSeasonal naïve method 14.31 13.40 3.17 0.94\nIt is obvious from the graph that the seasonal naïve method is best for these data,\nalthough it can still be improved, as we will discover later. Sometimes, different\naccuracy measures will lead to different results as to which forecast method is best.\nHowever, in this case, all of the results point to the seasonal naïve method as the\nbest of these three methods for this data set.\nTo take a non-seasonal example, consider the Google stock price. The following\ngraph shows the 200 observations ending on 6 Dec 2013, along with forecasts of the\nnext 40 days obtained from three different methods.\nbeer3 <- window(ausbeer, start=2008 )\naccuracy(beerfit1, beer3)\naccuracy(beerfit2, beer3)\naccuracy(beerfit3, beer3)\ngoogfc1 <- meanf(goog200, h=40)\ngoogfc2 <- rwf(goog200, h=40)\ngoogfc3 <- rwf(goog200, drift=TRUE , h=40)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 171,
      "total_chunks": 873
    }
  },
  {
    "text": "accuracy(beerfit1, beer3)\naccuracy(beerfit2, beer3)\naccuracy(beerfit3, beer3)\ngoogfc1 <- meanf(goog200, h=40)\ngoogfc2 <- rwf(goog200, h=40)\ngoogfc3 <- rwf(goog200, drift=TRUE , h=40)\nautoplot(subset(goog, end = 240)) +\n  autolayer(googfc1, PI=FALSE , series=\"Mean\") +\n  autolayer(googfc2, PI=FALSE , series=\"Naïve\") +\n  autolayer(googfc3, PI=FALSE , series=\"Drift\") +\n  xlab(\"Day\") + ylab(\"Closing Price (US$)\") +\n  ggtitle(\"Google stock price (daily ending 6 Dec 13)\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n97",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 172,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 3.10: Forecasts of the Google stock price from 7 Dec 2013.\nRMSERMSE MAEMAE MAPEMAPE MASEMASE\nMean method 114.21 113.27 20.32 30.28\nNaïve method 28.43 24.59 4.36 6.57\nDrift method 14.08 11.67 2.07 3.12\nHere, the best method is the drift method (regardless of which accuracy measure is\nused).\nTime series cross-validation\nA more sophisticated version of training/test sets is time series cross-validation. In\nthis procedure, there are a series of test sets, each consisting of a single observation.\nThe corresponding training set consists only of observations that occurred prior to\nthe observation that forms the test set. Thus, no future observations can be used in\nconstructing the forecast. Since it is not possible to obtain a reliable forecast based\non a small training set, the earliest observations are not considered as test sets.\nThe following diagram illustrates the series of training and test sets, where the blue",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 173,
      "total_chunks": 873
    }
  },
  {
    "text": "on a small training set, the earliest observations are not considered as test sets.\nThe following diagram illustrates the series of training and test sets, where the blue\nobservations form the training sets, and the red observations form the test sets.\ngoogtest <- window(goog, start=201, end= 240)\naccuracy(googfc1, googtest)\naccuracy(googfc2, googtest)\naccuracy(googfc3, googtest)\n98",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 174,
      "total_chunks": 873
    }
  },
  {
    "text": "The forecast accuracy is computed by averaging over the test sets. This procedure is\nsometimes known as “evaluation on a rolling forecasting origin” because the\n“origin” at which the forecast is based rolls forward in time.\nWith time series forecasting, one-step forecasts may not be as relevant as multi-\nstep forecasts. In this case, the cross-validation procedure based on a rolling\nforecasting origin can be modified to allow multi-step errors to be used. Suppose\nthat we are interested in models that produce good -step-ahead forecasts. Then the\ncorresponding diagram is shown below.\nTime series cross-validation is implemented with the ıtsCV()ı function. In the\nfollowing example, we compare the RMSE obtained via time series cross-validation\nwith the residual RMSE.\n/g15\n99",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 175,
      "total_chunks": 873
    }
  },
  {
    "text": "As expected, the RMSE from the residuals is smaller, as the corresponding\n“forecasts” are based on a model fitted to the entire data set, rather than being true\nforecasts.\nA good way to choose the best forecasting model is to find the model with the\nsmallest RMSE computed using time series cross-validation.\nPipe operatorPipe operator\nThe ugliness of the above R code makes this a good opportunity to introduce some\nalternative ways of stringing R functions together. In the above code, we are nesting\nfunctions within functions within functions, so you have to read the code from the\ninside out, making it difficult to understand what is being computed. Instead, we can\nuse the pipe operator ı%>%ı as follows.\nThe left hand side of each pipe is passed as the first argument to the function on the\nright hand side. This is consistent with the way we read from left to right in English.\nWhen using pipes, all other arguments must be named, which also helps readability.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 176,
      "total_chunks": 873
    }
  },
  {
    "text": "right hand side. This is consistent with the way we read from left to right in English.\nWhen using pipes, all other arguments must be named, which also helps readability.\nWhen using pipes, it is natural to use the right arrow assignment ı->ı rather than the\nleft arrow. For example, the third line above can be read as “Take the ıgoog200ı\nseries, pass it to ırwf()ı with ıdrift=TRUEı, compute the resulting residuals, and\nstore them as ıresı”.\nWe will use the pipe operator whenever it makes the code easier to read. In order to\nbe consistent, we will always follow a function with parentheses to differentiate it\nfrom other objects, even if it has no arguments. See, for example, the use of ısqrt()ı\nand ıresiduals()ı in the code above.\ne <- tsCV(goog200, rwf, drift=TRUE , h=1)\nsqrt(mean(e^2, na.rm=TRUE))\n#> [1] 6.233\nsqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))\n#> [1] 6.169\ngoog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e\ne^2 %>% mean(na.rm=TRUE) %>% sqrt()",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 177,
      "total_chunks": 873
    }
  },
  {
    "text": "#> [1] 6.233\nsqrt(mean(residuals(rwf(goog200, drift=TRUE))^2, na.rm=TRUE))\n#> [1] 6.169\ngoog200 %>% tsCV(forecastfunction=rwf, drift=TRUE, h=1) -> e\ne^2 %>% mean(na.rm=TRUE) %>% sqrt()\n#> [1] 6.233\ngoog200 %>% rwf(drift=TRUE) %>% residuals() -> res\nres^2 %>% mean(na.rm=TRUE) %>% sqrt()\n#> [1] 6.169\n100",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 178,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: using tsCV()Example: using tsCV()\nThe ıgoog200ı data, plotted in Figure 3.5, includes daily closing stock price of Google\nInc from the NASDAQ exchange for 200 consecutive trading days starting on 25\nFebruary 2013.\nThe code below evaluates the forecasting performance of 1- to 8-step-ahead naïve\nforecasts with ıtsCV()ı, using MSE as the forecast error measure. The plot shows\nthat the forecast error increases as the forecast horizon increases, as we would\nexpect.\nBibliographyBibliography\nArmstrong, J. S. (1978). Long-range forecasting: From crystal ball to computer .\nJohn Wiley & Sons. [Amazon]\ne <- tsCV(goog200, forecastfunction=naive, h=8 )\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = T)\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:8, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()\n101",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 179,
      "total_chunks": 873
    }
  },
  {
    "text": "Hyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast\naccuracy. International Journal of Forecasting , 22(4), 679–688. [DOI]\n3. That is, a percentage is valid on a ratio scale, but not on an interval scale. Only\nratio scale variables have meaningful zeros. ↩ \n102",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 180,
      "total_chunks": 873
    }
  },
  {
    "text": "3.53.5  Prediction intervalsPrediction intervals\nAs discussed in Section 1.7, a prediction interval gives an interval within which we\nexpect  to lie with a specified probability. For example, assuming that the forecast\nerrors are normally distributed, a 95% prediction interval for the -step forecast is\nwhere  is an estimate of the standard deviation of the -step forecast distribution.\nMore generally, a prediction interval can be written as\nwhere the multiplier  depends on the coverage probability. In this book we usually\ncalculate 80% intervals and 95% intervals, although any percentage may be used.\nThe following table gives the value of  for a range of coverage probabilities\nassuming normally distributed forecast errors.\nTable 3.1: Multipliers to be used for prediction intervals.\nPercentagePercentage MultiplierMultiplier\n50 0.67\n55 0.76\n60 0.84\n65 0.93\n70 1.04\n75 1.15\n80 1.28\n85 1.44\n90 1.64\n95 1.96\n96 2.05\n97 2.17\n98 2.33\n99 2.58\n103",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 181,
      "total_chunks": 873
    }
  },
  {
    "text": "The value of prediction intervals is that they express the uncertainty in the forecasts.\nIf we only produce point forecasts, there is no way of telling how accurate the\nforecasts are. However, if we also produce prediction intervals, then it is clear how\nmuch uncertainty is associated with each forecast. For this reason, point forecasts\ncan be of almost no value without the accompanying prediction intervals.\nOne-step prediction intervalsOne-step prediction intervals\nWhen forecasting one step ahead, the standard deviation of the forecast distribution\nis almost the same as the standard deviation of the residuals. (In fact, the two\nstandard deviations are identical if there are no parameters to be estimated, as is the\ncase with the naïve method. For forecasting methods involving parameters to be\nestimated, the standard deviation of the forecast distribution is slightly larger than\nthe residual standard deviation, although this difference is often ignored.)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 182,
      "total_chunks": 873
    }
  },
  {
    "text": "estimated, the standard deviation of the forecast distribution is slightly larger than\nthe residual standard deviation, although this difference is often ignored.)\nFor example, consider a naïve forecast for the Google stock price data ıgoog200ı\n(shown in Figure 3.5). The last value of the observed series is 531.48, so the forecast\nof the next value of the GSP is 531.48. The standard deviation of the residuals from\nthe naïve method is 6.21. Hence, a 95% prediction interval for the next value of the\nGSP is\nSimilarly, an 80% prediction interval is given by\nThe value of the multiplier (1.96 or 1.28) is taken from Table 3.1.\nMulti-step prediction intervalsMulti-step prediction intervals\nA common feature of prediction intervals is that they increase in length as the\nforecast horizon increases. The further ahead we forecast, the more uncertainty is\nassociated with the forecast, and thus the wider the prediction intervals. That is,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 183,
      "total_chunks": 873
    }
  },
  {
    "text": "forecast horizon increases. The further ahead we forecast, the more uncertainty is\nassociated with the forecast, and thus the wider the prediction intervals. That is, \nusually increases with  (although there are some non-linear forecasting methods\nthat do not have this property).\nTo produce a prediction interval, it is necessary to have an estimate of . As already\nnoted, for one-step forecasts ( ), the residual standard deviation provides a\ngood estimate of the forecast standard deviation . For multi-step forecasts, a more\n104",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 184,
      "total_chunks": 873
    }
  },
  {
    "text": "complicated method of calculation is required. These calculations assume that the\nresiduals are uncorrelated.\nBenchmark methodsBenchmark methods\nFor the four benchmark methods, it is possible to mathematically derive the forecast\nstandard deviation under the assumption of uncorrelated residuals. If  denotes the\nstandard deviation of the -step forecast distribution, and  is the residual standard\ndeviation, then we can use the following expressions.\nMean forecastsMean forecasts : \nNaïve forecastsNaïve forecasts : \nSeasonal naïve forecastsSeasonal naïve forecasts  , where  is the integer part of \nand  is the seasonal period.\nDrift forecastsDrift forecasts : .\nNote that when  and  is large, these all give the same approximate value .\nPrediction intervals will be computed for you when using any of the benchmark\nforecasting methods. For example, here is the output when using the naïve method\nfor the Google stock price.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 185,
      "total_chunks": 873
    }
  },
  {
    "text": "Prediction intervals will be computed for you when using any of the benchmark\nforecasting methods. For example, here is the output when using the naïve method\nfor the Google stock price.\nWhen plotted, the prediction intervals are shown as shaded region, with the strength\nof colour indicating the probability associated with the interval.\nnaive(goog200)\n#>     Point Forecast Lo 80 Hi 80 Lo 95 Hi 95\n#> 201          531.5 523.5 539.4 519.3 543.6\n#> 202          531.5 520.2 542.7 514.3 548.7\n#> 203          531.5 517.7 545.3 510.4 552.6\n#> 204          531.5 515.6 547.4 507.1 555.8\n#> 205          531.5 513.7 549.3 504.3 558.7\n#> 206          531.5 512.0 551.0 501.7 561.3\n#> 207          531.5 510.4 552.5 499.3 563.7\n#> 208          531.5 509.0 554.0 497.1 565.9\n#> 209          531.5 507.6 555.3 495.0 568.0\n#> 210          531.5 506.3 556.6 493.0 570.0\nautoplot(naive(goog200))\n105",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 186,
      "total_chunks": 873
    }
  },
  {
    "text": "Prediction intervals from bootstrapped residualsPrediction intervals from bootstrapped residuals\nWhen a normal distribution for the forecast errors is an unreasonable assumption,\none alternative is to use bootstrapping, which only assumes that the forecast errors\nare uncorrelated.\nA forecast error is defined as . We can re-write this as\nSo we can simulate the next observation of a time series using\nwhere  is the one-step forecast and  is the unknown future error.\nAssuming future errors will be similar to past errors, we can replace  by\nsampling from the collection of errors we have seen in the past (i.e., the residuals).\nAdding the new simulated observation to our data set, we can repeat the process to\nobtain\nwhere  is another draw from the collection of residuals. Continuing in this way,\nwe can simulate an entire set of future values for our time series.\n106",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 187,
      "total_chunks": 873
    }
  },
  {
    "text": "Doing this repeatedly, we obtain many possible futures. Then we can compute\nprediction intervals by calculating percentiles for each forecast horizon. The result is\ncalled a bootstrappedbootstrapped  prediction interval. The name “bootstrap” is a reference to\npulling ourselves up by our bootstraps, because the process allows us to measure\nfuture uncertainty by only using the historical data.\nTo generate such intervals, we can simply add the ıbootstrapı argument to our\nforecasting functions. For example:\nIn this case, they are similar (but not identical) to the prediction intervals based on\nthe normal distribution.\nPrediction intervals with transformationsPrediction intervals with transformations\nIf a transformation has been used, then the prediction interval should be computed\non the transformed scale, and the end points back-transformed to give a prediction\ninterval on the original scale. This approach preserves the probability coverage of the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 188,
      "total_chunks": 873
    }
  },
  {
    "text": "on the transformed scale, and the end points back-transformed to give a prediction\ninterval on the original scale. This approach preserves the probability coverage of the\nprediction interval, although it will no longer be symmetric around the point\nforecast.\nThe back-transformation of prediction intervals is done automatically using the\nfunctions in the forecastforecast package in R, provided you have used the ılambdaı argument\nwhen computing the forecasts.\nnaive(goog200, bootstrap=TRUE )\n#>     Point Forecast Lo 80 Hi 80 Lo 95 Hi 95\n#> 201          531.5 525.7 537.8 522.9 542.9\n#> 202          531.5 523.2 539.5 519.4 547.0\n#> 203          531.5 520.9 541.2 516.7 552.3\n#> 204          531.5 519.0 543.0 514.0 560.3\n#> 205          531.5 517.5 544.6 511.8 582.1\n#> 206          531.5 516.1 545.9 509.5 582.4\n#> 207          531.5 514.8 547.3 508.0 583.5\n#> 208          531.5 513.5 548.9 505.8 584.9\n#> 209          531.5 512.3 549.8 503.9 586.6\n#> 210          531.5 510.7 551.4 502.1 587.5",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 189,
      "total_chunks": 873
    }
  },
  {
    "text": "#> 207          531.5 514.8 547.3 508.0 583.5\n#> 208          531.5 513.5 548.9 505.8 584.9\n#> 209          531.5 512.3 549.8 503.9 586.6\n#> 210          531.5 510.7 551.4 502.1 587.5\n107",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 190,
      "total_chunks": 873
    }
  },
  {
    "text": "3.63.6  The forecast package in RThe forecast package in R\nThis book uses the facilities in the forecastforecast package in R (which is loaded\nautomatically whenever you load the fpp2fpp2 package). This appendix briefly\nsummarises some of the features of the package. Please refer to the help files for\nindividual functions to learn more, and to see some examples of their use.\nFunctions that output a forecast object:Functions that output a forecast object:\nMany functions, including ımeanf()ı, ınaive()ı, ısnaive()ı and ırwf()ı, produce\noutput in the form of a ıforecastı object (i.e., an object of class ıforecastı). This\nallows other functions (such as ıautoplot()ı) to work consistently across a range of\nforecasting models.\nObjects of class ıforecastı contain information about the forecasting method, the\ndata used, the point forecasts obtained, prediction intervals, residuals and fitted\nvalues. There are several functions designed to work with these objects including",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 191,
      "total_chunks": 873
    }
  },
  {
    "text": "data used, the point forecasts obtained, prediction intervals, residuals and fitted\nvalues. There are several functions designed to work with these objects including\nıautoplot()ı, ısummary()ı and ıprint()ı.\nThe following list shows all the functions that produce ıforecastı objects.\nımeanf()ı\nınaive()ı, ısnaive()ı\nırwf()ı\nıcroston()ı\nıstlf()ı\nıses()ı\nıholt()ı, ıhw()ı\nısplinef()ı\nıthetaf()ı\nıforecast()ı\n108",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 192,
      "total_chunks": 873
    }
  },
  {
    "text": "ĳforecast()ĳ functionfunction\nSo far we have used functions which produce a ıforecastı object directly. But a more\ncommon approach, which we will focus on in the rest of the book, will be to fit a\nmodel to the data, and then use the ıforecast()ı function to produce forecasts from\nthat model.\nThe ıforecast()ı function works with many different types of inputs. It generally\ntakes a time series or time series model as its main argument, and produces\nforecasts appropriately. It always returns objects of class ıforecastı.\nIf the first argument is of class ıtsı, it returns forecasts from the automatic ETS\nalgorithm discussed in Chapter 7.\nHere is a simple example, applying ıforecast()ı to the ıausbeerı data:\nThat works quite well if you have no idea what sort of model to use. But by the end of\nthis book, you should not need to use ıforecast()ı in this “blind” fashion. Instead,\nyou will fit a model appropriate to the data, and then use ıforecast()ı to produce\nforecasts from that model.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 193,
      "total_chunks": 873
    }
  },
  {
    "text": "this book, you should not need to use ıforecast()ı in this “blind” fashion. Instead,\nyou will fit a model appropriate to the data, and then use ıforecast()ı to produce\nforecasts from that model.\nforecast(ausbeer, h=4 )\n#>         Point Forecast Lo 80 Hi 80 Lo 95 Hi 95\n#> 2010 Q3          404.6 385.9 423.3 376.0 433.3\n#> 2010 Q4          480.4 457.5 503.3 445.4 515.4\n#> 2011 Q1          417.0 396.5 437.6 385.6 448.4\n#> 2011 Q2          383.1 363.5 402.7 353.1 413.1\n109",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 194,
      "total_chunks": 873
    }
  },
  {
    "text": "3.73.7  ExercisesExercises\n1. For the following series, find an appropriate Box-Cox transformation in order to\nstabilise the variance.\nıusnetelecı\nıusgdpı\nımcopperı\nıenplanementsı\n2. Why is a Box-Cox transformation unhelpful for the ıcangası data?\n3. What Box-Cox transformation would you select for your retail data (from\nExercise 3 in Section 2.10)?\n4. For each of the following series, make a graph of the data. If transforming seems\nappropriate, do so and describe the effect. ıdoleı, ıusdeathsı, ıbricksqı.\n5. Calculate the residuals from a seasonal naïve forecast applied to the quarterly\nAustralian beer production data from 1992. The following code will help.\nTest if the residuals are white noise and normally distributed.\nWhat do you conclude?\n6. Repeat the exercise for the ıWWWusageı and ıbricksqı data. Use whichever of\nınaive()ı or ısnaive()ı is more appropriate in each case.\n7. Are the following statements true or false? Explain your answer.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 195,
      "total_chunks": 873
    }
  },
  {
    "text": "ınaive()ı or ısnaive()ı is more appropriate in each case.\n7. Are the following statements true or false? Explain your answer.\na. Good forecast methods should have normally distributed residuals.\nb. A model with small residuals will give good forecasts.\nc. The best measure of forecast accuracy is MAPE.\nbeer <- window(ausbeer, start=1992 )\nfc <- snaive(beer)\nautoplot(fc)\nres <- residuals(fc)\nautoplot(res)\ncheckresiduals(fc)\n110",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 196,
      "total_chunks": 873
    }
  },
  {
    "text": "d. If your model doesn’t forecast well, you should make it more complicated.\ne. Always choose the model with the best forecast accuracy as measured on the\ntest set.\n8. For your retail time series (from Exercise 3 in Section 2.10):\na. Split the data into two parts using\nb. Check that your data have been split appropriately by producing the\nfollowing plot.\nc. Calculate forecasts using ısnaiveı applied to ımyts.trainı.\nd. Compare the accuracy of your forecasts against the actual values stored in\nımyts.testı.\ne. Check the residuals.\nDo the residuals appear to be uncorrelated and normally distributed?\nf. How sensitive are the accuracy measures to the training/test split?\n9. ıvisnightsı contains quarterly visitor nights (in millions) from 1998 to 2016 for\ntwenty regions of Australia.\na. Use ıwindow()ı to create three training sets for ıvisnights[,\"QLDMetro\"], ı\nomitting the last 1, 2 and 3 years; call these train1, train2, and train3,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 197,
      "total_chunks": 873
    }
  },
  {
    "text": "twenty regions of Australia.\na. Use ıwindow()ı to create three training sets for ıvisnights[,\"QLDMetro\"], ı\nomitting the last 1, 2 and 3 years; call these train1, train2, and train3,\nrespectively. For example ıtrain1 <- window(visnights[, \"QLDMetro\"], end =\nc(2015, 4))ı.\nb. Compute one year of forecasts for each training set using the ısnaive()ı\nmethod. Call these ıfc1ı, ıfc2ı and ıfc3ı, respectively.\nmyts.train <- window(myts, end=c(2010,12))\nmyts.test <- window(myts, start=2011 )\nautoplot(myts) +\n  autolayer(myts.train, series=\"Training\") +\n  autolayer(myts.test, series=\"Test\")\nfc <- snaive(myts.train)\naccuracy(fc,myts.test)\ncheckresiduals(fc)\n111",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 198,
      "total_chunks": 873
    }
  },
  {
    "text": "c. Use ıaccuracy()ı to compare the MAPE over the three test sets. Comment on\nthese.\n10. Use the Dow Jones index (data set ıdowjonesı) to do the following:\na. Produce a time plot of the series.\nb. Produce forecasts using the drift method and plot them.\nc. Show that the forecasts are identical to extending the line drawn between the\nfirst and last observations.\nd. Try using some of the other benchmark functions to forecast the same data\nset. Which do you think is best? Why?\n11. Consider the daily closing IBM stock prices (data set ıibmcloseı).\na. Produce some plots of the data in order to become familiar with it.\nb. Split the data into a training set of 300 observations and a test set of 69\nobservations.\nc. Try using various benchmark methods to forecast the training set and\ncompare the results on the test set. Which method did best?\nd. Check the residuals of your preferred method. Do they resemble white noise?",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 199,
      "total_chunks": 873
    }
  },
  {
    "text": "compare the results on the test set. Which method did best?\nd. Check the residuals of your preferred method. Do they resemble white noise?\n12. Consider the sales of new one-family houses in the USA, Jan 1973 – Nov 1995\n(data set ıhsalesı).\na. Produce some plots of the data in order to become familiar with it.\nb. Split the ıhsalesı data set into a training set and a test set, where the test set\nis the last two years of data.\nc. Try using various benchmark methods to forecast the training set and\ncompare the results on the test set. Which method did best?\nd. Check the residuals of your preferred method. Do they resemble white noise?\n112",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 200,
      "total_chunks": 873
    }
  },
  {
    "text": "3.83.8  Further readingFurther reading\nOrd et al. (2017) provides further discussion of simple benchmark forecasting\nmethods.\nA review of forecast evaluation methods is given in Hyndman & Koehler ( 2006),\nlooking at the strengths and weaknesses of different approaches. This is the\npaper that introduced the MASE as a general-purpose forecast accuracy measure.\nBibliographyBibliography\nHyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast\naccuracy. International Journal of Forecasting , 22(4), 679–688. [DOI]\nOrd, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\n113",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 201,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 4Chapter 4  Judgmental forecastsJudgmental forecasts\nForecasting using judgement is common in practice. In many cases, judgmental\nforecasting is the only option, such as when there is a complete lack of historical\ndata, or when a new product is being launched, or when a new competitor enters the\nmarket, or during completely new and unique market conditions. For example, in\nDecember 2012, the Australian government was the first in the world to pass\nlegislation that banned the use of company logos on cigarette packets, and required\nall cigarette packets to be a dark green colour. Judgement must be applied in order to\nforecast the effect of such a policy, as there are no historical precedents.\nThere are also situations where the data are incomplete, or only become available\nafter some delay. For example, central banks include judgement when forecasting\nthe current level of economic activity, a procedure known as nowcasting, as GDP is\nonly available on a quarterly basis.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 202,
      "total_chunks": 873
    }
  },
  {
    "text": "after some delay. For example, central banks include judgement when forecasting\nthe current level of economic activity, a procedure known as nowcasting, as GDP is\nonly available on a quarterly basis.\nResearch in this area  has shown that the accuracy of judgmental forecasting\nimproves when the forecaster has (i) important domain knowledge, and (ii) more\ntimely, up-to-date information. A judgmental approach can be quick to adjust to\nsuch changes, information or events.\nOver the years, the acceptance of judgmental forecasting as a science has increased,\nas has the recognition of its need. More importantly, the quality of judgmental\nforecasts has also improved, as a direct result of recognising that improvements in\njudgmental forecasting can be achieved by implementing well-structured and\nsystematic approaches. It is important to recognise that judgmental forecasting is\nsubjective and comes with limitations. However, implementing systematic and well-",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 203,
      "total_chunks": 873
    }
  },
  {
    "text": "systematic approaches. It is important to recognise that judgmental forecasting is\nsubjective and comes with limitations. However, implementing systematic and well-\nstructured approaches can confine these limitations and markedly improve forecast\naccuracy.\nThere are three general settings in which judgmental forecasting is used: (i) there\nare no available data, so that statistical methods are not applicable and judgmental\nforecasting is the only feasible approach; (ii) data are available, statistical forecasts\nare generated, and these are then adjusted using judgement; and (iii) data are\navailable and statistical and judgmental forecasts are generated independently and\nthen combined. We should clarify that when data are available, applying statistical\n4\n114",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 204,
      "total_chunks": 873
    }
  },
  {
    "text": "methods (such as those discussed in other chapters of this book), is preferable and\nshould always be used as a starting point. Statistical forecasts are generally superior\nto generating forecasts using only judgement. For the majority of the chapter, we\nfocus on the first setting where no data are available, and in the last section we\ndiscuss the judgmental adjustment of statistical forecasts. We discuss combining\nforecasts in Section 12.4.\nBibliographyBibliography\nLawrence, M., Goodwin, P., O’Connor, M., & Önkal, D. (2006). Judgmental\nforecasting: A review of progress over the last 25 years. International Journal of\nForecasting , 22(3), 493–518. [DOI]\n4. Lawrence, Goodwin, O’Connor, & Önkal (2006 )↩ \n115",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 205,
      "total_chunks": 873
    }
  },
  {
    "text": "4.14.1  Beware of limitationsBeware of limitations\nJudgmental forecasts are subjective, and therefore do not come free of bias or\nlimitations.\nJudgmental forecasts can be inconsistent. Unlike statistical forecasts, which can be\ngenerated by the same mathematical formulas every time, judgmental forecasts\ndepend heavily on human cognition, and are vulnerable to its limitations. For\nexample, a limited memory may render recent events more important than they\nactually are and may ignore momentous events from the more distant past; or a\nlimited attention span may result in important information being missed; or a\nmisunderstanding of causal relationships may lead to erroneous inferences.\nFurthermore, human judgement can vary due to the effect of psychological factors.\nOne can imagine a manager who is in a positive frame of mind one day, generating\nforecasts that may tend to be somewhat optimistic, and in a negative frame of mind\nanother day, generating somewhat less optimistic forecasts.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 206,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasts that may tend to be somewhat optimistic, and in a negative frame of mind\nanother day, generating somewhat less optimistic forecasts.\nJudgement can be clouded by personal or political agendas, where targets and\nforecasts (as defined in Chapter 1 ) are not segregated. For example, if a sales\nmanager knows that the forecasts she generates will be used to set sales\nexpectations (targets), she may tend to set these low in order to show a good\nperformance (i.e., exceed the expected targets). Even in cases where targets and\nforecasts are well segregated, judgement may be plagued by optimism or wishful\nthinking. For example, it would be highly unlikely that a team working towards\nlaunching a new product would forecast its failure. As we will discuss later, this\noptimism can be accentuated in a group meeting setting. “Beware of the enthusiasm\nof your marketing and sales colleagues” .\nAnother undesirable property which is commonly seen in judgmental forecasting is",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 207,
      "total_chunks": 873
    }
  },
  {
    "text": "of your marketing and sales colleagues” .\nAnother undesirable property which is commonly seen in judgmental forecasting is\nthe effect of anchoring. In this case, the subsequent forecasts tend to converge or be\nclose to an initial familiar reference point. For example, it is common to take the last\nobserved value as a reference point. The forecaster is influenced unduly by prior\ninformation, and therefore gives this more weight in the forecasting process.\nAnchoring may lead to conservatism and undervaluing new and more current\ninformation, and thereby create a systematic bias.\n5\n116",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 208,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nFildes, R., & Goodwin, P. (2007b). Good and bad judgment in forecasting: Lessons\nfrom four companies. Foresight: The International Journal of Applied\nForecasting , (8), 5–10.\n5. Fildes & Goodwin ( 2007b)↩ \n117",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 209,
      "total_chunks": 873
    }
  },
  {
    "text": "4.24.2  Key principlesKey principles\nUsing a systematic and well structured approach in judgmental forecasting helps to\nreduce the adverse effects of the limitations of judgmental forecasting, some of\nwhich we listed in the previous section. Whether this approach involves one\nindividual or many, the following principles should be followed.\nSet the forecasting task clearly and conciselySet the forecasting task clearly and concisely\nCare is needed when setting the forecasting challenges and expressing the\nforecasting tasks. It is important that everyone be clear about what the task is. All\ndefinitions should be clear and comprehensive, avoiding ambiguous and vague\nexpressions. Also, it is important to avoid incorporating emotive terms and\nirrelevant information that may distract the forecaster. In the Delphi method that\nfollows (see Section 4.3), it may sometimes be useful to conduct a preliminary round\nof information gathering before setting the forecasting task.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 210,
      "total_chunks": 873
    }
  },
  {
    "text": "follows (see Section 4.3), it may sometimes be useful to conduct a preliminary round\nof information gathering before setting the forecasting task.\nImplement a systematic approachImplement a systematic approach\nForecast accuracy and consistency can be improved by using a systematic approach\nto judgmental forecasting involving checklists of categories of information which\nare relevant to the forecasting task. For example, it is helpful to identify what\ninformation is important and how this information is to be weighted. When\nforecasting the demand for a new product, what factors should we account for and\nhow should we account for them? Should it be the price, the quality and/or quantity\nof the competition, the economic environment at the time, the target population of\nthe product? It is worthwhile to devote significant effort and resources to put\ntogether decision rules that will lead to the best possible systematic approach.\nDocument and justifyDocument and justify",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 211,
      "total_chunks": 873
    }
  },
  {
    "text": "together decision rules that will lead to the best possible systematic approach.\nDocument and justifyDocument and justify\nFormalising and documenting the decision rules and assumptions implemented in\nthe systematic approach can promote consistency, as the same rules can be\nimplemented repeatedly. Also, requesting a forecaster to document and justify their\n118",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 212,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasts leads to accountability, which can lead to reduced bias. Furthermore,\nformal documentation aids significantly in the systematic evaluation process that is\nsuggested next.\nSystematically evaluate forecastsSystematically evaluate forecasts\nSystematically monitoring the forecasting process can identify unforeseen\nirregularities. In particular, keep records of forecasts and use them to obtain\nfeedback when the corresponding observations become available. Although you may\ndo your best as a forecaster, the environment you operate in is dynamic. Changes\noccur, and you need to monitor these in order to evaluate the decision rules and\nassumptions. Feedback and evaluation help forecasters learn and improve their\nforecast accuracy.\nSegregate forecasters and usersSegregate forecasters and users\nForecast accuracy may be impeded if the forecasting task is carried out by users of\nthe forecasts, such as those responsible for implementing plans of action about",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 213,
      "total_chunks": 873
    }
  },
  {
    "text": "Forecast accuracy may be impeded if the forecasting task is carried out by users of\nthe forecasts, such as those responsible for implementing plans of action about\nwhich the forecast is concerned. We should clarify again here (as in Section 1.2), that\nforecasting is about predicting the future as accurately as possible, given all of the\ninformation available, including historical data and knowledge of any future events\nthat may impact the forecasts. Forecasters and users should be clearly segregated. A\nclassic case is that of a new product being launched. The forecast should be a\nreasonable estimate of the sales volume of a new product, which may differ\nconsiderably from what management expects or hopes the sales will be in order to\nmeet company financial objectives. In this case, a forecaster may be delivering a\nreality check to the user.\nIt is important that forecasters communicate forecasts to potential users thoroughly.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 214,
      "total_chunks": 873
    }
  },
  {
    "text": "meet company financial objectives. In this case, a forecaster may be delivering a\nreality check to the user.\nIt is important that forecasters communicate forecasts to potential users thoroughly.\nAs we will see in Section 4.7, users may feel distant and disconnected from forecasts,\nand may not have full confidence in them. Explaining and clarifying the process and\njustifying the basic assumptions that led to the forecasts will provide some\nassurance to users.\nThe way in which forecasts may then be used and implemented will clearly depend\non managerial decision making. For example, management may decide to adjust a\nforecast upwards (be over-optimistic), as the forecast may be used to guide\npurchasing and stock keeping levels. Such a decision may be taken after a cost-\n119",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 215,
      "total_chunks": 873
    }
  },
  {
    "text": "benefit analysis reveals that the cost of holding excess stock is much lower than that\nof lost sales. This type of adjustment should be part of setting goals or planning\nsupply, rather than part of the forecasting process. In contrast, if forecasts are used\nas targets, they may be set low so that they can be exceeded more easily. Again,\nsetting targets is different from producing forecasts, and the two should not be\nconfused.\nThe example that follows comes from our experience in industry. It exemplifies two\ncontrasting styles of judgmental forecasting — one that adheres to the principles we\nhave just presented and one that does not.\nExample: Pharmaceutical Benefits Scheme (PBS)Example: Pharmaceutical Benefits Scheme (PBS)\nThe Australian government subsidises the cost of a wide range of prescription\nmedicines as part of the PBS. Each subsidised medicine falls into one of four\ncategories: concession copayments, concession safety net, general copayments, and",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 216,
      "total_chunks": 873
    }
  },
  {
    "text": "medicines as part of the PBS. Each subsidised medicine falls into one of four\ncategories: concession copayments, concession safety net, general copayments, and\ngeneral safety net. Each person with a concession card makes a concession\ncopayment per PBS medicine ($5.80) , until they reach a set threshold amount\nlabelled the concession safety net ($348). For the rest of the financial year, all PBS-\nlisted medicines are free. Each general patient makes a general copayment per PBS\nmedicine ($35.40) until the general safety net amount is reached ($1,363.30). For the\nrest of the financial year, they contribute a small amount per PBS-listed medicine\n($5.80). The PBS forecasting process uses 84 groups of PBS-listed medicines, and\nproduces forecasts of the medicine volume and the total expenditure for each group\nand for each of the four PBS categories, a total of 672 series. This forecasting process\naids in setting the government budget allocated to the PBS, which is over $7 billion",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 217,
      "total_chunks": 873
    }
  },
  {
    "text": "and for each of the four PBS categories, a total of 672 series. This forecasting process\naids in setting the government budget allocated to the PBS, which is over $7 billion\nper year, or approximately 1% of GDP.\n6\n120",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 218,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 4.1: Process for producing PBS forecasts.\nFigure 4.1 summarises the forecasting process. Judgmental forecasts are generated\nfor new listings of medicines and for estimating the impact of new policies. These\nare shown by the green items. The pink items indicate the data used which were\nobtained from various government departments and associated authorities. The blue\nitems show things that are calculated from the data provided. There were judgmental\nadjustments to the data to take account of new listings and new policies, and there\nwere also judgmental adjustments to the forecasts. Because of the changing size of\nboth the concession population and the total population, forecasts are produced on a\nper-capita basis, and then multiplied by the forecast population to obtain forecasts\nof total volume and expenditure per month.\nOne of us (Hyndman) was asked to evaluate the forecasting process a few years ago.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 219,
      "total_chunks": 873
    }
  },
  {
    "text": "of total volume and expenditure per month.\nOne of us (Hyndman) was asked to evaluate the forecasting process a few years ago.\nWe found that using judgement for new listings and new policy impacts gave better\nforecasts than using a statistical model alone. However, we also found that the\n121",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 220,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasting accuracy and consistency could be improved through a more structured\nand systematic process, especially for policy impacts.\nForecasting new listings:  Companies who apply for their medicine to be added to the\nPBS are asked to submit detailed forecasts for various aspects of the medicine, such\nas projected patient numbers, market share of the new medicine, substitution\neffects, etc. The Pharmaceutical Benefits Advisory Committee provides guidelines\ndescribing a highly structured and systematic approach for generating these\nforecasts, and requires careful documentation for each step of the process. This\nstructured process helps to reduce the likelihood and effects of deliberate self-\nserving biases. Two detailed evaluation rounds of the company forecasts are\nimplemented by a sub-committee, one before the medicine is added to the PBS and\none after it is added. Finally, comparisons of observations versus forecasts for some",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 221,
      "total_chunks": 873
    }
  },
  {
    "text": "implemented by a sub-committee, one before the medicine is added to the PBS and\none after it is added. Finally, comparisons of observations versus forecasts for some\nselected new listings are performed, 12 months and 24 months after the listings, and\nthe results are sent back to the companies for comment.\nPolicy impact forecasts:  In contrast to the highly structured process used for new\nlistings, there were no systematic procedures for policy impact forecasts. On many\noccasions, forecasts of policy impacts were calculated by a small team, and were\noften heavily reliant on the work of one person. The forecasts were not usually\nsubject to a formal review process. There were no guidelines for how to construct\njudgmental forecasts for policy impacts, and there was often a lack of adequate\ndocumentation about how these forecasts were obtained, the assumptions\nunderlying them, etc.\nConsequently, we recommended several changes:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 222,
      "total_chunks": 873
    }
  },
  {
    "text": "documentation about how these forecasts were obtained, the assumptions\nunderlying them, etc.\nConsequently, we recommended several changes:\nthat guidelines for forecasting new policy impacts be developed, to encourage a\nmore systematic and structured forecasting approach;\nthat the forecast methodology be documented in each case, including all\nassumptions made in forming the forecasts;\nthat new policy forecasts be made by at least two people from different areas of\nthe organisation;\nthat a review of forecasts be conducted one year after the implementation of\neach new policy by a review committee, especially for new policies that have a\nsignificant annual projected cost or saving. The review committee should include\nthose involved in generating the forecasts, but also others.\nThese recommendations reflect the principles outlined in this section.\n122",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 223,
      "total_chunks": 873
    }
  },
  {
    "text": "6. These are Australian dollar amounts published by the Australian government for\n2012.↩ \n123",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 224,
      "total_chunks": 873
    }
  },
  {
    "text": "4.34.3  The Delphi methodThe Delphi method\nThe Delphi method was invented by Olaf Helmer and Norman Dalkey of the Rand\nCorporation in the 1950s for the purpose of addressing a specific military problem.\nThe method relies on the key assumption that forecasts from a group are generally\nmore accurate than those from individuals. The aim of the Delphi method is to\nconstruct consensus forecasts from a group of experts in a structured iterative\nmanner. A facilitator is appointed in order to implement and manage the process.\nThe Delphi method generally involves the following stages:\n1. A panel of experts is assembled.\n2. Forecasting tasks/challenges are set and distributed to the experts.\n3. Experts return initial forecasts and justifications. These are compiled and\nsummarised in order to provide feedback.\n4. Feedback is provided to the experts, who now review their forecasts in light of\nthe feedback. This step may be iterated until a satisfactory level of consensus is\nreached.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 225,
      "total_chunks": 873
    }
  },
  {
    "text": "4. Feedback is provided to the experts, who now review their forecasts in light of\nthe feedback. This step may be iterated until a satisfactory level of consensus is\nreached.\n5. Final forecasts are constructed by aggregating the experts’ forecasts.\nEach stage of the Delphi method comes with its own challenges. In what follows, we\nprovide some suggestions and discussions about each one of these.\nExperts and anonymityExperts and anonymity\nThe first challenge of the facilitator is to identify a group of experts who can\ncontribute to the forecasting task. The usual suggestion is somewhere between 5 and\n20 experts with diverse expertise. Experts submit forecasts and also provide detailed\nqualitative justifications for these.\nA key feature of the Delphi method is that the participating experts remain\nanonymous at all times. This means that the experts cannot be influenced by\npolitical and social pressures in their forecasts. Furthermore, all experts are given an",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 226,
      "total_chunks": 873
    }
  },
  {
    "text": "anonymous at all times. This means that the experts cannot be influenced by\npolitical and social pressures in their forecasts. Furthermore, all experts are given an\nequal say and all are held accountable for their forecasts. This avoids the situation\nwhere a group meeting is held and some members do not contribute, while others\ndominate. It also prevents members exerting undue influence based on seniority or\npersonality. There have been suggestions that even something as simple as the\n7\n124",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 227,
      "total_chunks": 873
    }
  },
  {
    "text": "seating arrangements in a group setting can influence the group dynamics.\nFurthermore, there is ample evidence that a group meeting setting promotes\nenthusiasm and influences individual judgement, leading to optimism and\noverconfidence.\nA by-product of anonymity is that the experts do not need to meet as a group in a\nphysical location. An important advantage of this is that it increases the likelihood of\ngathering experts with diverse skills and expertise from varying locations.\nFurthermore, it makes the process cost-effective by eliminating the expense and\ninconvenience of travel, and it makes it flexible, as the experts only have to meet a\ncommon deadline for submitting forecasts, rather than having to set a common\nmeeting time.\nSetting the forecasting task in a Delphi\nIn a Delphi setting, it may be useful to conduct a preliminary round of information\ngathering from the experts before setting the forecasting tasks. Alternatively, as",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 228,
      "total_chunks": 873
    }
  },
  {
    "text": "In a Delphi setting, it may be useful to conduct a preliminary round of information\ngathering from the experts before setting the forecasting tasks. Alternatively, as\nexperts submit their initial forecasts and justifications, valuable information which\nis not shared between all experts can be identified by the facilitator when compiling\nthe feedback.\nFeedbackFeedback\nFeedback to the experts should include summary statistics of the forecasts and\noutlines of qualitative justifications. Numerical data summaries and graphical\nrepresentations can be used to summarise the experts’ forecasts.\nAs the feedback is controlled by the facilitator, there may be scope to direct attention\nand information from the experts to areas where it is most required. For example,\nthe facilitator may direct the experts’ attention to responses that fall outside the\ninterquartile range, and the qualitative justification for such forecasts.\nIterationIteration",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 229,
      "total_chunks": 873
    }
  },
  {
    "text": "the facilitator may direct the experts’ attention to responses that fall outside the\ninterquartile range, and the qualitative justification for such forecasts.\nIterationIteration\nThe process of the experts submitting forecasts, receiving feedback, and reviewing\ntheir forecasts in light of the feedback, is repeated until a satisfactory level of\nconsensus between the experts is reached. Satisfactory consensus does not mean\ncomplete convergence in the forecast value; it simply means that the variability of\n8\n125",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 230,
      "total_chunks": 873
    }
  },
  {
    "text": "the responses has decreased to a satisfactory level. Usually two or three rounds are\nsufficient. Experts are more likely to drop out as the number of iterations increases,\nso too many rounds should be avoided.\nFinal forecastsFinal forecasts\nThe final forecasts are usually constructed by giving equal weight to all of the\nexperts’ forecasts. However, the facilitator should keep in mind the possibility of\nextreme values which can distort the final forecast.\nLimitations and variationsLimitations and variations\nApplying the Delphi method can be time consuming. In a group meeting, final\nforecasts can possibly be reached in hours or even minutes — something which is\nalmost impossible to do in a Delphi setting. If it is taking a long time to reach a\nconsensus in a Delphi setting, the panel may lose interest and cohesiveness.\nIn a group setting, personal interactions can lead to quicker and better clarifications",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 231,
      "total_chunks": 873
    }
  },
  {
    "text": "consensus in a Delphi setting, the panel may lose interest and cohesiveness.\nIn a group setting, personal interactions can lead to quicker and better clarifications\nof qualitative justifications. A variation of the Delphi method which is often applied\nis the “estimate-talk-estimate” method, where the experts can interact between\niterations, although the forecast submissions can still remain anonymous. A\ndisadvantage of this variation is the possibility of the loudest person exerting undue\ninfluence.\nThe facilitatorThe facilitator\nThe role of the facilitator is of the utmost importance. The facilitator is largely\nresponsible for the design and administration of the Delphi process. The facilitator is\nalso responsible for providing feedback to the experts and generating the final\nforecasts. In this role, the facilitator needs to be experienced enough to recognise\nareas that may need more attention, and to direct the experts’ attention to these.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 232,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasts. In this role, the facilitator needs to be experienced enough to recognise\nareas that may need more attention, and to direct the experts’ attention to these.\nAlso, as there is no face-to-face interaction between the experts, the facilitator is\nresponsible for disseminating important information. The efficiency and\neffectiveness of the facilitator can dramatically increase the probability of a\nsuccessful Delphi method in a judgmental forecasting setting.\n126",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 233,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nBuehler, R., Messervey, D., & Griffin, D. (2005). Collaborative planning and\nprediction: Does group discussion affect optimistic biases in time estimation?\nOrganizational Behavior and Human Decision Processes , 97(1), 47–63. [DOI]\nRowe, G. (2007). A guide to Delphi. Foresight: The International Journal of\nApplied Forecasting , (8), 11–16.\nRowe, G., & Wright, G. (1999). The Delphi technique as a forecasting tool: Issues\nand analysis. International Journal of Forecasting , 15(4), 353–375. [DOI]\n7. For further reading, refer to: Rowe ( 2007); Rowe & Wright ( 1999)↩ \n8. Buehler, Messervey, & Griffin (2005) ↩ \n127",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 234,
      "total_chunks": 873
    }
  },
  {
    "text": "4.44.4  Forecasting by analogyForecasting by analogy\nA useful judgmental approach which is often implemented in practice is forecasting\nby analogy. A common example is the pricing of a house through an appraisal\nprocess. An appraiser estimates the market value of a house by comparing it to\nsimilar properties that have sold in the area. The degree of similarity depends on the\nattributes considered. With house appraisals, attributes such as land size, dwelling\nsize, numbers of bedrooms and bathrooms, and garage space are usually considered.\nEven thinking and discussing analogous products or situations can generate useful\n(and sometimes crucial) information. We illustrate this point with the following\nexample.\nExample: Designing a high school curriculumExample: Designing a high school curriculum\nA small group of academics and teachers were assigned the task of developing a\ncurriculum for teaching judgement and decision making under uncertainty for high",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 235,
      "total_chunks": 873
    }
  },
  {
    "text": "A small group of academics and teachers were assigned the task of developing a\ncurriculum for teaching judgement and decision making under uncertainty for high\nschools in Israel. Each group member was asked to forecast how long it would take\nfor the curriculum to be completed. Responses ranged between 18 and 30 months.\nOne of the group members who was an expert in curriculum design was asked to\nconsider analogous curricula developments around the world. He concluded that\n40% of analogous groups he considered never completed the task. The rest took\nbetween 7 to 10 years. The Israel project was completed in 8 years.\nObviously, forecasting by analogy comes with challenges. We should aspire to base\nforecasts on multiple analogies rather than a single analogy, which may create\nbiases. However, these may be challenging to identify. Similarly, we should aspire to\nconsider multiple attributes. Identifying or even comparing these may not always be",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 236,
      "total_chunks": 873
    }
  },
  {
    "text": "biases. However, these may be challenging to identify. Similarly, we should aspire to\nconsider multiple attributes. Identifying or even comparing these may not always be\nstraightforward. As always, we suggest performing these comparisons and the\nforecasting process using a systematic approach. Developing a detailed scoring\nmechanism to rank attributes and record the process of ranking will always be\nuseful.\n9\n128",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 237,
      "total_chunks": 873
    }
  },
  {
    "text": "A structured analogyA structured analogy\nAlternatively, a structured approach comprising a panel of experts can be\nimplemented, as was proposed by Green & Armstrong ( 2007). The concept is similar\nto that of a Delphi; however, the forecasting task is completed by considering\nanalogies. First, a facilitator is appointed. Then the structured approach involves the\nfollowing steps.\n1. A panel of experts who are likely to have experience with analogous situations is\nassembled.\n2. Tasks/challenges are set and distributed to the experts.\n3. Experts identify and describe as many analogies as they can, and generate\nforecasts based on each analogy.\n4. Experts list similarities and differences of each analogy to the target situation,\nthen rate the similarity of each analogy to the target situation on a scale.\n5. Forecasts are derived by the facilitator using a set rule. This can be a weighted\naverage, where the weights can be guided by the ranking scores of each analogy\nby the experts.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 238,
      "total_chunks": 873
    }
  },
  {
    "text": "5. Forecasts are derived by the facilitator using a set rule. This can be a weighted\naverage, where the weights can be guided by the ranking scores of each analogy\nby the experts.\nAs with the Delphi approach, anonymity of the experts may be an advantage in not\nsuppressing creativity, but could hinder collaboration. Green and Armstrong found\nno gain in collaboration between the experts in their results. A key finding was that\nexperts with multiple analogies (more than two), and who had direct experience\nwith the analogies, generated the most accurate forecasts.\nBibliographyBibliography\nGreen, K. C., & Armstrong, J. S. (2007). Structured analogies for forecasting.\nInternational Journal of Forecasting , 23(3), 365–376. [DOI]\nKahneman, D., & Lovallo, D. (1993). Timid choices and bold forecasts: A cognitive\nperspective on risk taking. Management Science , 39(1), 17–31. [DOI]\n9. This example is extracted from Kahneman & Lovallo ( 1993)↩ \n129",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 239,
      "total_chunks": 873
    }
  },
  {
    "text": "4.54.5  Scenario forecastingScenario forecasting\nA fundamentally different approach to judgmental forecasting is scenario-based\nforecasting. The aim of this approach is to generate forecasts based on plausible\nscenarios. In contrast to the two previous approaches (Delphi and forecasting by\nanalogy) where the resulting forecast is intended to be a likely outcome, each\nscenario-based forecast may have a low probability of occurrence. The scenarios are\ngenerated by considering all possible factors or drivers, their relative impacts, the\ninteractions between them, and the targets to be forecast.\nBuilding forecasts based on scenarios allows a wide range of possible forecasts to be\ngenerated and some extremes to be identified. For example it is usual for “best”,\n“middle” and “worst” case scenarios to be presented, although many other\nscenarios will be generated. Thinking about and documenting these contrasting\nextremes can lead to early contingency planning.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 240,
      "total_chunks": 873
    }
  },
  {
    "text": "“middle” and “worst” case scenarios to be presented, although many other\nscenarios will be generated. Thinking about and documenting these contrasting\nextremes can lead to early contingency planning.\nWith scenario forecasting, decision makers often participate in the generation of\nscenarios. While this may lead to some biases, it can ease the communication of the\nscenario-based forecasts, and lead to a better understanding of the results.\n130",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 241,
      "total_chunks": 873
    }
  },
  {
    "text": "4.64.6  New product forecastingNew product forecasting\nThe definition of a new product can vary. It may be an entirely new product which\nhas been launched, a variation of an existing product (“new and improved”), a\nchange in the pricing scheme of an existing product, or even an existing product\nentering a new market.\nJudgmental forecasting is usually the only available method for new product\nforecasting, as historical data are unavailable. The approaches we have already\noutlined (Delphi, forecasting by analogy and scenario forecasting) are all applicable\nwhen forecasting the demand for a new product.\nOther methods which are more specific to the situation are also available. We briefly\ndescribe three such methods which are commonly applied in practice. These\nmethods are less structured than those already discussed, and are likely to lead to\nmore biased forecasts as a result.\nSales force compositeSales force composite",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 242,
      "total_chunks": 873
    }
  },
  {
    "text": "methods are less structured than those already discussed, and are likely to lead to\nmore biased forecasts as a result.\nSales force compositeSales force composite\nIn this approach, forecasts for each outlet/branch/store of a company are generated\nby salespeople, and are then aggregated. This usually involves sales managers\nforecasting the demand for the outlet they manage. Salespeople are usually closest to\nthe interaction between customers and products, and often develop an intuition\nabout customer purchasing intentions. They bring this valuable experience and\nexpertise to the forecast.\nHowever, having salespeople generate forecasts violates the key principle of\nsegregating forecasters and users, which can create biases in many directions. It is\ncommon for the performance of a salesperson to be evaluated against the sales\nforecasts or expectations set beforehand. In this case, the salesperson acting as a",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 243,
      "total_chunks": 873
    }
  },
  {
    "text": "common for the performance of a salesperson to be evaluated against the sales\nforecasts or expectations set beforehand. In this case, the salesperson acting as a\nforecaster may introduce some self-serving bias by generating low forecasts. On the\nother hand, one can imagine an enthusiastic salesperson, full of optimism,\ngenerating high forecasts.\nMoreover a successful salesperson is not necessarily a successful nor well-informed\nforecaster. A large proportion of salespeople will have no or limited formal training\nin forecasting. Finally, salespeople will feel customer displeasure at first hand if, for\n131",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 244,
      "total_chunks": 873
    }
  },
  {
    "text": "example, the product runs out or is not introduced in their store. Such interactions\nwill cloud their judgement.\nExecutive opinionExecutive opinion\nIn contrast to the sales force composite, this approach involves staff at the top of the\nmanagerial structure generating aggregate forecasts. Such forecasts are usually\ngenerated in a group meeting, where executives contribute information from their\nown area of the company. Having executives from different functional areas of the\ncompany promotes great skill and knowledge diversity in the group.\nThis process carries all of the advantages and disadvantages of a group meeting\nsetting which we discussed earlier. In this setting, it is important to justify and\ndocument the forecasting process. That is, executives need to be held accountable in\norder to reduce the biases generated by the group meeting setting. There may also be\nscope to apply variations to a Delphi approach in this setting; for example, the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 245,
      "total_chunks": 873
    }
  },
  {
    "text": "order to reduce the biases generated by the group meeting setting. There may also be\nscope to apply variations to a Delphi approach in this setting; for example, the\nestimate-talk-estimate process described earlier.\nCustomer intentionsCustomer intentions\nCustomer intentions can be used to forecast the demand for a new product or for a\nvariation on an existing product. Questionnaires are filled in by customers on their\nintentions to buy the product. A structured questionnaire is used, asking customers\nto rate the likelihood of them purchasing the product on a scale; for example, highly\nlikely, likely, possible, unlikely, highly unlikely.\nSurvey design challenges, such as collecting a representative sample, applying a\ntime- and cost-effective method, and dealing with non-responses, need to be\naddressed.\nFurthermore, in this survey setting we must keep in mind the relationship between\npurchase intention and purchase behaviour. Customers do not always do what they",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 246,
      "total_chunks": 873
    }
  },
  {
    "text": "addressed.\nFurthermore, in this survey setting we must keep in mind the relationship between\npurchase intention and purchase behaviour. Customers do not always do what they\nsay they will. Many studies have found a positive correlation between purchase\nintentions and purchase behaviour; however, the strength of these correlations\nvaries substantially. The factors driving this variation include the timings of data\ncollection and product launch, the definition of “new” for the product, and the type\nof industry. Behavioural theory tells us that intentions predict behaviour if the\nintentions are measured just before the behaviour.  The time between intention and\nbehaviour will vary depending on whether it is a completely new product or a\n10\n11\n132",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 247,
      "total_chunks": 873
    }
  },
  {
    "text": "variation on an existing product. Also, the correlation between intention and\nbehaviour is found to be stronger for variations on existing and familiar products\nthan for completely new products.\nWhichever method of new product forecasting is used, it is important to thoroughly\ndocument the forecasts made, and the reasoning behind them, in order to be able to\nevaluate them when data become available.\nBibliographyBibliography\nGroves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &\nTourangeau, R. (2009). Survey methodology  (2nd ed). John Wiley & Sons.\n[Amazon]\nRandall, D. M., & Wolff, J. A. (1994). The time interval in the intention-behaviour\nrelationship: Meta-analysis. British Journal of Social Psychology , 33(4), 405–\n418. [DOI]\n10. Groves et al. ( 2009)↩ \n11. Randall & Wolff ( 1994)↩ \n133",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 248,
      "total_chunks": 873
    }
  },
  {
    "text": "4.74.7  Judgmental adjustmentsJudgmental adjustments\nIn this final section, we consider the situation where historical data are available and\nare used to generate statistical forecasts. It is common for practitioners to then\napply judgmental adjustments to these forecasts. These adjustments can potentially\nprovide all of the advantages of judgmental forecasting which have been discussed\nearlier in this chapter. For example, they provide an avenue for incorporating factors\nthat may not be accounted for in the statistical model, such as promotions, large\nsporting events, holidays, or recent events that are not yet reflected in the data.\nHowever, these advantages come to fruition only when the right conditions are\npresent. Judgmental adjustments, like judgmental forecasts, come with biases and\nlimitations, and we must implement methodical strategies in order to minimise\nthem.\nUse adjustments sparinglyUse adjustments sparingly",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 249,
      "total_chunks": 873
    }
  },
  {
    "text": "limitations, and we must implement methodical strategies in order to minimise\nthem.\nUse adjustments sparinglyUse adjustments sparingly\nPractitioners adjust much more often than they should, and many times for the\nwrong reasons. By adjusting statistical forecasts, users of forecasts create a feeling\nof ownership and credibility. Users often do not understand or appreciate the\nmechanisms that generate the statistical forecasts (as they will usually have no\ntraining in this area). By implementing judgmental adjustments, users feel that they\nhave contributed to and completed the forecasts, and they can now relate their own\nintuition and interpretations to these. The forecasts have become their own.\nJudgmental adjustments should not aim to correct for a systematic pattern in the\ndata that is thought to have been missed by the statistical model. This has been\nproven to be ineffective, as forecasters tend to read non-existent patterns in noisy",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 250,
      "total_chunks": 873
    }
  },
  {
    "text": "data that is thought to have been missed by the statistical model. This has been\nproven to be ineffective, as forecasters tend to read non-existent patterns in noisy\nseries. Statistical models are much better at taking account of data patterns, and\njudgmental adjustments only hinder accuracy.\nJudgmental adjustments are most effective when there is significant additional\ninformation at hand or strong evidence of the need for an adjustment. We should\nonly adjust when we have important extra information which is not incorporated in\nthe statistical model. Hence, adjustments seem to be most accurate when they are\nlarge in size. Small adjustments (especially in the positive direction promoting the\nillusion of optimism) have been found to hinder accuracy, and should be avoided.\n134",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 251,
      "total_chunks": 873
    }
  },
  {
    "text": "Apply a structured approachApply a structured approach\nUsing a structured and systematic approach will improve the accuracy of judgmental\nadjustments. Following the key principles outlined in Section 4.2 is vital. In\nparticular, having to document and justify adjustments will make it more\nchallenging to override the statistical forecasts, and will guard against unnecessary\nadjustments.\nIt is common for adjustments to be implemented by a panel (see the example that\nfollows). Using a Delphi setting carries great advantages. However, if adjustments\nare implemented in a group meeting, it is wise to consider the forecasts of key\nmarkets or products first, as panel members will get tired during this process. Fewer\nadjustments tend to be made as the meeting goes on through the day.\nExample: Tourism Forecasting Committee (TFC)Example: Tourism Forecasting Committee (TFC)\nTourism Australia publishes forecasts for all aspects of Australian tourism twice a",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 252,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Tourism Forecasting Committee (TFC)Example: Tourism Forecasting Committee (TFC)\nTourism Australia publishes forecasts for all aspects of Australian tourism twice a\nyear. The published forecasts are generated by the TFC, an independent body which\ncomprises experts from various government and industry sectors; for example, the\nAustralian Commonwealth Treasury, airline companies, consulting firms, banking\nsector companies, and tourism bodies.\nThe forecasting methodology applied is an iterative process. First, model-based\nstatistical forecasts are generated by the forecasting unit within Tourism Australia,\nthen judgmental adjustments are made to these in two rounds. In the first round, the\nTFC Technical Committee  (comprising senior researchers, economists and\nindependent advisers) adjusts the model-based forecasts. In the second and final\nround, the TFC (comprising industry and government experts) makes final\nadjustments. In both rounds, adjustments are made by consensus.\n12\n135",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 253,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 4.2: Long run annual forecasts for domestic visitor nights for Australia. We study\nregression models in Chapter 5, and ETS (ExponenTial Smoothing) models in Chapter 7.\nIn 2008, we  analysed forecasts for Australian domestic tourism. We concluded that\nthe published TFC forecasts were optimistic, especially for the long-run, and we\nproposed alternative model-based forecasts. We now have access to observed data\nup to and including 2011. In Figure 4.2, we plot the published forecasts against the\nactual data. We can see that the published TFC forecasts have continued to be\noptimistic.\nWhat can we learn from this example? Although the TFC clearly states in its\nmethodology that it produces ‘forecasts’ rather than ‘targets’, could this be a case\nwhere these have been confused? Are the forecasters and users sufficiently well-\nsegregated in this process? Could the iterative process itself be improved? Could the\nadjustment process in the meetings be improved? Could it be that the group",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 254,
      "total_chunks": 873
    }
  },
  {
    "text": "segregated in this process? Could the iterative process itself be improved? Could the\nadjustment process in the meetings be improved? Could it be that the group\nmeetings have promoted optimism? Could it be that domestic tourism should have\nbeen considered earlier in the day?\nBibliographyBibliography\nAthanasopoulos, G., & Hyndman, R. J. (2008). Modelling and forecasting\nAustralian domestic tourism. Tourism Management , 29(1), 19–31. [DOI]\n12. GA was an observer on this technical committee for a few years. ↩ \n13\n136",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 255,
      "total_chunks": 873
    }
  },
  {
    "text": "13. Athanasopoulos & Hyndman ( 2008)↩ \n137",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 256,
      "total_chunks": 873
    }
  },
  {
    "text": "4.84.8  Further readingFurther reading\nMany forecasting textbooks ignore judgmental forecasting altogether. Here are three\nwhich do cover it in some detail.\nChapter 11 of Ord et al. (2017) provides an excellent review of some of the same\ntopics as this chapter, but also includes using judgement to assessing forecast\nuncertainty, and forecasting using prediction markets.\nGoodwin & Wright (2009 ) is a book-length treatment of the use of judgement in\ndecision making by two of the leading researchers in the field.\nKahn (2006) covers techniques for new product forecasting, where judgmental\nmethods play an important role.\nThere have been some helpful survey papers on judgmental forecasting published in\nthe last 20 years. We have found these three particularly helpful.\nFildes & Goodwin (2007b)\nFildes & Goodwin (2007a)\nHarvey (2001)\nSome helpful papers on individual judgmental forecasting methods are listed in the\ntable below.\nForecasting Method Recommended papersRecommended papers",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 257,
      "total_chunks": 873
    }
  },
  {
    "text": "Fildes & Goodwin (2007a)\nHarvey (2001)\nSome helpful papers on individual judgmental forecasting methods are listed in the\ntable below.\nForecasting Method Recommended papersRecommended papers\nDelphi Rowe & Wright ( 1999)\nRowe (2007)\nAdjustments Sanders et al. ( 2005)\nEroglu & Croxton ( 2010)\nFranses & Legerstee ( 2013)\nAnalogy Green & Armstrong ( 2007)\nScenarios Önkal, Say ım, & Gönül ( 2013)\nCustomer intentions Morwitz, Steckel, & Gupta ( 2007)\nBibliographyBibliography\nEroglu, C., & Croxton, K. L. (2010). Biases in judgmental adjustments of statistical\nforecasts: The role of individual differences. International Journal of\nForecasting , 26(1), 116–133. [DOI]138",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 258,
      "total_chunks": 873
    }
  },
  {
    "text": "Fildes, R., & Goodwin, P. (2007a). Against your better judgment? How\norganizations can improve their use of management judgment in forecasting.\nInterfaces , 37(6), 570–576. [DOI]\nFildes, R., & Goodwin, P. (2007b). Good and bad judgment in forecasting: Lessons\nfrom four companies. Foresight: The International Journal of Applied\nForecasting , (8), 5–10.\nFranses, P. H., & Legerstee, R. (2013). Do statistical forecasting models for SKU-\nlevel data benefit from including past expert knowledge? International Journal\nof Forecasting , 29(1), 80–87. [DOI]\nGoodwin, P., & Wright, G. (2009). Decision analysis for management judgment\n(4th ed). Chichester: John Wiley & Sons. [Amazon]\nGreen, K. C., & Armstrong, J. S. (2007). Structured analogies for forecasting.\nInternational Journal of Forecasting , 23(3), 365–376. [DOI]\nHarvey, N. (2001). Improving judgment in forecasting. In J. S. Armstrong (Ed.),\nPrinciples of forecasting: A handbook for researchers and practitioners  (pp.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 259,
      "total_chunks": 873
    }
  },
  {
    "text": "Harvey, N. (2001). Improving judgment in forecasting. In J. S. Armstrong (Ed.),\nPrinciples of forecasting: A handbook for researchers and practitioners  (pp.\n59–80). Boston, MA: Kluwer Academic Publishers. [DOI]\nKahn, K. B. (2006). New product forecasting: An applied approach . M.E. Sharp.\n[Amazon]\nMorwitz, V. G., Steckel, J. H., & Gupta, A. (2007). When do purchase intentions\npredict sales? International Journal of Forecasting , 23(3), 347–364. [DOI]\nÖnkal, D., Sayı m, K. Z., & Gönül, M. S. (2013). Scenarios as channels of forecast\nadvice. Technological Forecasting and Social Change , 80(4), 772–788. [DOI]\nOrd, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\nRowe, G. (2007). A guide to Delphi. Foresight: The International Journal of\nApplied Forecasting , (8), 11–16.\nRowe, G., & Wright, G. (1999). The Delphi technique as a forecasting tool: Issues",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 260,
      "total_chunks": 873
    }
  },
  {
    "text": "Rowe, G. (2007). A guide to Delphi. Foresight: The International Journal of\nApplied Forecasting , (8), 11–16.\nRowe, G., & Wright, G. (1999). The Delphi technique as a forecasting tool: Issues\nand analysis. International Journal of Forecasting , 15(4), 353–375. [DOI]\nSanders, N., Goodwin, P., Önkal, D., Gönül, M. S., Harvey, N., Lee, A., & Kjolso, L.\n(2005). When and how should statistical forecasts be judgmentally adjusted?\nForesight: The International Journal of Applied Forecasting , 1(1), 5–23.\nhttp://www.forecastpro.com/Trends/pdf/Nada%20Sanders%20Judgmental%2\n0Adjustments%20to%20Statistical%20Forecasts%20July%202008.pdf\n139",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 261,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 5Chapter 5  Time series regression modelsTime series regression models\nIn this chapter we discuss regression models. The basic concept is that we forecast\nthe time series of interest  assuming that it has a linear relationship with other time\nseries .\nFor example, we might wish to forecast monthly sales  using total advertising\nspend  as a predictor. Or we might forecast daily electricity demand  using\ntemperature  and the day of week  as predictors.\nThe forecast variableforecast variable   is sometimes also called the regressand, dependent or\nexplained variable. The predictor variablespredictor variables   are sometimes also called the\nregressors, independent or explanatory variables. In this book we will always refer to\nthem as the “forecast” variable and “predictor” variables.\n140",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 262,
      "total_chunks": 873
    }
  },
  {
    "text": "5.15.1  The linear modelThe linear model\nSimple linear regressionSimple linear regression\nIn the simplest case, the regression model allows for a linear relationship between\nthe forecast variable  and a single predictor variable :\nAn artificial example of data from such a model is shown in Figure 5.1. The\ncoefficients  and  denote the intercept and the slope of the line respectively. The\nintercept  represents the predicted value of  when . The slope  represents\nthe average predicted change in  resulting from a one unit increase in .\nFigure 5.1: An example of data from a simple linear regression model.\nNotice that the observations do not lie on the straight line but are scattered around\nit. We can think of each observation  as consisting of the systematic or explained\npart of the model, , and the random “error”, . The “error” term does not\nimply a mistake, but a deviation from the underlying straight line model. It captures\nanything that may affect  other than .\n141",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 263,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: US consumption expenditureExample: US consumption expenditure\nFigure 5.2  shows time series of quarterly percentage changes (growth rates) of real\npersonal consumption expenditure, , and real personal disposable income, , for\nthe US from 1970 Q1 to 2016 Q3.\nFigure 5.2: Percentage changes in personal consumption expenditure and personal\nincome for the US.\nA scatter plot of consumption changes against income changes is shown in Figure 5.3\nalong with the estimated regression line\n(We put a “hat” above  to indicate that this is the value of  predicted by the model.)\nautoplot(uschange[,c(\"Consumption\",\"Income\")]) +\n  ylab(\"% change\") + xlab(\"Year\")\nuschange %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Income, y=Consumption)) +\n    ylab(\"Consumption (quarterly % change)\" ) +\n    xlab(\"Income (quarterly % change)\" ) +\n    geom_point() +\n    geom_smooth(method=\"lm\", se=FALSE)\n#> `geom_smooth()` using formula = 'y ~ x'\n142",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 264,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.3: Scatterplot of quarterly changes in consumption expenditure versus quarterly\nchanges in personal income and the fitted regression line.\nThe equation is estimated in R using the ıtslm()ı function:\nWe will discuss how ıtslm()ı computes the coefficients in Section 5.2.\nThe fitted line has a positive slope, reflecting the positive relationship between\nincome and consumption. The slope coefficient shows that a one unit increase in  (a\n1 percentage point increase in personal disposable income) results on average in 0.28\nunits increase in  (an average increase of 0.28 percentage points in personal\nconsumption expenditure). Alternatively the estimated equation shows that a value\nof 1 for  (the percentage increase in personal disposable income) will result in a\nforecast value of  for  (the percentage increase in personal\nconsumption expenditure).\ntslm(Consumption ~ Income, data=uschange)\n#> \n#> Call:\n#> tslm(formula = Consumption ~ Income, data = uschange)\n#> \n#> Coefficients:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 265,
      "total_chunks": 873
    }
  },
  {
    "text": "consumption expenditure).\ntslm(Consumption ~ Income, data=uschange)\n#> \n#> Call:\n#> tslm(formula = Consumption ~ Income, data = uschange)\n#> \n#> Coefficients:\n#> (Intercept)       Income  \n#>       0.545        0.281\n/g34\n/g35\n/g34\n/g11/gF/g16/g16 /gC /g11/gF/g13/g19 /g67 /g12 /g1E /g11/gF/g19/g14 /g35\n143",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 266,
      "total_chunks": 873
    }
  },
  {
    "text": "The interpretation of the intercept requires that a value of  makes sense. In this\ncase when  (i.e., when there is no change in personal disposable income since\nthe last quarter) the predicted value of  is 0.55 (i.e., an average increase in personal\nconsumption expenditure of 0.55%). Even when  does not make sense, the\nintercept is an important part of the model. Without it, the slope coefficient can be\ndistorted unnecessarily. The intercept should always be included unless the\nrequirement is to force the regression line “through the origin”. In what follows we\nassume that an intercept is always included in the model.\nMultiple linear regression\nWhen there are two or more predictor variables, the model is called a multiplemultiple\nregression modelregression model . The general form of a multiple regression model is\nwhere  is the variable to be forecast and  are the  predictor variables.\nEach of the predictor variables must be numerical. The coefficients",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 267,
      "total_chunks": 873
    }
  },
  {
    "text": "where  is the variable to be forecast and  are the  predictor variables.\nEach of the predictor variables must be numerical. The coefficients \nmeasure the effect of each predictor after taking into account the effects of all the\nother predictors in the model. Thus, the coefficients measure the marginal effects  of\nthe predictor variables.\nExample: US consumption expenditureExample: US consumption expenditure\nFigure 5.4 shows additional predictors that may be useful for forecasting US\nconsumption expenditure. These are quarterly percentage changes in industrial\nproduction and personal savings, and quarterly changes in the unemployment rate\n(as this is already a percentage). Building a multiple linear regression model can\npotentially generate more accurate forecasts as we expect consumption expenditure\nto not only depend on personal income but on other predictors as well.\n144",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 268,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.4: Quarterly percentage changes in industrial production and personal savings\nand quarterly changes in the unemployment rate for the US over the period 1970Q1-\n2016Q3.\nFigure 5.5  is a scatterplot matrix of five variables. The first column shows the\nrelationships between the forecast variable (consumption) and each of the\npredictors. The scatterplots show positive relationships with income and industrial\nproduction, and negative relationships with savings and unemployment. The\nstrength of these relationships are shown by the correlation coefficients across the\nfirst row. The remaining scatterplots and correlation coefficients show the\nrelationships between the predictors.\nuschange %>%\n  as.data.frame() %>%\n  GGally::ggpairs()\n145",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 269,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.5: A scatterplot matrix of US consumption expenditure and the four predictors.\nAssumptionsAssumptions\nWhen we use a linear regression model, we are implicitly making some assumptions\nabout the variables in Equation (5.1).\nFirst, we assume that the model is a reasonable approximation to reality; that is, the\nrelationship between the forecast variable and the predictor variables satisfies this\nlinear equation.\nSecond, we make the following assumptions about the errors :\nthey have mean zero; otherwise the forecasts will be systematically biased.146",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 270,
      "total_chunks": 873
    }
  },
  {
    "text": "they are not autocorrelated; otherwise the forecasts will be inefficient, as there is\nmore information in the data that can be exploited.\nthey are unrelated to the predictor variables; otherwise there would be more\ninformation that should be included in the systematic part of the model.\nIt is also useful to have the errors being normally distributed with a constant\nvariance  in order to easily produce prediction intervals.\nAnother important assumption in the linear regression model is that each predictor\n is not a random variable. If we were performing a controlled experiment in a\nlaboratory, we could control the values of each  (so they would not be random) and\nobserve the resulting values of . With observational data (including most data in\nbusiness and economics), it is not possible to control the value of , we simply\nobserve it. Hence we make this an assumption.\n/g55 /g13\n/g34\n/g34\n/g35\n/g34\n147",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 271,
      "total_chunks": 873
    }
  },
  {
    "text": "5.25.2  Least squares estimationLeast squares estimation\nIn practice, of course, we have a collection of observations but we do not know the\nvalues of the coefficients . These need to be estimated from the data.\nThe least squares principle provides a way of choosing the coefficients effectively by\nminimising the sum of the squared errors. That is, we choose the values of\n that minimise\nThis is called least squaresleast squares  estimation because it gives the least value for the sum of\nsquared errors. Finding the best estimates of the coefficients is often called “fitting”\nthe model to the data, or sometimes “learning” or “training” the model. The line\nshown in Figure 5.3 was obtained in this way.\nWhen we refer to the estimated coefficients, we will use the notation . The\nequations for these will be given in Section 5.7 .\nThe ıtslm()ı function fits a linear regression model to time series data. It is similar",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 272,
      "total_chunks": 873
    }
  },
  {
    "text": "equations for these will be given in Section 5.7 .\nThe ıtslm()ı function fits a linear regression model to time series data. It is similar\nto the ılm()ı function which is widely used for linear models, but ıtslm()ı provides\nadditional facilities for handling time series.\nExample: US consumption expenditureExample: US consumption expenditure\nA multiple linear regression model for US consumption is\nwhere  is the percentage change in real personal consumption expenditure,  is\nthe percentage change in real personal disposable income,  is the percentage\nchange in industrial production,  is the percentage change in personal savings and\n is the change in the unemployment rate.\nThe following output provides information about the fitted model. The first column\nof ıCoefficientsı gives an estimate of each  coefficient and the second column gives\nits standard error (i.e., the standard deviation which would be obtained from\n148",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 273,
      "total_chunks": 873
    }
  },
  {
    "text": "repeatedly estimating the  coefficients on similar data sets). The standard error\ngives a measure of the uncertainty in the estimated  coefficient.\nFor forecasting purposes, the final two columns are of limited interest. The “t value”\nis the ratio of an estimated  coefficient to its standard error and the last column\ngives the p-value: the probability of the estimated  coefficient being as large as it is\nif there was no real relationship between consumption and the corresponding\npredictor. This is useful when studying the effect of each predictor, but is not\nparticularly useful for forecasting.\n/g44\n/g44\nfit.consMR <- tslm(\n  Consumption ~ Income + Production + Unemployment + Savings,\n  data=uschange)\nsummary(fit.consMR)\n#> \n#> Call:\n#> tslm(formula = Consumption ~ Income + Production + Unemployment + \n#>     Savings, data = uschange)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.8830 -0.1764 -0.0368  0.1525  1.2055 \n#> \n#> Coefficients:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 274,
      "total_chunks": 873
    }
  },
  {
    "text": "#>     Savings, data = uschange)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.8830 -0.1764 -0.0368  0.1525  1.2055 \n#> \n#> Coefficients:\n#>              Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   0.26729    0.03721    7.18  1.7e-11 ***\n#> Income        0.71448    0.04219   16.93  < 2e-16 ***\n#> Production    0.04589    0.02588    1.77    0.078 .  \n#> Unemployment -0.20477    0.10550   -1.94    0.054 .  \n#> Savings      -0.04527    0.00278  -16.29  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.329 on 182 degrees of freedom\n#> Multiple R-squared:  0.754,  Adjusted R-squared:  0.749 \n#> F-statistic:  139 on 4 and 182 DF,  p-value: <2e-16\n/g44\n/g44\n149",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 275,
      "total_chunks": 873
    }
  },
  {
    "text": "Fitted valuesFitted values\nPredictions of  can be obtained by using the estimated coefficients in the regression\nequation and setting the error term to zero. In general we write,\nPlugging in the values of  for  returns predictions of \nwithin the training-sample, referred to as fitted values . Note that these are\npredictions of the data used to estimate the model, not genuine forecasts of future\nvalues of .\nThe following plots show the actual values compared to the fitted values for the\npercentage change in the US consumption expenditure series. The time plot in Figure\n5.6 shows that the fitted values follow the actual data fairly closely. This is verified\nby the strong positive relationship shown by the scatterplot in Figure 5.7.\nautoplot(uschange[,'Consumption'], series=\"Data\") +\n  autolayer(fitted(fit.consMR), series=\"Fitted\") +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Percent change in US consumption expenditure\" ) +\n  guides(colour=guide_legend(title=\" \"))\n150",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 276,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.6: Time plot of actual US consumption expenditure and predicted US\nconsumption expenditure.\nFigure 5.7: Actual US consumption expenditure plotted against predicted US consumption\nexpenditure.\nGoodness-of-fitGoodness-of-fit\nA common way to summarise how well a linear regression model fits the data is via\nthe coefficient of determination, or . This can be calculated as the square of the\ncorrelation between the observed  values and the predicted  values. Alternatively,\nit can also be calculated as,\ncbind(Data = uschange[,\"Consumption\"],\n      Fitted = fitted(fit.consMR)) %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Data, y=Fitted)) +\n    geom_point() +\n    ylab(\"Fitted (predicted values)\") +\n    xlab(\"Data (actual values)\") +\n    ggtitle(\"Percent change in US consumption expenditure\" ) +\n    geom_abline(intercept=0, slope=1)\n151",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 277,
      "total_chunks": 873
    }
  },
  {
    "text": "where the summations are over all observations. Thus, it reflects the proportion of\nvariation in the forecast variable that is accounted for (or explained) by the\nregression model.\nIn simple linear regression, the value of  is also equal to the square of the\ncorrelation between  and  (provided an intercept has been included).\nIf the predictions are close to the actual values, we would expect  to be close to 1.\nOn the other hand, if the predictions are unrelated to the actual values, then \n(again, assuming there is an intercept). In all cases,  lies between 0 and 1.\nThe  value is used frequently, though often incorrectly, in forecasting. The value\nof  will never decrease when adding an extra predictor to the model and this can\nlead to over-fitting. There are no set rules for what is a good  value, and typical\nvalues of  depend on the type of data used. Validating a model’s forecasting\nperformance on the test data is much better than measuring the  value on the\ntraining data.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 278,
      "total_chunks": 873
    }
  },
  {
    "text": "values of  depend on the type of data used. Validating a model’s forecasting\nperformance on the test data is much better than measuring the  value on the\ntraining data.\nExample: US consumption expenditureExample: US consumption expenditure\nFigure 5.7  plots the actual consumption expenditure values versus the fitted values.\nThe correlation between these variables is  hence  (shown in\nthe output above). In this case model does an excellent job as it explains 75.4% of the\nvariation in the consumption data. Compare that to the  value of 0.16 obtained\nfrom the simple regression with the same data set in Section 5.1. Adding the three\nextra predictors has allowed a lot more of the variation in the consumption data to\nbe explained.\nStandard error of the regressionStandard error of the regression\nAnother measure of how well the model has fitted the data is the standard deviation\nof the residuals, which is often known as the “residual standard error”. This is",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 279,
      "total_chunks": 873
    }
  },
  {
    "text": "Another measure of how well the model has fitted the data is the standard deviation\nof the residuals, which is often known as the “residual standard error”. This is\nshown in the above output with the value 0.329.\n152",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 280,
      "total_chunks": 873
    }
  },
  {
    "text": "It is calculated using\nwhere  is the number of predictors in the model. Notice that we divide by \nbecause we have estimated  parameters (the intercept and a coefficient for each\npredictor variable) in computing the residuals.\nThe standard error is related to the size of the average error that the model produces.\nWe can compare this error to the sample mean of  or with the standard deviation of\n to gain some perspective on the accuracy of the model.\nThe standard error will be used when generating prediction intervals, discussed in\nSection 5.6.\n/g3F/g55 /g21 /g1E\n/g2B/g2A/g2A\n/g27\n/g16\n/g11\n/g30/g1E/g12\n/g21 /g13\n/g30 /gD /g9/g16/gF/g14/gA/g12\n/g16 /gC3 /g27 /gC3/g12\n/g27/g16 /gC3 /g27 /gC3/g12\n/g27 /gC/g12\n/g35\n/g35\n153",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 281,
      "total_chunks": 873
    }
  },
  {
    "text": "5.35.3  Evaluating the regression modelEvaluating the regression model\nThe differences between the observed  values and the corresponding fitted  values\nare the training-set errors or “residuals” defined as,\nfor . Each residual is the unpredictable component of the associated\nobservation.\nThe residuals have some useful properties including the following two:\nAs a result of these properties, it is clear that the average of the residuals is zero, and\nthat the correlation between the residuals and the observations for the predictor\nvariable is also zero. (This is not necessarily true when the intercept is omitted from\nthe model.)\nAfter selecting the regression variables and fitting a regression model, it is necessary\nto plot the residuals to check that the assumptions of the model have been satisfied.\nThere are a series of plots that should be produced in order to check different aspects\nof the fitted model and the underlying assumptions. We will now discuss each of\nthem in turn.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 282,
      "total_chunks": 873
    }
  },
  {
    "text": "There are a series of plots that should be produced in order to check different aspects\nof the fitted model and the underlying assumptions. We will now discuss each of\nthem in turn.\nACF plot of residuals\nWith time series data, it is highly likely that the value of a variable observed in the\ncurrent time period will be similar to its value in the previous period, or even the\nperiod before that, and so on. Therefore when fitting a regression model to time\nseries data, it is common to find autocorrelation in the residuals. In this case, the\nestimated model violates the assumption of no autocorrelation in the errors, and our\nforecasts may be inefficient — there is some information left over which should be\naccounted for in the model in order to obtain better forecasts. The forecasts from a\n154",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 283,
      "total_chunks": 873
    }
  },
  {
    "text": "model with autocorrelated errors are still unbiased, and so they are not “wrong”, but\nthey will usually have larger prediction intervals than they need to. Therefore we\nshould always look at an ACF plot of the residuals.\nAnother useful test of autocorrelation in the residuals designed to take account for\nthe regression model is the Breusch-GodfreyBreusch-Godfrey  test, also referred to as the LM\n(Lagrange Multiplier) test for serial correlation. It is used to test the joint hypothesis\nthat there is no autocorrelation in the residuals up to a certain specified order. A\nsmall p-value indicates there is significant autocorrelation remaining in the\nresiduals.\nThe Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically\ndesigned for use with regression models.\nHistogram of residualsHistogram of residuals\nIt is always a good idea to check whether the residuals are normally distributed. As",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 284,
      "total_chunks": 873
    }
  },
  {
    "text": "designed for use with regression models.\nHistogram of residualsHistogram of residuals\nIt is always a good idea to check whether the residuals are normally distributed. As\nwe explained earlier, this is not essential for forecasting, but it does make the\ncalculation of prediction intervals much easier.\nExampleExample\nUsing the ıcheckresiduals()ı function introduced in Section 3.3, we can obtain all the\nuseful residual diagnostics mentioned above.\ncheckresiduals(fit.consMR)\n155",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 285,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.8: Analysing the residuals from a regression model for US quarterly consumption.\n#> \n#>  Breusch-Godfrey test for serial correlation of order up to 8\n#> \n#> data:  Residuals from Linear regression model\n#> LM test = 15, df = 8, p-value = 0.06\nFigure 5.8  shows a time plot, the ACF and the histogram of the residuals from the\nmultiple regression model fitted to the US quarterly consumption data, as well as the\nBreusch-Godfrey test for jointly testing up to 8th order autocorrelation. (The\nıcheckresiduals()ı function will use the Breusch-Godfrey test for regression models,\nbut the Ljung-Box test otherwise.)\nThe time plot shows some changing variation over time, but is otherwise relatively\nunremarkable. This heteroscedasticity will potentially make the prediction interval\ncoverage inaccurate.\nThe histogram shows that the residuals seem to be slightly skewed, which may also\naffect the coverage probability of the prediction intervals.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 286,
      "total_chunks": 873
    }
  },
  {
    "text": "coverage inaccurate.\nThe histogram shows that the residuals seem to be slightly skewed, which may also\naffect the coverage probability of the prediction intervals.\nThe autocorrelation plot shows a significant spike at lag 7, but it is not quite enough\nfor the Breusch-Godfrey to be significant at the 5% level. In any case, the\nautocorrelation is not particularly large, and at lag 7 it is unlikely to have any\n156",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 287,
      "total_chunks": 873
    }
  },
  {
    "text": "noticeable impact on the forecasts or the prediction intervals. In Chapter 9 we\ndiscuss dynamic regression models used for better capturing information left in the\nresiduals.\nResidual plots against predictorsResidual plots against predictors\nWe would expect the residuals to be randomly scattered without showing any\nsystematic patterns. A simple and quick way to check this is to examine scatterplots\nof the residuals against each of the predictor variables. If these scatterplots show a\npattern, then the relationship may be nonlinear and the model will need to be\nmodified accordingly. See Section 5.8 for a discussion of nonlinear regression.\nIt is also necessary to plot the residuals against any predictors that are not in the\nmodel. If any of these show a pattern, then the corresponding predictor may need to\nbe added to the model (possibly in a nonlinear form).\nExampleExample\nThe residuals from the multiple regression model for forecasting US consumption",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 288,
      "total_chunks": 873
    }
  },
  {
    "text": "be added to the model (possibly in a nonlinear form).\nExampleExample\nThe residuals from the multiple regression model for forecasting US consumption\nplotted against each predictor in Figure 5.9 seem to be randomly scattered.\nTherefore we are satisfied with these in this case.\ndf <- as.data.frame(uschange)\ndf[,\"Residuals\"]  <- as.numeric(residuals(fit.consMR))\np1 <- ggplot(df, aes(x=Income, y=Residuals)) +\n  geom_point()\np2 <- ggplot(df, aes(x=Production, y=Residuals)) +\n  geom_point()\np3 <- ggplot(df, aes(x=Savings, y=Residuals)) +\n  geom_point()\np4 <- ggplot(df, aes(x=Unemployment, y=Residuals)) +\n  geom_point()\ngridExtra::grid.arrange(p1, p2, p3, p4, nrow= 2)\n157",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 289,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.9: Scatterplots of residuals versus each predictor.\nResidual plots against fitted valuesResidual plots against fitted values\nA plot of the residuals against the fitted values should also show no pattern. If a\npattern is observed, there may be “heteroscedasticity” in the errors which means\nthat the variance of the residuals may not be constant. If this problem occurs, a\ntransformation of the forecast variable such as a logarithm or square root may be\nrequired (see Section 3.2.)\nExampleExample\nContinuing the previous example, Figure 5.10 shows the residuals plotted against the\nfitted values. The random scatter suggests the errors are homoscedastic.\ncbind(Fitted = fitted(fit.consMR),\n      Residuals=residuals(fit.consMR)) %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Fitted, y=Residuals)) +  geom_point()\n158",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 290,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.10: Scatterplots of residuals versus fitted values.\nOutliers and influential observationsOutliers and influential observations\nObservations that take extreme values compared to the majority of the data are\ncalled outliersoutliers. Observations that have a large influence on the estimated coefficients\nof a regression model are called influential observationsinfluential observations . Usually, influential\nobservations are also outliers that are extreme in the  direction.\nThere are formal methods for detecting outliers and influential observations that are\nbeyond the scope of this textbook. As we suggested at the beginning of Chapter 2,\nbecoming familiar with your data prior to performing any analysis is of vital\nimportance. A scatter plot of  against each  is always a useful starting point in\nregression analysis, and often helps to identify unusual observations.\nOne source of outliers is incorrect data entry. Simple descriptive statistics of your",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 291,
      "total_chunks": 873
    }
  },
  {
    "text": "regression analysis, and often helps to identify unusual observations.\nOne source of outliers is incorrect data entry. Simple descriptive statistics of your\ndata can identify minima and maxima that are not sensible. If such an observation is\nidentified, and it has been recorded incorrectly, it should be corrected or removed\nfrom the sample immediately.\nOutliers also occur when some observations are simply different. In this case it may\nnot be wise for these observations to be removed. If an observation has been\nidentified as a likely outlier, it is important to study it and analyse the possible\n159",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 292,
      "total_chunks": 873
    }
  },
  {
    "text": "reasons behind it. The decision to remove or retain an observation can be a\nchallenging one (especially when outliers are influential observations). It is wise to\nreport results both with and without the removal of such observations.\nExampleExample\nFigure 5.11 highlights the effect of a single outlier when regressing US consumption\non income (the example introduced in Section 5.1). In the left panel the outlier is only\nextreme in the direction of , as the percentage change in consumption has been\nincorrectly recorded as -4%. The red line is the regression line fitted to the data\nwhich includes the outlier, compared to the black line which is the line fitted to the\ndata without the outlier. In the right panel the outlier now is also extreme in the\ndirection of  with the 4% decrease in consumption corresponding to a 6% increase\nin income. In this case the outlier is extremely influential as the red line now\ndeviates substantially from the black line.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 293,
      "total_chunks": 873
    }
  },
  {
    "text": "in income. In this case the outlier is extremely influential as the red line now\ndeviates substantially from the black line.\nFigure 5.11: The effect of outliers and influential observations on regression\nSpurious regression\nMore often than not, time series data are “non-stationary”; that is, the values of the\ntime series do not fluctuate around a constant mean or with a constant variance. We\nwill deal with time series stationarity in more detail in Chapter 8, but here we need to\naddress the effect that non-stationary data can have on regression models.\n160",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 294,
      "total_chunks": 873
    }
  },
  {
    "text": "For example, consider the two variables plotted in Figure 5.12. These appear to be\nrelated simply because they both trend upwards in the same manner. However, air\npassenger traffic in Australia has nothing to do with rice production in Guinea.\nFigure 5.12: Trending time series data can appear to be related, as shown in this example\nwhere air passengers in Australia are regres sed against rice production in Guinea.\nRegressing non-stationary time series can lead to spurious regressions. The output\nof regressing Australian air passengers on rice production in Guinea is shown in\nFigure 5.13. High  and high residual autocorrelation can be signs of spurious\nregression. Notice these features in the output below. We discuss the issues\nsurrounding non-stationary data and spurious regressions in more detail in Chapter\n9.\nCases of spurious regression might appear to give reasonable short-term forecasts,\nbut they will generally not continue to work into the future.\n/g14 /g13\n161",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 295,
      "total_chunks": 873
    }
  },
  {
    "text": "aussies <- window(ausair, end=2011)\nfit <- tslm(aussies ~ guinearice)\nsummary(fit)\n#> \n#> Call:\n#> tslm(formula = aussies ~ guinearice)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -5.945 -1.892 -0.327  1.862 10.421 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    -7.49       1.20   -6.23  2.3e-07 ***\n#> guinearice     40.29       1.34   30.13  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.24 on 40 degrees of freedom\n#> Multiple R-squared:  0.958,  Adjusted R-squared:  0.957 \n#> F-statistic:  908 on 1 and 40 DF,  p-value: <2e-16\ncheckresiduals(fit)\n162\n\n\nFigure 5.13: Residuals from a spurious regression.\n#> \n#>  Breusch-Godfrey test for serial correlation of order up to 8\n#> \n#> data:  Residuals from Linear regression model\n#> LM test = 29, df = 8, p-value = 3e-04\n163",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 296,
      "total_chunks": 873
    }
  },
  {
    "text": "5.45.4  Some useful predictorsSome useful predictors\nThere are several useful predictors that occur frequently when using regression for\ntime series data.\nTrendTrend\nIt is common for time series data to be trending. A linear trend can be modelled by\nsimply using  as a predictor,\nwhere . A trend variable can be specified in the ıtslm()ı function using\nthe ıtrendı predictor. In Section 5.8  we discuss how we can also model a nonlinear\ntrends.\nDummy variablesDummy variables\nSo far, we have assumed that each predictor takes numerical values. But what about\nwhen a predictor is a categorical variable taking only two values (e.g., “yes” and\n“no”)? Such a variable might arise, for example, when forecasting daily sales and\nyou want to take account of whether the day is a public holidaypublic holiday  or not. So the\npredictor takes value “yes” on a public holiday, and “no” otherwise.\nThis situation can still be handled within the framework of multiple regression",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 297,
      "total_chunks": 873
    }
  },
  {
    "text": "predictor takes value “yes” on a public holiday, and “no” otherwise.\nThis situation can still be handled within the framework of multiple regression\nmodels by creating a “dummy variable” which takes value 1 corresponding to “yes”\nand 0 corresponding to “no”. A dummy variable is also known as an “indicator\nvariable”.\nA dummy variable can also be used to account for an outlieroutlier in the data. Rather than\nomit the outlier, a dummy variable removes its effect. In this case, the dummy\nvariable takes value 1 for that observation and 0 everywhere else. An example is the\ncase where a special event has occurred. For example when forecasting tourist\narrivals to Brazil, we will need to account for the effect of the Rio de Janeiro summer\nOlympics in 2016.\n164",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 298,
      "total_chunks": 873
    }
  },
  {
    "text": "If there are more than two categories, then the variable can be coded using several\ndummy variables (one fewer than the total number of categories). ıtslm()ı will\nautomatically handle this case if you specify a factor variable as a predictor. There is\nusually no need to manually create the corresponding dummy variables.\nSeasonal dummy variablesSeasonal dummy variables\nSuppose that we are forecasting daily data and we want to account for the day of the\nweek as a predictor. Then the following dummy variables can be created.\nMonday 1 0 0 0 0 0\nTuesday 0 1 0000\nWednesday 0 0 1 0 0 0\nThursday 0001 00\nFriday 00001 0\nSaturday 000001\nSunday 0 0 0 0 0 0\nMonday 1 00000\n⋮⋮⋮⋮⋮⋮⋮\nNotice that only six dummy variables are needed to code seven categories. That is\nbecause the seventh category (in this case Sunday) is captured by the intercept, and\nis specified when the dummy variables are all set to zero.\nMany beginners will try to add a seventh dummy variable for the seventh category.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 299,
      "total_chunks": 873
    }
  },
  {
    "text": "is specified when the dummy variables are all set to zero.\nMany beginners will try to add a seventh dummy variable for the seventh category.\nThis is known as the “dummy variable trap”, because it will cause the regression to\nfail. There will be one too many parameters to estimate when an intercept is also\nincluded. The general rule is to use one fewer dummy variables than categories. So\nfor quarterly data, use three dummy variables; for monthly data, use 11 dummy\nvariables; and for daily data, use six dummy variables, and so on.\nThe interpretation of each of the coefficients associated with the dummy variables is\nthat it is a measure of the effect of that category relative to the omitted category . In\nthe above example, the coefficient of  associated with Monday will measure the\neffect of Monday on the forecast variable compared to the effect of Sunday. An\nexample of interpreting estimated dummy variable coefficients capturing the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 300,
      "total_chunks": 873
    }
  },
  {
    "text": "effect of Monday on the forecast variable compared to the effect of Sunday. An\nexample of interpreting estimated dummy variable coefficients capturing the\nquarterly seasonality of Australian beer production follows.\nThe ıtslm()ı function will automatically handle this situation if you specify the\npredictor ıseasonı.165",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 301,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Australian quarterly beer productionExample: Australian quarterly beer production\nRecall the Australian quarterly beer production data shown again in Figure 5.14.\nFigure 5.14: Australian quarterly beer production.\nWe want to forecast the value of future beer production. We can model this data\nusing a regression model with a linear trend and quarterly dummy variables,\nwhere  if  is in quarter  and 0 otherwise. The first quarter variable has been\nomitted, so the coefficients associated with the other quarters are measures of the\ndifference between those quarters and the first quarter.\nbeer2 <- window(ausbeer, start=1992 )\nautoplot(beer2) + xlab(\"Year\") + ylab(\"Megalitres\")\n166",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 302,
      "total_chunks": 873
    }
  },
  {
    "text": "Note that ıtrendı and ıseasonı are not objects in the R workspace; they are created\nautomatically by ıtslm()ı when specified in this way.\nThere is an average downward trend of -0.34 megalitres per quarter. On average, the\nsecond quarter has production of 34.7 megalitres lower than the first quarter, the\nthird quarter has production of 17.8 megalitres lower than the first quarter, and the\nfourth quarter has production of 72.8 megalitres higher than the first quarter.\nfit.beer <- tslm(beer2 ~ trend + season)\nsummary(fit.beer)\n#> \n#> Call:\n#> tslm(formula = beer2 ~ trend + season)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -42.90  -7.60  -0.46   7.99  21.79 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 441.8004     3.7335  118.33  < 2e-16 ***\n#> trend        -0.3403     0.0666   -5.11  2.7e-06 ***\n#> season2     -34.6597     3.9683   -8.73  9.1e-13 ***\n#> season3     -17.8216     4.0225   -4.43  3.4e-05 ***",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 303,
      "total_chunks": 873
    }
  },
  {
    "text": "#> trend        -0.3403     0.0666   -5.11  2.7e-06 ***\n#> season2     -34.6597     3.9683   -8.73  9.1e-13 ***\n#> season3     -17.8216     4.0225   -4.43  3.4e-05 ***\n#> season4      72.7964     4.0230   18.09  < 2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.2 on 69 degrees of freedom\n#> Multiple R-squared:  0.924,  Adjusted R-squared:  0.92 \n#> F-statistic:  211 on 4 and 69 DF,  p-value: <2e-16\nautoplot(beer2, series=\"Data\") +\n  autolayer(fitted(fit.beer), series=\"Fitted\") +\n  xlab(\"Year\") + ylab(\"Megalitres\") +\n  ggtitle(\"Quarterly Beer Production\")\n167",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 304,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.15: Time plot of beer production and predicted beer production.\ncbind(Data=beer2, Fitted=fitted(fit.beer)) %>%\n  as.data.frame() %>%\n  ggplot(aes(x = Data, y = Fitted,\n             colour = as.factor(cycle(beer2)))) +\n    geom_point() +\n    ylab(\"Fitted\") + xlab(\"Actual values\") +\n    ggtitle(\"Quarterly beer production\") +\n    scale_colour_brewer(palette=\"Dark2\", name= \"Quarter\") +\n    geom_abline(intercept=0, slope=1)\n168",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 305,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.16: Actual beer production plotted against predicted beer production.\nIntervention variablesIntervention variables\nIt is often necessary to model interventions that may have affected the variable to be\nforecast. For example, competitor activity, advertising expenditure, industrial\naction, and so on, can all have an effect.\nWhen the effect lasts only for one period, we use a “spike” variable. This is a dummy\nvariable that takes value one in the period of the intervention and zero elsewhere. A\nspike variable is equivalent to a dummy variable for handling an outlier.\nOther interventions have an immediate and permanent effect. If an intervention\ncauses a level shift (i.e., the value of the series changes suddenly and permanently\nfrom the time of intervention), then we use a “step” variable. A step variable takes\nvalue zero before the intervention and one from the time of intervention onward.\nAnother form of permanent effect is a change of slope. Here the intervention is",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 306,
      "total_chunks": 873
    }
  },
  {
    "text": "value zero before the intervention and one from the time of intervention onward.\nAnother form of permanent effect is a change of slope. Here the intervention is\nhandled using a piecewise linear trend; a trend that bends at the time of intervention\nand hence is nonlinear. We will discuss this in Section 5.8.\n169",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 307,
      "total_chunks": 873
    }
  },
  {
    "text": "Trading daysTrading days\nThe number of trading days in a month can vary considerably and can have a\nsubstantial effect on sales data. To allow for this, the number of trading days in each\nmonth can be included as a predictor.\nFor monthly or quarterly data, the ıbizdays()ı function will compute the number of\ntrading days in each period.\nAn alternative that allows for the effects of different days of the week has the\nfollowing predictors:\nDistributed lagsDistributed lags\nIt is often useful to include advertising expenditure as a predictor. However, since\nthe effect of advertising can last beyond the actual campaign, we need to include\nlagged values of advertising expenditure. Thus, the following predictors may be\nused.\nIt is common to require the coefficients to decrease as the lag increases, although\nthis is beyond the scope of this book.\nEasterEaster\nEaster differs from most holidays because it is not held on the same date each year,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 308,
      "total_chunks": 873
    }
  },
  {
    "text": "this is beyond the scope of this book.\nEasterEaster\nEaster differs from most holidays because it is not held on the same date each year,\nand its effect can last for several days. In this case, a dummy variable can be used\nwith value one where the holiday falls in the particular time period and zero\notherwise.\n170",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 309,
      "total_chunks": 873
    }
  },
  {
    "text": "With monthly data, if Easter falls in March then the dummy variable takes value 1 in\nMarch, and if it falls in April the dummy variable takes value 1 in April. When Easter\nstarts in March and finishes in April, the dummy variable is split proportionally\nbetween months.\nThe ıeaster()ı function will compute the dummy variable for you.\nFourier seriesFourier series\nAn alternative to using seasonal dummy variables, especially for long seasonal\nperiods, is to use Fourier terms. Jean-Baptiste Fourier was a French mathematician,\nborn in the 1700s, who showed that a series of sine and cosine terms of the right\nfrequencies can approximate any periodic function. We can use them for seasonal\npatterns.\nIf  is the seasonal period, then the first few Fourier terms are given by\nand so on. If we have monthly seasonality, and we use the first 11 of these predictor\nvariables, then we will get exactly the same forecasts as using 11 dummy variables.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 310,
      "total_chunks": 873
    }
  },
  {
    "text": "and so on. If we have monthly seasonality, and we use the first 11 of these predictor\nvariables, then we will get exactly the same forecasts as using 11 dummy variables.\nWith Fourier terms, we often need fewer predictors than with dummy variables,\nespecially when  is large. This makes them useful for weekly data, for example,\nwhere . For short seasonal periods (e.g., quarterly data), there is little\nadvantage in using Fourier terms over seasonal dummy variables.\nThese Fourier terms are produced using the ıfourier()ı function. For example, the\nAustralian beer data can be modelled like this.\n171",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 311,
      "total_chunks": 873
    }
  },
  {
    "text": "The first argument to ıfourier()ı allows it to identify the seasonal period  and the\nlength of the predictors to return. The second argument ıKı specifies how many pairs\nof sin and cos terms to include. The maximum allowed is  where  is the\nseasonal period. Because we have used the maximum here, the results are identical\nto those obtained when using seasonal dummy variables.\nIf only the first two Fourier terms are used (  and ), the seasonal pattern will\nfollow a simple sine wave. A regression model containing Fourier terms is often\ncalled a harmonic regressionharmonic regression  because the successive Fourier terms represent\nharmonics of the first two Fourier terms.\nfourier.beer <- tslm(beer2 ~ trend + fourier(beer2, K=2))\nsummary(fourier.beer)\n#> \n#> Call:\n#> tslm(formula = beer2 ~ trend + fourier(beer2, K = 2))\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -42.90  -7.60  -0.46   7.99  21.79 \n#> \n#> Coefficients:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 312,
      "total_chunks": 873
    }
  },
  {
    "text": "#> \n#> Call:\n#> tslm(formula = beer2 ~ trend + fourier(beer2, K = 2))\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -42.90  -7.60  -0.46   7.99  21.79 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               446.8792     2.8732  155.53  < 2e-16 ***\n#> trend                      -0.3403     0.0666   -5.11  2.7e-06 ***\n#> fourier(beer2, K = 2)S1-4   8.9108     2.0112    4.43  3.4e-05 ***\n#> fourier(beer2, K = 2)C1-4  53.7281     2.0112   26.71  < 2e-16 ***\n#> fourier(beer2, K = 2)C2-4  13.9896     1.4226    9.83  9.3e-15 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 12.2 on 69 degrees of freedom\n#> Multiple R-squared:  0.924,  Adjusted R-squared:  0.92 \n#> F-statistic:  211 on 4 and 69 DF,  p-value: <2e-16\n172",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 313,
      "total_chunks": 873
    }
  },
  {
    "text": "5.55.5  Selecting predictorsSelecting predictors\nWhen there are many possible predictors, we need some strategy for selecting the\nbest predictors to use in a regression model.\nA common approach that is not recommended  is to plot the forecast variable against\na particular predictor and if there is no noticeable relationship, drop that predictor\nfrom the model. This is invalid because it is not always possible to see the\nrelationship from a scatterplot, especially when the effects of other predictors have\nnot been accounted for.\nAnother common approach which is also invalid is to do a multiple linear regression\non all the predictors and disregard all variables whose -values are greater than\n0.05. To start with, statistical significance does not always indicate predictive value.\nEven if forecasting is not the goal, this is not a good strategy because the -values\ncan be misleading when two or more predictors are correlated with each other (see\nSection 5.9).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 314,
      "total_chunks": 873
    }
  },
  {
    "text": "Even if forecasting is not the goal, this is not a good strategy because the -values\ncan be misleading when two or more predictors are correlated with each other (see\nSection 5.9).\nInstead, we will use a measure of predictive accuracy. Five such measures are\nintroduced in this section. They can be calculated using the ıCV()ı function, here\napplied to the model for US consumption:\nWe compare these values against the corresponding values from other models. For\nthe CV, AIC, AICc and BIC measures, we want to find the model with the lowest value;\nfor Adjusted , we seek the model with the highest value.\nAdjusted RAdjusted R\nComputer output for a regression will always give the  value, discussed in Section\n5.2. However, it is not a good measure of the predictive ability of a model. It\nmeasures how well the model fits the historical data, but not how well the model will\nforecast future data.\nCV(fit.consMR)\n#>        CV       AIC      AICc       BIC     AdjR2",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 315,
      "total_chunks": 873
    }
  },
  {
    "text": "measures how well the model fits the historical data, but not how well the model will\nforecast future data.\nCV(fit.consMR)\n#>        CV       AIC      AICc       BIC     AdjR2 \n#>    0.1163 -409.2980 -408.8314 -389.9114    0.7486\n173",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 316,
      "total_chunks": 873
    }
  },
  {
    "text": "In addition,  does not allow for “degrees of freedom”. Adding any variable tends\nto increase the value of , even if that variable is irrelevant. For these reasons,\nforecasters should not use  to determine whether a model will give good\npredictions, as it will lead to overfitting.\nAn equivalent idea is to select the model which gives the minimum sum of squared\nerrors (SSE), given by\nMinimising the SSE is equivalent to maximising  and will always choose the model\nwith the most variables, and so is not a valid way of selecting predictors.\nAn alternative which is designed to overcome these problems is the adjusted  (also\ncalled “R-bar-squared”):\nwhere  is the number of observations and  is the number of predictors. This is an\nimprovement on , as it will no longer increase with each added predictor. Using\nthis measure, the best model will be the one with the largest value of . Maximising\n is equivalent to minimising the standard error  given in Equation (5.3).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 317,
      "total_chunks": 873
    }
  },
  {
    "text": "this measure, the best model will be the one with the largest value of . Maximising\n is equivalent to minimising the standard error  given in Equation (5.3).\nMaximising  works quite well as a method of selecting predictors, although it does\ntend to err on the side of selecting too many predictors.\nCross-validationCross-validation\nTime series cross-validation was introduced in Section 3.4 as a general tool for\ndetermining the predictive ability of a model. For regression models, it is also\npossible to use classical leave-one-out cross-validation to selection predictors\n(Bergmeir, Hyndman, & Koo, 2018). This is faster and makes more efficient use of\nthe data. The procedure uses the following steps:\n1. Remove observation  from the data set, and fit the model using the remaining\ndata. Then compute the error ( ) for the omitted observation. (This is\nnot the same as the residual because the th observation was not used in\nestimating the value of .)\n2. Repeat step 1 for .\n174",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 318,
      "total_chunks": 873
    }
  },
  {
    "text": "3. Compute the MSE from . We shall call this the CVCV.\nAlthough this looks like a time-consuming procedure, there are fast methods of\ncalculating CV, so that it takes no longer than fitting one model to the full data set.\nThe equation for computing CV efficiently is given in Section 5.7 . Under this\ncriterion, the best model is the one with the smallest value of CV.\nAkaike’s Information CriterionAkaike’s Information Criterion\nA closely-related method is Akaike’s Information Criterion, which we define as\nwhere  is the number of observations used for estimation and  is the number of\npredictors in the model. Different computer packages use slightly different\ndefinitions for the AIC, although they should all lead to the same model being\nselected. The  part of the equation occurs because there are  parameters in\nthe model: the  coefficients for the predictors, the intercept and the variance of the\nresiduals. The idea here is to penalise the fit of the model (SSE) with the number of",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 319,
      "total_chunks": 873
    }
  },
  {
    "text": "the model: the  coefficients for the predictors, the intercept and the variance of the\nresiduals. The idea here is to penalise the fit of the model (SSE) with the number of\nparameters that need to be estimated.\nThe model with the minimum value of the AIC is often the best model for\nforecasting. For large values of , minimising the AIC is equivalent to minimising\nthe CV value.\nCorrected Akaike’s Information Criterion\nFor small values of , the AIC tends to select too many predictors, and so a bias-\ncorrected version of the AIC has been developed,\nAs with the AIC, the AICc should be minimised.\n175",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 320,
      "total_chunks": 873
    }
  },
  {
    "text": "Schwarz’s Bayesian Information CriterionSchwarz’s Bayesian Information Criterion\nA related measure is Schwarz’s Bayesian Information Criterion (usually abbreviated\nto BIC, SBIC or SC):\nAs with the AIC, minimising the BIC is intended to give the best model. The model\nchosen by the BIC is either the same as that chosen by the AIC, or one with fewer\nterms. This is because the BIC penalises the number of parameters more heavily than\nthe AIC. For large values of , minimising BIC is similar to leave- -out cross-\nvalidation when .\nWhich measure should we use?Which measure should we use?\nWhile  is widely used, and has been around longer than the other measures, its\ntendency to select too many predictor variables makes it less suitable for forecasting.\nMany statisticians like to use the BIC because it has the feature that if there is a true\nunderlying model, the BIC will select that model given enough data. However, in",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 321,
      "total_chunks": 873
    }
  },
  {
    "text": "Many statisticians like to use the BIC because it has the feature that if there is a true\nunderlying model, the BIC will select that model given enough data. However, in\nreality, there is rarely, if ever, a true underlying model, and even if there was a true\nunderlying model, selecting that model will not necessarily give the best forecasts\n(because the parameter estimates may not be accurate).\nConsequently, we recommend that one of the AICc, AIC, or CV statistics be used, each\nof which has forecasting as their objective. If the value of  is large enough, they will\nall lead to the same model. In most of the examples in this book, we use the AICc\nvalue to select the forecasting model.\nExample: US consumptionExample: US consumption\nIn the multiple regression example for forecasting US consumption we considered\nfour predictors. With four predictors, there are  possible models. Now we can\ncheck if all four predictors are actually useful, or whether we can drop one or more of",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 322,
      "total_chunks": 873
    }
  },
  {
    "text": "four predictors. With four predictors, there are  possible models. Now we can\ncheck if all four predictors are actually useful, or whether we can drop one or more of\nthem. All 16 models were fitted and the results are summarised in Table 5.1. A “1”\nindicates that the predictor was included in the model, and a “0” means that the\npredictor was not included in the model. Hence the first row shows the measures of\npredictive accuracy for a model including all four predictors.\n176",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 323,
      "total_chunks": 873
    }
  },
  {
    "text": "The results have been sorted according to the AICc. Therefore the best models are\ngiven at the top of the table, and the worst at the bottom of the table.\nTable 5.1: All 16 possible models for forecasting US consumption with 4 predictors.\nIncomeIncome ProductionProduction SavingsSavings UnemploymentUnemployment CVCV AICAIC AICcAICc BICBIC AdjR2AdjR2\n1 1 1 1 0.116 -409.3 -408.8 -389.9 0.749\n1 0 1 1 0.116 -408.1 -407.8 -391.9 0.746\n1 1 1 0 0.118 -407.5 -407.1 -391.3 0.745\n1 0 1 0 0.129 -388.7 -388.5 -375.8 0.716\n1 1 0 1 0.278 -243.2 -242.8 -227.0 0.386\n1 0 0 1 0.283 -237.9 -237.7 -225.0 0.365\n1 1 0 0 0.289 -236.1 -235.9 -223.2 0.359\n0 1 1 1 0.293 -234.4 -234.0 -218.2 0.356\n0 1 1 0 0.300 -228.9 -228.7 -216.0 0.334\n0 1 0 1 0.303 -226.3 -226.1 -213.4 0.324\n0 0 1 1 0.306 -224.6 -224.4 -211.7 0.318\n0 1 0 0 0.314 -219.6 -219.5 -209.9 0.296\n0 0 0 1 0.314 -217.7 -217.5 -208.0 0.288\n1 0 0 0 0.372 -185.4 -185.3 -175.7 0.154\n0 0 1 0 0.414 -164.1 -164.0 -154.4 0.052",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 324,
      "total_chunks": 873
    }
  },
  {
    "text": "0 1 0 0 0.314 -219.6 -219.5 -209.9 0.296\n0 0 0 1 0.314 -217.7 -217.5 -208.0 0.288\n1 0 0 0 0.372 -185.4 -185.3 -175.7 0.154\n0 0 1 0 0.414 -164.1 -164.0 -154.4 0.052\n0 0 0 0 0.432 -155.1 -155.0 -148.6 0.000\nThe best model contains all four predictors. However, a closer look at the results\nreveals some interesting features. There is clear separation between the models in\nthe first four rows and the ones below. This indicates that Income and Savings are\nboth more important variables than Production and Unemployment. Also, the first\ntwo rows have almost identical values of CV, AIC and AICc. So we could possibly drop\nthe Production variable and get similar forecasts. Note that Production and\nUnemployment are highly (negatively) correlated, as shown in Figure 5.5, so most of\nthe predictive information in Production is also contained in the Unemployment\nvariable.\nBest subset regressionBest subset regression\nWhere possible, all potential regression models should be fitted (as was done in the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 325,
      "total_chunks": 873
    }
  },
  {
    "text": "variable.\nBest subset regressionBest subset regression\nWhere possible, all potential regression models should be fitted (as was done in the\nexample above) and the best model should be selected based on one of the measures\ndiscussed. This is known as “best subsets” regression or “all possible subsets”\nregression.\n177",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 326,
      "total_chunks": 873
    }
  },
  {
    "text": "Stepwise regressionStepwise regression\nIf there are a large number of predictors, it is not possible to fit all possible models.\nFor example, 40 predictors leads to  1 trillion possible models! Consequently, a\nstrategy is required to limit the number of models to be explored.\nAn approach that works quite well is backwards stepwise regression :\nStart with the model containing all potential predictors.\nRemove one predictor at a time. Keep the model if it improves the measure of\npredictive accuracy.\nIterate until no further improvement.\nIf the number of potential predictors is too large, then the backwards stepwise\nregression will not work and forward stepwise regression  can be used instead. This\nprocedure starts with a model that includes only the intercept. Predictors are added\none at a time, and the one that most improves the measure of predictive accuracy is\nretained in the model. The procedure is repeated until no further improvement can\nbe achieved.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 327,
      "total_chunks": 873
    }
  },
  {
    "text": "one at a time, and the one that most improves the measure of predictive accuracy is\nretained in the model. The procedure is repeated until no further improvement can\nbe achieved.\nAlternatively for either the backward or forward direction, a starting model can be\none that includes a subset of potential predictors. In this case, an extra step needs to\nbe included. For the backwards procedure we should also consider adding a predictor\nwith each step, and for the forward procedure we should also consider dropping a\npredictor with each step. These are referred to as hybrid procedures.\nIt is important to realise that any stepwise approach is not guaranteed to lead to the\nbest possible model, but it almost always leads to a good model. For further details\nsee James, Witten, Hastie, & Tibshirani (2014).\nBeware of inference after selecting predictorsBeware of inference after selecting predictors\nWe do not discuss statistical inference of the predictors in this book (e.g., looking at",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 328,
      "total_chunks": 873
    }
  },
  {
    "text": "Beware of inference after selecting predictorsBeware of inference after selecting predictors\nWe do not discuss statistical inference of the predictors in this book (e.g., looking at\n-values associated with each predictor). If you do wish to look at the statistical\nsignificance of the predictors, beware that any procedure involving selecting\npredictors first will invalidate the assumptions behind the -values. The procedures\nwe recommend for selecting predictors are helpful when the model is used for\nforecasting; they are not helpful if you wish to study the effect of any predictor on\nthe forecast variable.\n178",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 329,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nBergmeir, C., Hyndman, R. J., & Koo, B. (2018). A note on the validity of cross-\nvalidation for evaluating autoregressive time series prediction. Computational\nStatistics and Data Analysis , 120, 70–83. [DOI]\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2014). An introduction to\nstatistical learning: With applications in R . New York: Springer. [Amazon]\n179",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 330,
      "total_chunks": 873
    }
  },
  {
    "text": "5.65.6  Forecasting with regressionForecasting with regression\nRecall that predictions of  can be obtained using\nwhich comprises the estimated coefficients and ignores the error in the regression\nequation. Plugging in the values of the predictor variables  for\n returned the fitted (training-sample) values of . What we are\ninterested in here, however, is forecasting future values of .\nEx-ante versus ex-post forecastsEx-ante versus ex-post forecasts\nWhen using regression models for time series data, we need to distinguish between\nthe different types of forecasts that can be produced, depending on what is assumed\nto be known when the forecasts are computed.\nEx-ante forecastsEx-ante forecasts  are those that are made using only the information that is\navailable in advance. For example, ex-ante forecasts for the percentage change in US\nconsumption for quarters following the end of the sample, should only use\ninformation that was available up to and including  2016 Q3. These are genuine",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 331,
      "total_chunks": 873
    }
  },
  {
    "text": "consumption for quarters following the end of the sample, should only use\ninformation that was available up to and including  2016 Q3. These are genuine\nforecasts, made in advance using whatever information is available at the time.\nTherefore in order to generate ex-ante forecasts, the model requires forecasts of the\npredictors. To obtain these we can use one of the simple methods introduced in\nSection 3.1 or more sophisticated pure time series approaches that follow in Chapters\n7 and 8. Alternatively, forecasts from some other source, such as a government\nagency, may be available and can be used.\nEx-post forecasts  are those that are made using later information on the predictors.\nFor example, ex-post forecasts of consumption may use the actual observations of\nthe predictors, once these have been observed. These are not genuine forecasts, but\nare useful for studying the behaviour of forecasting models.\nThe model from which ex-post forecasts are produced should not be estimated using",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 332,
      "total_chunks": 873
    }
  },
  {
    "text": "are useful for studying the behaviour of forecasting models.\nThe model from which ex-post forecasts are produced should not be estimated using\ndata from the forecast period. That is, ex-post forecasts can assume knowledge of\nthe predictor variables (the  variables), but should not assume knowledge of the\ndata that are to be forecast (the  variable).\n180",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 333,
      "total_chunks": 873
    }
  },
  {
    "text": "A comparative evaluation of ex-ante forecasts and ex-post forecasts can help to\nseparate out the sources of forecast uncertainty. This will show whether forecast\nerrors have arisen due to poor forecasts of the predictor or due to a poor forecasting\nmodel.\nExample: Australian quarterly beer productionExample: Australian quarterly beer production\nNormally, we cannot use actual future values of the predictor variables when\nproducing ex-ante forecasts because their values will not be known in advance.\nHowever, the special predictors introduced in Section 5.4 are all known in advance,\nas they are based on calendar variables (e.g., seasonal dummy variables or public\nholiday indicators) or deterministic functions of time (e.g.Ö time trend). In such cases,\nthere is no difference between ex-ante and ex-post forecasts.\nbeer2 <- window(ausbeer, start=1992 )\nfit.beer <- tslm(beer2 ~ trend + season)\nfcast <- forecast(fit.beer)\nautoplot(fcast) +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 334,
      "total_chunks": 873
    }
  },
  {
    "text": "there is no difference between ex-ante and ex-post forecasts.\nbeer2 <- window(ausbeer, start=1992 )\nfit.beer <- tslm(beer2 ~ trend + season)\nfcast <- forecast(fit.beer)\nautoplot(fcast) +\n  ggtitle(\"Forecasts of beer production using regression\" ) +\n  xlab(\"Year\") + ylab(\"megalitres\")\n181",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 335,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.17: Forecasts from the regression model for beer production. The dark shaded\nregion shows 80% prediction intervals and the light shaded region shows 95% prediction\nintervals.\nScenario based forecastingScenario based forecasting\nIn this setting, the forecaster assumes possible scenarios for the predictor variables\nthat are of interest. For example, a US policy maker may be interested in comparing\nthe predicted change in consumption when there is a constant growth of 1% and\n0.5% respectively for income and savings with no change in the employment rate,\nversus a respective decline of 1% and 0.5%, for each of the four quarters following\nthe end of the sample. The resulting forecasts are calculated below and shown in\nFigure 5.18. We should note that prediction intervals for scenario based forecasts do\nnot include the uncertainty associated with the future values of the predictor\nvariables. They assume that the values of the predictors are known in advance.\nfit.consBest <- tslm(",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 336,
      "total_chunks": 873
    }
  },
  {
    "text": "not include the uncertainty associated with the future values of the predictor\nvariables. They assume that the values of the predictors are known in advance.\nfit.consBest <- tslm(\n  Consumption ~ Income + Savings + Unemployment,\n  data = uschange)\nh <- 4\nnewdata <- data.frame(\n    Income = c(1, 1, 1, 1),\n    Savings = c(0.5, 0.5, 0.5, 0.5),\n    Unemployment = c(0, 0, 0, 0))\nfcast.up <- forecast(fit.consBest, newdata = newdata)\nnewdata <- data.frame(\n    Income = rep(-1, h),\n    Savings = rep(-0.5, h),\n    Unemployment = rep(0, h))\nfcast.down <- forecast(fit.consBest, newdata = newdata)\nautoplot(uschange[, 1 ]) +\n  ylab(\"% change in US consumption\") +\n  autolayer(fcast.up, PI = TRUE, series = \"increase\") +\n  autolayer(fcast.down, PI =  TRUE, series = \"decrease\") +\n  guides(colour = guide_legend(title = \"Scenario\"))\n182",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 337,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.18: Forecasting percentage changes in personal consumption expenditure for the\nUS under scenario based forecasting.\nBuilding a predictive regression modelBuilding a predictive regression model\nThe great advantage of regression models is that they can be used to capture\nimportant relationships between the forecast variable of interest and the predictor\nvariables. A major challenge however, is that in order to generate ex-ante forecasts,\nthe model requires future values of each predictor. If scenario based forecasting is of\ninterest then these models are extremely useful. However, if ex-ante forecasting is\nthe main focus, obtaining forecasts of the predictors can be challenging (in many\ncases generating forecasts for the predictor variables can be more challenging than\nforecasting directly the forecast variable without using predictors).\nAn alternative formulation is to use as predictors their lagged values. Assuming that",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 338,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasting directly the forecast variable without using predictors).\nAn alternative formulation is to use as predictors their lagged values. Assuming that\nwe are interested in generating a -step ahead forecast we write\nfor . The predictor set is formed by values of the s that are observed \ntime periods prior to observing . Therefore when the estimated model is projected\ninto the future, i.e., beyond the end of the sample , all predictor values are\navailable.\n183",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 339,
      "total_chunks": 873
    }
  },
  {
    "text": "Including lagged values of the predictors does not only make the model operational\nfor easily generating forecasts, it also makes it intuitively appealing. For example,\nthe effect of a policy change with the aim of increasing production may not have an\ninstantaneous effect on consumption expenditure. It is most likely that this will\nhappen with a lagging effect. We touched upon this in Section 5.4 when briefly\nintroducing distributed lags as predictors. Several directions for generalising\nregression models to better incorporate the rich dynamics observed in time series\nare discussed in Section 9.\nPrediction intervals\nWith each forecast for the change in consumption in Figure 5.18, 95% and 80%\nprediction intervals are also included. The general formulation of how to calculate\nprediction intervals for multiple regression models is presented in Section 5.7. As\nthis involves some advanced matrix algebra we present here the case for calculating",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 340,
      "total_chunks": 873
    }
  },
  {
    "text": "prediction intervals for multiple regression models is presented in Section 5.7. As\nthis involves some advanced matrix algebra we present here the case for calculating\nprediction intervals for a simple regression, where a forecast can be generated using\nthe equation,\nAssuming that the regression errors are normally distributed, an approximate 95%\nprediction interval associated with this forecast is given by\nwhere  is the total number of observations,  is the mean of the observed  values,\n is the standard deviation of the observed  values and  is the standard error of\nthe regression given by Equation (5.3). Similarly, an 80% prediction interval can be\nobtained by replacing 1.96 by 1.28. Other prediction intervals can be obtained by\nreplacing the 1.96 with the appropriate value given in Table 3.1. If R is used to obtain\nprediction intervals, more exact calculations are obtained (especially for small\nvalues of ) than what is given by Equation (5.4).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 341,
      "total_chunks": 873
    }
  },
  {
    "text": "prediction intervals, more exact calculations are obtained (especially for small\nvalues of ) than what is given by Equation (5.4).\nEquation (5.4) shows that the prediction interval is wider when  is far from . That\nis, we are more certain about our forecasts when considering values of the predictor\nvariable close to its sample mean.\n184",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 342,
      "total_chunks": 873
    }
  },
  {
    "text": "ExampleExample\nThe estimated simple regression line in the US consumption example is\nAssuming that for the next four quarters, personal income will increase by its\nhistorical mean value of , consumption is forecast to increase by \nand the corresponding  and  prediction intervals are  and\n respectively (calculated using R). If we assume an extreme increase of\n in income, then the prediction intervals are considerably wider as shown in\nFigure 5.19 .\nfit.cons <- tslm(Consumption ~ Income, data = uschange)\nh <- 4\nfcast.ave <- forecast(fit.cons,\n  newdata = data.frame(\n    Income = rep(mean(uschange[,\"Income\"]), h)))\nfcast.up <- forecast(fit.cons,\n  newdata = data.frame(Income = rep(5, h)))\nautoplot(uschange[, \"Consumption\"]) +\n  ylab(\"% change in US consumption\") +\n  autolayer(fcast.ave, series = \"Average increase\",\n    PI = TRUE) +\n  autolayer(fcast.up, series = \"Extreme increase\",\n    PI = TRUE) +\n  guides(colour = guide_legend(title = \"Scenario\"))\n185",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 343,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.19: Prediction intervals if income is increased by its historical mean of \nversus an extreme increase of 5%.\n/g11/gF/g18/g13/g6\n186",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 344,
      "total_chunks": 873
    }
  },
  {
    "text": "5.75.7  Matrix formulationMatrix formulation\nWarning: this is a more advanced, optional section and assumes knowledge of matrix\nalgebra.\nRecall that multiple regression model can be written as\nwhere  has mean zero and variance . This expresses the relationship between a\nsingle value of the forecast variable and the predictors.\nIt can be convenient to write this in matrix form where all the values of the forecast\nvariable are given in a single equation. Let , ,\n and\nThen\nwhere  has mean  and variance . Note that the  matrix has  rows reflecting\nthe number of observations and  columns reflecting the intercept which is\nrepresented by the column of ones plus the number of predictors.\nLeast squares estimationLeast squares estimation\nLeast squares estimation is performed by minimising the expression\n. It can be shown that this is minimised when  takes the\nvalue\nThis is sometimes known as the “normal equation”. The estimated coefficients",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 345,
      "total_chunks": 873
    }
  },
  {
    "text": ". It can be shown that this is minimised when  takes the\nvalue\nThis is sometimes known as the “normal equation”. The estimated coefficients\nrequire the inversion of the matrix . If  is not of full column rank then matrix\n187",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 346,
      "total_chunks": 873
    }
  },
  {
    "text": "is singular and the model cannot be estimated. This will occur, for example, if\nyou fall for the “dummy variable trap”, i.e., having the same number of dummy\nvariables as there are categories of a categorical predictor, as discussed in Section\n5.4.\nThe residual variance is estimated using\nFitted values and cross-validationFitted values and cross-validation\nThe normal equation shows that the fitted values can be calculated using\nwhere  is known as the “hat-matrix” because it is used to\ncompute  (“y-hat”).\nIf the diagonal values of  are denoted by , then the cross-validation\nstatistic can be computed using\nwhere  is the residual obtained from fitting the model to all  observations. Thus,\nit is not necessary to actually fit  separate models when computing the CV statistic.\nForecasts and prediction intervalsForecasts and prediction intervals\nLet  be a row vector containing the values of the predictors (in the same format as",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 347,
      "total_chunks": 873
    }
  },
  {
    "text": "Forecasts and prediction intervalsForecasts and prediction intervals\nLet  be a row vector containing the values of the predictors (in the same format as\n) for which we want to generate a forecast . Then the forecast is given by\nand its estimated variance is given by\nA 95% prediction interval can be calculated (assuming normally distributed errors)\nas\n188",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 348,
      "total_chunks": 873
    }
  },
  {
    "text": "This takes into account the uncertainty due to the error term  and the uncertainty in\nthe coefficient estimates. However, it ignores any errors in . Thus, if the future\nvalues of the predictors are uncertain, then the prediction interval calculated using\nthis expression will be too narrow.\n/g3F/g35 /g65 /g12/gF/g1A/g17 /g3F/g55 /g21 /g12 /g12/gC/g34 /gC7 /g9/g1A /g9B /g1A /gA /gC3/g12 /g9/g34 /gC7 /gA /g9B /gF\n/g47\n/g34 /gC7\n189",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 349,
      "total_chunks": 873
    }
  },
  {
    "text": "5.85.8  Nonlinear regressionNonlinear regression\nAlthough the linear relationship assumed so far in this chapter is often adequate,\nthere are many cases in which a nonlinear functional form is more suitable. To keep\nthings simple in this section we assume that we only have one predictor .\nThe simplest way of modelling a nonlinear relationship is to transform the forecast\nvariable  and/or the predictor variable  before estimating a regression model.\nWhile this provides a non-linear functional form, the model is still linear in the\nparameters. The most commonly used transformation is the (natural) logarithm (see\nSection 3.2).\nA log-loglog-log functional form is specified as\nIn this model, the slope  can be interpreted as an elasticity:  is the average\npercentage change in  resulting from a  increase in . Other useful forms can\nalso be specified. The log-linearlog-linear  form is specified by only transforming the forecast",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 350,
      "total_chunks": 873
    }
  },
  {
    "text": "percentage change in  resulting from a  increase in . Other useful forms can\nalso be specified. The log-linearlog-linear  form is specified by only transforming the forecast\nvariable and the linear-loglinear-log  form is obtained by transforming the predictor.\nRecall that in order to perform a logarithmic transformation to a variable, all of its\nobserved values must be greater than zero. In the case that variable  contains zeros,\nwe use the transformation ; i.e., we add one to the value of the variable\nand then take logarithms. This has a similar effect to taking logarithms but avoids\nthe problem of zeros. It also has the neat side-effect of zeros on the original scale\nremaining zeros on the transformed scale.\nThere are cases for which simply transforming the data will not be adequate and a\nmore general specification may be required. Then the model we use is\nwhere  is a nonlinear function. In standard (linear) regression, . In",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 351,
      "total_chunks": 873
    }
  },
  {
    "text": "more general specification may be required. Then the model we use is\nwhere  is a nonlinear function. In standard (linear) regression, . In\nthe specification of nonlinear regression that follows, we allow  to be a more\nflexible nonlinear function of , compared to simply a logarithmic or other\ntransformation.\n190",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 352,
      "total_chunks": 873
    }
  },
  {
    "text": "One of the simplest specifications is to make  piecewise linearpiecewise linear . That is, we introduce\npoints where the slope of  can change. These points are called knotsknots. This can be\nachieved by letting  and introducing variable  such that\nThe notation  means the value  if it is positive and 0 otherwise. This\nforces the slope to bend at point . Additional bends can be included in the\nrelationship by adding further variables of the above form.\nAn example of this follows by considering  and fitting a piecewise linear trend\nto a time series.\nPiecewise linear relationships constructed in this way are a special case of regressionregression\nsplinessplines. In general, a linear regression spline is obtained using\nwhere  are the knots (the points at which the line can bend). Selecting\nthe number of knots ( ) and where they should be positioned can be difficult and\nsomewhat arbitrary. Some automatic knot selection algorithms are available in some",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 353,
      "total_chunks": 873
    }
  },
  {
    "text": "the number of knots ( ) and where they should be positioned can be difficult and\nsomewhat arbitrary. Some automatic knot selection algorithms are available in some\nsoftware, but are not yet widely used.\nA smoother result can be obtained using piecewise cubics rather than piecewise lines.\nThese are constrained to be continuous (they join up) and smooth (so that there are\nno sudden changes of direction, as we see with piecewise linear splines). In general,\na cubic regression spline is written as\nCubic splines usually give a better fit to the data. However, forecasts of  become\nunreliable when  is outside the range of the historical data.\nForecasting with a nonlinear trendForecasting with a nonlinear trend\nIn Section 5.4 fitting a linear trend to a time series by setting  was introduced.\nThe simplest way of fitting a nonlinear trend is using quadratic or higher order\ntrends obtained by specifying\n191",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 354,
      "total_chunks": 873
    }
  },
  {
    "text": "However, it is not recommended that quadratic or higher order trends be used in\nforecasting. When they are extrapolated, the resulting forecasts are often unrealistic.\nA better approach is to use the piecewise specification introduced above and fit a\npiecewise linear trend which bends at some point in time. We can think of this as a\nnonlinear trend constructed of linear pieces. If the trend bends at time , then it can\nbe specified by simply replacing  and  above such that we include the\npredictors,\nin the model. If the associated coefficients of  and  are  and , then  gives\nthe slope of the trend before time , while the slope of the line after time  is given\nby . Additional bends can be included in the relationship by adding further\nvariables of the form  where  is the “knot” or point in time at which the\nline should bend.\nExample: Boston marathon winning timesExample: Boston marathon winning times\nThe top panel of Figure 5.20 shows the Boston marathon winning times (in minutes).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 355,
      "total_chunks": 873
    }
  },
  {
    "text": "line should bend.\nExample: Boston marathon winning timesExample: Boston marathon winning times\nThe top panel of Figure 5.20 shows the Boston marathon winning times (in minutes).\nThe course was lengthened (from 24.5 miles to 26.2 miles) in 1924, which led to a\njump in the winning times, so we only consider data from that date onwards. The\ntime series shows a general downward trend as the winning times have been\nimproving over the years. The bottom panel shows the residuals from fitting a linear\ntrend to the data. The plot shows an obvious nonlinear pattern which has not been\ncaptured by the linear trend. There is also some heteroscedasticity, with decreasing\nvariation over time.\n192",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 356,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.20: Fitting a linear trend to the Boston marathon winning times is inadequate\nFitting an exponential trend (equivalent to a log-linear regression) to the data can be\nachieved by transforming the  variable so that the model to be fitted is,\nThis also addresses the heteroscedasticity. The fitted exponential trend and\nforecasts are shown in Figure 5.21. Although the exponential trend does not seem to\nfit the data much better than the linear trend, it gives a more sensible projection in\nthat the winning times will decrease in the future but at a decaying rate rather than a\nfixed linear rate.\nThe plot of winning times reveals three different periods. There is a lot of volatility\nin the winning times up to about 1950, with the winning times barely declining. After\n1950 there is a near-linear decrease in times, followed by a flattening out after the\n1980s, with the suggestion of an upturn towards the end of the sample. To account",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 357,
      "total_chunks": 873
    }
  },
  {
    "text": "1950 there is a near-linear decrease in times, followed by a flattening out after the\n1980s, with the suggestion of an upturn towards the end of the sample. To account\nfor these changes, we specify the years 1950 and 1980 as knots. We should warn here\nthat subjective identification of knots can lead to over-fitting, which can be\ndetrimental to the forecast performance of a model, and should be performed with\ncaution.\n/g35\n/g4D/g50/g48 /g35 /g30 /g1E /g44 /g11 /gC /g44 /g12 /g30 /gC /g47 /g30 /gF\n193",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 358,
      "total_chunks": 873
    }
  },
  {
    "text": "boston_men <- window(marathon, start=1924 )\nh <- 10\nfit.lin <- tslm(boston_men ~ trend)\nfcasts.lin <- forecast(fit.lin, h =  h)\nfit.exp <- tslm(boston_men ~ trend, lambda = 0)\nfcasts.exp <- forecast(fit.exp, h =  h)\nt <- time(boston_men)\nt.break1 <- 1950\nt.break2 <- 1980\ntb1 <- ts(pmax(0, t - t.break1), start = 1924)\ntb2 <- ts(pmax(0, t - t.break2), start = 1924)\nfit.pw <- tslm(boston_men ~ t + tb1 + tb2)\nt.new <- t[length(t)] + seq(h)\ntb1.new <- tb1[length(tb1)] + seq(h)\ntb2.new <- tb2[length(tb2)] + seq(h)\nnewdata <- cbind(t=t.new, tb1=tb1.new, tb2=tb2.new) %>%\n  as.data.frame()\nfcasts.pw <- forecast(fit.pw, newdata = newdata)\nfit.spline <- tslm(boston_men ~ t + I(t^2) + I(t^3) +\n  I(tb1^3) + I(tb2^3))\nfcasts.spl <- forecast(fit.spline, newdata = newdata)\nautoplot(boston_men) +\n  autolayer(fitted(fit.lin), series = \"Linear\") +\n  autolayer(fitted(fit.exp), series = \"Exponential\") +\n  autolayer(fitted(fit.pw), series = \"Piecewise\") +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 359,
      "total_chunks": 873
    }
  },
  {
    "text": "autoplot(boston_men) +\n  autolayer(fitted(fit.lin), series = \"Linear\") +\n  autolayer(fitted(fit.exp), series = \"Exponential\") +\n  autolayer(fitted(fit.pw), series = \"Piecewise\") +\n  autolayer(fitted(fit.spline), series = \"Cubic Spline\") +\n  autolayer(fcasts.pw, series=\"Piecewise\") +\n  autolayer(fcasts.lin, series=\"Linear\", PI=FALSE ) +\n  autolayer(fcasts.exp, series=\"Exponential\", PI=FALSE ) +\n  autolayer(fcasts.spl, series=\"Cubic Spline\", PI=FALSE ) +\n  xlab(\"Year\") + ylab(\"Winning times in minutes\") +\n194",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 360,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.21: Projecting forecasts from a linear, exponential, piecewise linear trends and a\ncubic spline for the Boston marathon winning times\nFigure 5.21 above shows the fitted lines and forecasts from linear, exponential,\npiecewise linear, and cubic spline trends. The best forecasts appear to come from the\npiecewise linear trend, while the cubic spline gives the best fit to the historical data\nbut poor forecasts.\nThere is an alternative formulation of cubic splines (called natural cubic smoothingnatural cubic smoothing\nsplinessplines) that imposes some constraints, so the spline function is linear at the end,\nwhich usually gives much better forecasts without compromising the fit. In Figure\n5.22, we have used the ısplinef()ı function to produce the cubic spline forecasts.\nThis uses many more knots than we used in Figure 5.21, but the coefficients are\nconstrained to prevent over-fitting, and the curve is linear at both ends. This has the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 361,
      "total_chunks": 873
    }
  },
  {
    "text": "This uses many more knots than we used in Figure 5.21, but the coefficients are\nconstrained to prevent over-fitting, and the curve is linear at both ends. This has the\nadded advantage that knot selection is not subjective. We have also used a log\ntransformation (ılambda=0ı) to handle the heteroscedasticity.\n  ggtitle(\"Boston Marathon\") +\n  guides(colour = guide_legend(title = \" \"))\nboston_men %>%\n  splinef(lambda=0) %>%\n  autoplot()\n195",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 362,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 5.22: Natural cubic smoothing splines applied to the marathon data. The forecasts\nare a linear projection of the trend at the end of the observed data.\nThe residuals plotted in Figure 5.23 show that this model has captured the trend\nwell, although there is some heteroscedasticity remaining. The wide prediction\ninterval associated with the forecasts reflects the volatility observed in the historical\nwinning times.\nboston_men %>%\n  splinef(lambda=0) %>%\n  checkresiduals()\n196\n\n\nFigure 5.23: Residuals from the cubic spline trend.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from Cubic Smoothing Spline\n#> Q* = 10, df = 10, p-value = 0.4\n#> \n#> Model df: 0.   Total lags used: 10\n197",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 363,
      "total_chunks": 873
    }
  },
  {
    "text": "5.95.9  Correlation, causation and forecastingCorrelation, causation and forecasting\nCorrelation is not causationCorrelation is not causation\nIt is important not to confuse correlation with causation, or causation with\nforecasting. A variable  may be useful for forecasting a variable , but that does not\nmean  is causing . It is possible that  is causing , but it may be that  is causing \n, or that the relationship between them is more complicated than simple causality.\nFor example, it is possible to model the number of drownings at a beach resort each\nmonth with the number of ice-creams sold in the same period. The model can give\nreasonable forecasts, not because ice-creams cause drownings, but because people\neat more ice-creams on hot days when they are also more likely to go swimming. So\nthe two variables (ice-cream sales and drownings) are correlated, but one is not\ncausing the other. They are both caused by a third variable (temperature). This is an",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 364,
      "total_chunks": 873
    }
  },
  {
    "text": "the two variables (ice-cream sales and drownings) are correlated, but one is not\ncausing the other. They are both caused by a third variable (temperature). This is an\nexample of “confounding” — where an omitted variable causes changes in both the\nresponse variable and at least one predictor variables.\nWe describe a variable that is not included in our forecasting model as a confounderconfounder\nwhen it influences both the response variable and at least one predictor variable.\nConfounding makes it difficult to determine what variables are causing changes in\nother variables, but it does not necessarily make forecasting more difficult.\nSimilarly, it is possible to forecast if it will rain in the afternoon by observing the\nnumber of cyclists on the road in the morning. When there are fewer cyclists than\nusual, it is more likely to rain later in the day. The model can give reasonable\nforecasts, not because cyclists prevent rain, but because people are more likely to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 365,
      "total_chunks": 873
    }
  },
  {
    "text": "usual, it is more likely to rain later in the day. The model can give reasonable\nforecasts, not because cyclists prevent rain, but because people are more likely to\ncycle when the published weather forecast is for a dry day. In this case, there is a\ncausal relationship, but in the opposite direction to our forecasting model. The\nnumber of cyclists falls because there is rain forecast. That is,  (rainfall) is affecting\n (cyclists).\nIt is important to understand that correlations are useful for forecasting, even when\nthere is no causal relationship between the two variables, or when the causality runs\nin the opposite direction to the model, or when there is confounding.\n198",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 366,
      "total_chunks": 873
    }
  },
  {
    "text": "However, often a better model is possible if a causal mechanism can be determined.\nA better model for drownings will probably include temperatures and visitor\nnumbers and exclude ice-cream sales. A good forecasting model for rainfall will not\ninclude cyclists, but it will include atmospheric observations from the previous few\ndays.\nForecasting with correlated predictorsForecasting with correlated predictors\nWhen two or more predictors are highly correlated it is always challenging to\naccurately separate their individual effects. Suppose we are forecasting monthly\nsales of a company for 2012, using data from 2000–2011. In January 2008, a new\ncompetitor came into the market and started taking some market share. At the same\ntime, the economy began to decline. In your forecasting model, you include both\ncompetitor activity (measured using advertising time on a local television station)\nand the health of the economy (measured using GDP). It will not be possible to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 367,
      "total_chunks": 873
    }
  },
  {
    "text": "competitor activity (measured using advertising time on a local television station)\nand the health of the economy (measured using GDP). It will not be possible to\nseparate the effects of these two predictors because they are highly correlated.\nHaving correlated predictors is not really a problem for forecasting, as we can still\ncompute forecasts without needing to separate out the effects of the predictors.\nHowever, it becomes a problem with scenario forecasting as the scenarios should\ntake account of the relationships between predictors. It is also a problem if some\nhistorical analysis of the contributions of various predictors is required.\nMulticollinearity and forecastingMulticollinearity and forecasting\nA closely related issue is multicollinearitymulticollinearity , which occurs when similar information is\nprovided by two or more of the predictor variables in a multiple regression.\nIt can occur when two predictors are highly correlated with each other (that is, they",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 368,
      "total_chunks": 873
    }
  },
  {
    "text": "provided by two or more of the predictor variables in a multiple regression.\nIt can occur when two predictors are highly correlated with each other (that is, they\nhave a correlation coefficient close to +1 or -1). In this case, knowing the value of one\nof the variables tells you a lot about the value of the other variable. Hence, they are\nproviding similar information. For example, foot size can be used to predict height,\nbut including the size of both left and right feet in the same model is not going to\nmake the forecasts any better, although it won’t make them worse either.\nMulticollinearity can also occur when a linear combination of predictors is highly\ncorrelated with another linear combination of predictors. In this case, knowing the\nvalue of the first group of predictors tells you a lot about the value of the second\ngroup of predictors. Hence, they are providing similar information.\n199",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 369,
      "total_chunks": 873
    }
  },
  {
    "text": "An example of this problem is the dummy variable trap discussed in Section 5.4.\nSuppose you have quarterly data and use four dummy variables, , ,  and .\nThen , so there is perfect correlation between  and\n.\nIn the case of perfect correlation (i.e., a correlation of +1 or -1, such as in the dummy\nvariable trap), it is not possible to estimate the regression model.\nIf there is high correlation (close to but not equal to +1 or -1), then the estimation of\nthe regression coefficients is computationally difficult. In fact, some software\n(notably Microsoft Excel) may give highly inaccurate estimates of the coefficients.\nMost reputable statistical software will use algorithms to limit the effect of\nmulticollinearity on the coefficient estimates, but you do need to be careful. The\nmajor software packages such as R, SPSS, SAS and Stata all use estimation\nalgorithms to avoid the problem as much as possible.\nWhen multicollinearity is present, the uncertainty associated with individual",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 370,
      "total_chunks": 873
    }
  },
  {
    "text": "algorithms to avoid the problem as much as possible.\nWhen multicollinearity is present, the uncertainty associated with individual\nregression coefficients will be large. This is because they are difficult to estimate.\nConsequently, statistical tests (e.g., t-tests) on regression coefficients are\nunreliable. (In forecasting we are rarely interested in such tests.) Also, it will not be\npossible to make accurate statements about the contribution of each separate\npredictor to the forecast.\nForecasts will be unreliable if the values of the future predictors are outside the\nrange of the historical values of the predictors. For example, suppose you have fitted\na regression model with predictors  and  which are highly correlated with each\nother, and suppose that the values of  in the fitting data ranged between 0 and 100.\nThen forecasts based on  or  will be unreliable. It is always a little\ndangerous when future values of the predictors lie much outside the historical range,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 371,
      "total_chunks": 873
    }
  },
  {
    "text": "Then forecasts based on  or  will be unreliable. It is always a little\ndangerous when future values of the predictors lie much outside the historical range,\nbut it is especially problematic when multicollinearity is present.\nNote that if you are using good statistical software, if you are not interested in the\nspecific contributions of each predictor, and if the future values of your predictor\nvariables are within their historical ranges, there is nothing to worry about —\nmulticollinearity is not a problem except when there is perfect correlation.\n/g20 /g12 /g20 /g13 /g20 /g14 /g20 /g15\n/g20 /g15 /g1E/g12 /gC3 /g20 /g12 /gC3 /g20 /g13 /gC3 /g20 /g14 /g20 /g15\n/g20 /g12 /gC /g20 /g13 /gC /g20 /g14\n/g34 /g12 /g34 /g13\n/g34 /g12\n/g34 /g12 /g1F /g12/g11/g11 /g34 /g12 /g1D/g11\n200",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 372,
      "total_chunks": 873
    }
  },
  {
    "text": "5.105.10  ExercisesExercises\n1. Daily electricity demand for Victoria, Australia, during 2014 is contained in\nıelecdailyı. The data for the first 20 days can be obtained as follows.\na. Plot the data and find the regression model for Demand with temperature as\nan explanatory variable. Why is there a positive relationship?\nb. Produce a residual plot. Is the model adequate? Are there any outliers or\ninfluential observations?\nc. Use the model to forecast the electricity demand that you would expect for\nthe next day if the maximum temperature was  and compare it with the\nforecast if the with maximum temperature was . Do you believe these\nforecasts?\nd. Give prediction intervals for your forecasts. The following R code will get you\nstarted:\ne. Plot Demand vs Temperature for all of the available data in ıelecdailyı. What\ndoes this say about your model?\n2. Data set ımens400ı contains the winning times (in seconds) for the men’s 400\nmeters final in each Olympic Games from 1896 to 2016.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 373,
      "total_chunks": 873
    }
  },
  {
    "text": "does this say about your model?\n2. Data set ımens400ı contains the winning times (in seconds) for the men’s 400\nmeters final in each Olympic Games from 1896 to 2016.\na. Plot the winning time against the year. Describe the main features of the\nplot.\ndaily20 <- head(elecdaily,20)\nautoplot(daily20, facets=TRUE )\ndaily20 %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Temperature, y=Demand)) +\n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE)\nfit <- tslm(Demand ~ Temperature, data=daily20)\ncheckresiduals(fit)\nforecast(fit, newdata=data.frame(Temperature=c(15,35)))\n201",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 374,
      "total_chunks": 873
    }
  },
  {
    "text": "b. Fit a regression line to the data. Obviously the winning times have been\ndecreasing, but at what average rate per year?\nc. Plot the residuals against the year. What does this indicate about the\nsuitability of the fitted line?\nd. Predict the winning time for the men’s 400 meters final in the 2020\nOlympics. Give a prediction interval for your forecasts. What assumptions\nhave you made in these calculations?\n3. Type ıeaster(ausbeer)ı and interpret what you see.\n4. An elasticity coefficient is the ratio of the percentage change in the forecast\nvariable ( ) to the percentage change in the predictor variable ( ).\nMathematically, the elasticity is defined as . Consider the log-\nlog model,\nExpress  as a function of  and show that the coefficient  is the elasticity\ncoefficient.\n5. The data set ıfancyı concerns the monthly sales figures of a shop which opened\nin January 1987 and sells gifts, souvenirs, and novelties. The shop is situated on",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 375,
      "total_chunks": 873
    }
  },
  {
    "text": "coefficient.\n5. The data set ıfancyı concerns the monthly sales figures of a shop which opened\nin January 1987 and sells gifts, souvenirs, and novelties. The shop is situated on\nthe wharf at a beach resort town in Queensland, Australia. The sales volume\nvaries with the seasonal population of tourists. There is a large influx of visitors\nto the town at Christmas and for the local surfing festival, held every March\nsince 1988. Over time, the shop has expanded its premises, range of products,\nand staff.\na. Produce a time plot of the data and describe the patterns in the graph.\nIdentify any unusual or unexpected fluctuations in the time series.\nb. Explain why it is necessary to take logarithms of these data before fitting a\nmodel.\nc. Use R to fit a regression model to the logarithms of these sales data with a\nlinear trend, seasonal dummies and a “surfing festival” dummy variable.\nd. Plot the residuals against time and against the fitted values. Do these plots",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 376,
      "total_chunks": 873
    }
  },
  {
    "text": "linear trend, seasonal dummies and a “surfing festival” dummy variable.\nd. Plot the residuals against time and against the fitted values. Do these plots\nreveal any problems with the model?\ne. Do boxplots of the residuals for each month. Does this reveal any problems\nwith the model?\nf. What do the values of the coefficients tell you about each variable?\ng. What does the Breusch-Godfrey test tell you about your model?\n/g35/g34\n/g9/g20/g35 /g10/g20/g34 /gA/g67/g9 /g34 /g10/g35 /gA\n/g4D/g50/g48 /g35 /g1E /g44 /g11 /gC /g44 /g12 /g4D/g50/g48 /g34 /gC /g47/gF\n/g35/g34 /g44 /g12\n202",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 377,
      "total_chunks": 873
    }
  },
  {
    "text": "h. Regardless of your answers to the above questions, use your regression\nmodel to predict the monthly sales for 1994, 1995, and 1996. Produce\nprediction intervals for each of your forecasts.\ni. Transform your predictions and intervals to obtain predictions and intervals\nfor the raw data.\nj. How could you improve these predictions by modifying the model?\n6. The ıgasolineı series consists of weekly data for supplies of US finished motor\ngasoline product, from 2 February 1991 to 20 January 2017. The units are in\n“million barrels per day”. Consider only the data to the end of 2004.\na. Fit a harmonic regression with trend to the data. Experiment with changing\nthe number Fourier terms. Plot the observed gasoline and fitted values and\ncomment on what you see.\nb. Select the appropriate number of Fourier terms to include by minimising the\nAICc or CV value.\nc. Check the residuals of the final model using the ıcheckresiduals()ı function.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 378,
      "total_chunks": 873
    }
  },
  {
    "text": "b. Select the appropriate number of Fourier terms to include by minimising the\nAICc or CV value.\nc. Check the residuals of the final model using the ıcheckresiduals()ı function.\nEven though the residuals fail the correlation tests, the results are probably\nnot severe enough to make much difference to the forecasts and prediction\nintervals. (Note that the correlations are relatively small, even though they\nare significant.)\nd. To forecast using harmonic regression, you will need to generate the future\nvalues of the Fourier terms. This can be done as follows.\nwhere ıfitı is the fitted model using ıtslm()ı, ıKı is the number of Fourier\nterms used in creating ıfitı, and ıhı is the forecast horizon required.\nForecast the next year of data.\ne. Plot the forecasts along with the actual data for 2005. What do you find?\n7. Data set ıhuronı gives the water level of Lake Huron in feet from 1875 to 1972.\na. Plot the data and comment on its features.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 379,
      "total_chunks": 873
    }
  },
  {
    "text": "7. Data set ıhuronı gives the water level of Lake Huron in feet from 1875 to 1972.\na. Plot the data and comment on its features.\nb. Fit a linear regression and compare this to a piecewise linear trend model\nwith a knot at 1915.\nc. Generate forecasts from these two models for the period up to 1980 and\ncomment on these.\n8. (For advanced readers following on from Section 5.7).\nfc <- forecast(fit, newdata=data.frame( fourier(x,K,h)))\n203",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 380,
      "total_chunks": 873
    }
  },
  {
    "text": "Using matrix notation it was shown that if , where  has mean \nand variance matrix , the estimated coefficients are given by\n and a forecast is given by \nwhere  is a row vector containing the values of the regressors for the forecast\n(in the same format as ), and the forecast variance is given by\nConsider the simple time trend model where . Using the following\nresults,\nderive the following expressions:\na. \nb. \nc. \nd. \n/g35 /g1E /g1A/g44 /gC /g47/g21 /g11\n/g55 /g13 /gB\n/g3F/g44 /g1E/g9 /g1A /g9B /g1A /gA /gC3/g12 /g1A /g9B /g35 /g3F/g35 /g1E /g34 /gC7 /g3F/g44 /g1E /g34 /gC7 /g9/g1A /g9B /g1A /gA /gC3/g12 /g1A /g9B /g35\n/g34 /gC7\n/g1A\n/g32/g1D/g2E/g9/g3F /g35 /gA/g1E/g55 /g13 /g5/g12/gC/g34 /gC7 /g9/g1A /g9B /g1A /gA /gC3/g12 /g9/g34 /gC7 /gA /g9B /g7 /gF\n/g35 /g30 /g1E /g44 /g11 /gC /g44 /g12 /g30\n/g16\n/g11\n/g30/g1E/g12\n/g30 /g1E /g16 /g9/g16 /gC /g12/gA/gD\n/g16\n/g11\n/g30/g1E/g12\n/g30 /g13 /g1E /g16 /g9/g16 /gC /g12/gA/g9/g13/g16 /gC/g12 /gA/g12\n/g13\n/g12\n/g17",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 381,
      "total_chunks": 873
    }
  },
  {
    "text": "/g16\n/g11\n/g30/g1E/g12\n/g30 /g1E /g16 /g9/g16 /gC /g12/gA/gD\n/g16\n/g11\n/g30/g1E/g12\n/g30 /g13 /g1E /g16 /g9/g16 /gC /g12/gA/g9/g13/g16 /gC/g12 /gA/g12\n/g13\n/g12\n/g17\n/g1A /g9B /g1A /g1E /g5 /g17/g16 /g14/g16 /g9/g16 /gC/g12 /gA\n/g14/g16 /g9/g16 /gC/g12 /gA /g16 /g9/g16 /gC /g12/gA/g9/g13/g16 /gC/g12 /gA /g7/g12\n/g17\n/g9/g1A /g9B /g1A /gA /gC3/g12 /g1E /g5 /g9/g16 /gC /g12/gA/g9/g13/g16 /gC /g12/gA /gC3/g14/g9/g16 /gC/g12 /gA\n/gC3/g14/g9/g16 /gC/g12 /gA /g17 /g7/g13\n/g16 /g9/g16 /g13 /gC3/g12 /gA\n/g3F/g44 /g11 /g1E /g5 /g9/g13/g16 /gC/g12 /gA\n/g16\n/g11\n/g30/g1E/g12\n/g35 /g30 /gC3/g14\n/g16\n/g11\n/g30/g1E/g12\n/g30/g35 /g30 /g7/g13\n/g16 /g9/g16 /gC3/g12 /gA\n/g3F/g44 /g12 /g1E /g5 /g13\n/g16\n/g11\n/g30/g1E/g12\n/g30/g35 /g30 /gC3/g9 /g16 /gC/g12 /gA\n/g16\n/g11\n/g30/g1E/g12\n/g35 /g30 /g7/g17\n/g16 /g9/g16 /g13 /gC3/g12 /gA\n/g37/g42/g53/g9 /g3F/g35 /g30 /gA/g1E /g3F/g55 /g13 /g5 /g12/gC /g2/g12/gC3/g15 /g16 /gC3/g17 /g24 /gC/g17 /g3/g7/g13\n/g16 /g9/g16 /gC3/g12 /gA\n/g9/g16 /gC /g24/gA /g13",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 382,
      "total_chunks": 873
    }
  },
  {
    "text": "/g16 /g9/g16 /g13 /gC3/g12 /gA\n/g37/g42/g53/g9 /g3F/g35 /g30 /gA/g1E /g3F/g55 /g13 /g5 /g12/gC /g2/g12/gC3/g15 /g16 /gC3/g17 /g24 /gC/g17 /g3/g7/g13\n/g16 /g9/g16 /gC3/g12 /gA\n/g9/g16 /gC /g24/gA /g13\n/g16 /gC/g12\n204",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 383,
      "total_chunks": 873
    }
  },
  {
    "text": "5.115.11  Further readingFurther reading\nThere are countless books on regression analysis, but few with a focus on regression\nfor time series and forecasting.\nA good general and modern book on regression is Sheather ( 2009).\nAnother general regression text full of excellent practical advice is Harrell\n(2015).\nOrd et al. (2017) provides a practical coverage of regression models for time\nseries in Chapters 7–9, with a strong emphasis on forecasting.\nBibliographyBibliography\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear\nmodels, logistic and ordinal regression, and survival analysis  (2nd ed). New\nYork, USA: Springer. [Amazon]\nOrd, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\nSheather, S. J. (2009). A modern approach to regression with R . New York, USA:\nSpringer. [Amazon]\n205",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 384,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 6Chapter 6  Time series decompositionTime series decomposition\nTime series data can exhibit a variety of patterns, and it is often helpful to split a\ntime series into several components, each representing an underlying pattern\ncategory.\nIn Section 2.3 we discussed three types of time series patterns: trend, seasonality and\ncycles. When we decompose a time series into components, we usually combine the\ntrend and cycle into a single trend-cycletrend-cycle  component (sometimes called the trendtrend for\nsimplicity). Thus we think of a time series as comprising three components: a trend-\ncycle component, a seasonal component, and a remainder component (containing\nanything else in the time series).\nIn this chapter, we consider some common methods for extracting these\ncomponents from a time series. Often this is done to help improve understanding of\nthe time series, but it can also be used to improve forecast accuracy.\n206",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 385,
      "total_chunks": 873
    }
  },
  {
    "text": "6.16.1  Time series componentsTime series components\nIf we assume an additive decomposition, then we can write\nwhere  is the data,  is the seasonal component,  is the trend-cycle component,\nand  is the remainder component, all at period . Alternatively, a multiplicative\ndecomposition would be written as\nThe additive decomposition is the most appropriate if the magnitude of the seasonal\nfluctuations, or the variation around the trend-cycle, does not vary with the level of\nthe time series. When the variation in the seasonal pattern, or the variation around\nthe trend-cycle, appears to be proportional to the level of the time series, then a\nmultiplicative decomposition is more appropriate. Multiplicative decompositions are\ncommon with economic time series.\nAn alternative to using a multiplicative decomposition is to first transform the data\nuntil the variation in the series appears to be stable over time, then use an additive",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 386,
      "total_chunks": 873
    }
  },
  {
    "text": "An alternative to using a multiplicative decomposition is to first transform the data\nuntil the variation in the series appears to be stable over time, then use an additive\ndecomposition. When a log transformation has been used, this is equivalent to using\na multiplicative decomposition because\nElectrical equipment manufacturingElectrical equipment manufacturing\nWe will look at several methods for obtaining the components ,  and  later in\nthis chapter, but first, it is helpful to see an example. We will decompose the new\norders index for electrical equipment shown in Figure 6.1. The data show the number\nof new orders for electrical equipment (computer, electronic and optical products) in\nthe Euro area (16 countries). The data have been adjusted by working days and\nnormalised so that a value of 100 corresponds to 2005.\n207",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 387,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.1: Electrical equipment orders: the trend-cycle component (red) and the raw data\n(grey).\nFigure 6.1  shows the trend-cycle component, , in red and the original data, , in\ngrey. The trend-cycle shows the overall movement in the series, ignoring the\nseasonality and any small random fluctuations.\nFigure 6.2  shows an additive decomposition of these data. The method used for\nestimating components in this example is STL, which is discussed in Section 6.6.\n/g16 /g30 /g35 /g30\n208",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 388,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.2: The electrical equipment orders (top) and its three additive components.\nThe three components are shown separately in the bottom three panels of Figure 6.2.\nThese components can be added together to reconstruct the data shown in the top\npanel. Notice that the seasonal component changes slowly over time, so that any two\nconsecutive years have similar patterns, but years far apart may have different\nseasonal patterns. The remainder component shown in the bottom panel is what is\nleft over when the seasonal and trend-cycle components have been subtracted from\nthe data.\nThe grey bars to the right of each panel show the relative scales of the components.\nEach grey bar represents the same length but because the plots are on different\nscales, the bars vary in size. The large grey bar in the bottom panel shows that the\nvariation in the remainder component is small compared to the variation in the data,\n209",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 389,
      "total_chunks": 873
    }
  },
  {
    "text": "which has a bar about one quarter the size. If we shrunk the bottom three panels\nuntil their bars became the same size as that in the data panel, then all the panels\nwould be on the same scale.\nSeasonally adjusted dataSeasonally adjusted data\nIf the seasonal component is removed from the original data, the resulting values are\nthe “seasonally adjusted” data. For an additive decomposition, the seasonally\nadjusted data are given by , and for multiplicative data, the seasonally\nadjusted values are obtained using .\nFigure 6.3  shows the seasonally adjusted electrical equipment orders.\nFigure 6.3: Seasonally adjusted electrical equipment orders (blue) and the original data\n(grey).\nIf the variation due to seasonality is not of primary interest, the seasonally adjusted\nseries can be useful. For example, monthly unemployment data are usually\nseasonally adjusted in order to highlight variation due to the underlying state of the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 390,
      "total_chunks": 873
    }
  },
  {
    "text": "series can be useful. For example, monthly unemployment data are usually\nseasonally adjusted in order to highlight variation due to the underlying state of the\neconomy rather than the seasonal variation. An increase in unemployment due to\nschool leavers seeking work is seasonal variation, while an increase in\nunemployment due to an economic recession is non-seasonal. Most economic\nanalysts who study unemployment data are more interested in the non-seasonal\nvariation. Consequently, employment data (and many other economic series) are\nusually seasonally adjusted.\n210",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 391,
      "total_chunks": 873
    }
  },
  {
    "text": "Seasonally adjusted series contain the remainder component as well as the trend-\ncycle. Therefore, they are not “smooth”, and “downturns” or “upturns” can be\nmisleading. If the purpose is to look for turning points in a series, and interpret any\nchanges in direction, then it is better to use the trend-cycle component rather than\nthe seasonally adjusted data.\n211",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 392,
      "total_chunks": 873
    }
  },
  {
    "text": "6.26.2  Moving averagesMoving averages\nThe classical method of time series decomposition originated in the 1920s and was\nwidely used until the 1950s. It still forms the basis of many time series\ndecomposition methods, so it is important to understand how it works. The first step\nin a classical decomposition is to use a moving average method to estimate the\ntrend-cycle, so we begin by discussing moving averages.\nMoving average smoothingMoving average smoothing\nA moving average of order  can be written as\nwhere . That is, the estimate of the trend-cycle at time  is obtained by\naveraging values of the time series within  periods of . Observations that are\nnearby in time are also likely to be close in value. Therefore, the average eliminates\nsome of the randomness in the data, leaving a smooth trend-cycle component. We\ncall this an -MA-MA, meaning a moving average of order .\nautoplot(elecsales) +  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Annual electricity sales: South Australia\" )\n212",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 393,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.4: Residential electricity sales (excluding hot water) for South Australia: 1989–\n2008.\nFor example, consider Figure 6.4 which shows the volume of electricity sold to\nresidential customers in South Australia each year from 1989 to 2008 (hot water\nsales have been excluded). The data are also shown in Table 6.1.\n213",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 394,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 6.1: Annual electricity sales to residential customers in South Australia. 1989–\n2008.\nYearYear Sales (GWh)Sales (GWh) 5-MA5-MA\n1989 2354.34\n1990 2379.71\n1991 2318.52 2381.53\n1992 2468.99 2424.56\n1993 2386.09 2463.76\n1994 2569.47 2552.60\n1995 2575.72 2627.70\n1996 2762.72 2750.62\n1997 2844.50 2858.35\n1998 3000.70 3014.70\n1999 3108.10 3077.30\n2000 3357.50 3144.52\n2001 3075.70 3188.70\n2002 3180.60 3202.32\n2003 3221.60 3216.94\n2004 3176.20 3307.30\n2005 3430.60 3398.75\n2006 3527.48 3485.43\n2007 3637.89\n2008 3655.00\nIn the last column of this table, a moving average of order 5 is shown, providing an\nestimate of the trend-cycle. The first value in this column is the average of the first\nfive observations (1989–1993); the second value in the 5-MA column is the average\nof the values for 1990–1994; and so on. Each value in the 5-MA column is the\naverage of the observations in the five year window centred on the corresponding",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 395,
      "total_chunks": 873
    }
  },
  {
    "text": "of the values for 1990–1994; and so on. Each value in the 5-MA column is the\naverage of the observations in the five year window centred on the corresponding\nyear. In the notation of Equation (6.1), column 5-MA contains the values of  with\n and . This is easily computed using\nThere are no values for either the first two years or the last two years, because we do\nnot have two observations on either side. Later we will use more sophisticated\nmethods of trend-cycle estimation which do allow estimates near the endpoints.\nTo see what the trend-cycle estimate looks like, we plot it along with the original\ndata in Figure 6.5.\nma(elecsales, 5)\n214",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 396,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.5: Residential electricity sales (black) along with the 5-MA estimate of the trend-\ncycle (red).\nNotice that the trend-cycle (in red) is smoother than the original data and captures\nthe main movement of the time series without all of the minor fluctuations. The\norder of the moving average determines the smoothness of the trend-cycle estimate.\nIn general, a larger order means a smoother curve. Figure 6.6 shows the effect of\nchanging the order of the moving average for the residential electricity sales data.\nautoplot(elecsales, series=\"Data\") +\n  autolayer(ma(elecsales,5), series=\"5-MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Annual electricity sales: South Australia\" ) +\n  scale_colour_manual (values=c(\"Data\"=\"grey50\",\"5-MA\"=\"red\"),\n                      breaks=c (\"Data\",\"5-MA\"))\n215",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 397,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.6: Different moving averages applied to the residential electricity sales data.\nSimple moving averages such as these are usually of an odd order (e.g., 3, 5, 7, etc.).\nThis is so they are symmetric: in a moving average of order , the middle\nobservation, and  observations on either side, are averaged. But if  was even, it\nwould no longer be symmetric.\nMoving averages of moving averagesMoving averages of moving averages\nIt is possible to apply a moving average to a moving average. One reason for doing\nthis is to make an even-order moving average symmetric.\nFor example, we might take a moving average of order 4, and then apply another\nmoving average of order 2 to the results. In the following table, this has been done\nfor the first few years of the Australian quarterly beer production data.\nbeer2 <- window(ausbeer,start=1992 )\nma4 <- ma(beer2, order=4, centre=FALSE)\nma2x4 <- ma(beer2, order=4, centre=TRUE)\n216",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 398,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 6.2: A moving average of order 4 applied to the quarterly beer data, followed by\na moving average of order 2.\nYearYear QuarterQuarter ObservationObservation 4-MA4-MA 2x4-MA2x4-MA\n1992 Q1 443\n1992 Q2 410 451.25\n1992 Q3 420 448.75 450.00\n1992 Q4 532 451.50 450.12\n1993 Q1 433 449.00 450.25\n1993 Q2 421 444.00 446.50\n1993 Q3 410 448.00 446.00\n1993 Q4 512 438.00 443.00\n1994 Q1 449 441.25 439.62\n1994 Q2 381 446.00 443.62\n1994 Q3 423 440.25 443.12\n1994 Q4 531 447.00 443.62\n1995 Q1 426 445.25 446.12\n1995 Q2 408 442.50 443.88\n1995 Q3 416 438.25 440.38\n1995 Q4 520 435.75 437.00\n1996 Q1 409 431.25 433.50\n1996 Q2 398 428.00 429.62\n1996 Q3 398 433.75 430.88\n1996 Q4 507 433.75 433.75\nThe notation “ -MA” in the last column means a 4-MA followed by a 2-MA. The\nvalues in the last column are obtained by taking a moving average of order 2 of the\nvalues in the previous column. For example, the first two values in the 4-MA column",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 399,
      "total_chunks": 873
    }
  },
  {
    "text": "values in the last column are obtained by taking a moving average of order 2 of the\nvalues in the previous column. For example, the first two values in the 4-MA column\nare 451.25=(443+410+420+532)/4 and 448.75=(410+420+532+433)/4. The first value\nin the 2x4-MA column is the average of these two: 450.00=(451.25+448.75)/2.\nWhen a 2-MA follows a moving average of an even order (such as 4), it is called a\n“centred moving average of order 4”. This is because the results are now symmetric.\nTo see that this is the case, we can write the -MA as follows:\n217",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 400,
      "total_chunks": 873
    }
  },
  {
    "text": "It is now a weighted average of observations that is symmetric. By default, the ıma()ı\nfunction in R will return a centred moving average for even orders (unless\nıcenter=FALSEı is specified).\nOther combinations of moving averages are also possible. For example, a -MA\nis often used, and consists of a moving average of order 3 followed by another\nmoving average of order 3. In general, an even order MA should be followed by an\neven order MA to make it symmetric. Similarly, an odd order MA should be followed\nby an odd order MA.\nEstimating the trend-cycle with seasonal dataEstimating the trend-cycle with seasonal data\nThe most common use of centred moving averages is for estimating the trend-cycle\nfrom seasonal data. Consider the -MA:\nWhen applied to quarterly data, each quarter of the year is given equal weight as the\nfirst and last terms apply to the same quarter in consecutive years. Consequently, the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 401,
      "total_chunks": 873
    }
  },
  {
    "text": "When applied to quarterly data, each quarter of the year is given equal weight as the\nfirst and last terms apply to the same quarter in consecutive years. Consequently, the\nseasonal variation will be averaged out and the resulting values of  will have little\nor no seasonal variation remaining. A similar effect would be obtained using a -\nMA or a -MA to quarterly data.\nIn general, a -MA is equivalent to a weighted moving average of order \nwhere all observations take the weight , except for the first and last terms which\ntake weights . So, if the seasonal period is even and of order , we use a\n-MA to estimate the trend-cycle. If the seasonal period is odd and of order ,\nwe use a -MA to estimate the trend-cycle. For example, a -MA can be used\nto estimate the trend-cycle of monthly data and a 7-MA can be used to estimate the\ntrend-cycle of daily data with a weekly seasonality.\nOther choices for the order of the MA will usually result in trend-cycle estimates",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 402,
      "total_chunks": 873
    }
  },
  {
    "text": "trend-cycle of daily data with a weekly seasonality.\nOther choices for the order of the MA will usually result in trend-cycle estimates\nbeing contaminated by the seasonality in the data.\n218",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 403,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Electrical equipment manufacturingExample: Electrical equipment manufacturing\nFigure 6.7: A 2x12-MA applied to the electrical equipment orders index.\nFigure 6.7 shows a -MA applied to the electrical equipment orders index.\nNotice that the smooth line shows no seasonality; it is almost the same as the trend-\ncycle shown in Figure 6.1, which was estimated using a much more sophisticated\nmethod than a moving average. Any other choice for the order of the moving average\n(except for 24, 36, etc.) would have resulted in a smooth line that showed some\nseasonal fluctuations.\nautoplot(elecequip, series=\"Data\") +\n  autolayer(ma(elecequip, 12), series=\"12-MA\") +\n  xlab(\"Year\") + ylab(\"New orders index\") +\n  ggtitle(\"Electrical equipment manufacturing (Euro area)\" ) +\n  scale_colour_manual (values=c(\"Data\"=\"grey\",\"12-MA\"=\"red\"),\n                      breaks=c(\"Data\",\"12-MA\"))\n219",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 404,
      "total_chunks": 873
    }
  },
  {
    "text": "Weighted moving averagesWeighted moving averages\nCombinations of moving averages result in weighted moving averages. For example,\nthe -MA discussed above is equivalent to a weighted 5-MA with weights given\nby . In general, a weighted -MA can be written as\nwhere , and the weights are given by . It is important\nthat the weights all sum to one and that they are symmetric so that . The\nsimple -MA is a special case where all of the weights are equal to .\nA major advantage of weighted moving averages is that they yield a smoother\nestimate of the trend-cycle. Instead of observations entering and leaving the\ncalculation at full weight, their weights slowly increase and then slowly decrease,\nresulting in a smoother curve.\n220",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 405,
      "total_chunks": 873
    }
  },
  {
    "text": "6.36.3  Classical decompositionClassical decomposition\nThe classical decomposition method originated in the 1920s. It is a relatively simple\nprocedure, and forms the starting point for most other methods of time series\ndecomposition. There are two forms of classical decomposition: an additive\ndecomposition and a multiplicative decomposition. These are described below for a\ntime series with seasonal period  (e.g.,  for quarterly data,  for\nmonthly data,  for daily data with a weekly pattern).\nIn classical decomposition, we assume that the seasonal component is constant from\nyear to year. For multiplicative seasonality, the  values that form the seasonal\ncomponent are sometimes called the “seasonal indices”.\nAdditive decompositionAdditive decomposition\nStep 1Step 1\nIf  is an even number, compute the trend-cycle component  using a -\nMA. If  is an odd number, compute the trend-cycle component  using an -\nMA.\nStep 2Step 2\nCalculate the detrended series: .\nStep 3Step 3",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 406,
      "total_chunks": 873
    }
  },
  {
    "text": "MA. If  is an odd number, compute the trend-cycle component  using an -\nMA.\nStep 2Step 2\nCalculate the detrended series: .\nStep 3Step 3\nTo estimate the seasonal component for each season, simply average the\ndetrended values for that season. For example, with monthly data, the seasonal\ncomponent for March is the average of all the detrended March values in the data.\nThese seasonal component values are then adjusted to ensure that they add to\nzero. The seasonal component is obtained by stringing together these monthly\nvalues, and then replicating the sequence for each year of data. This gives .\nStep 4Step 4\nThe remainder component is calculated by subtracting the estimated seasonal\nand trend-cycle components: .\n221",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 407,
      "total_chunks": 873
    }
  },
  {
    "text": "Multiplicative decompositionMultiplicative decomposition\nA classical multiplicative decomposition is similar, except that the subtractions are\nreplaced by divisions.\nStep 1Step 1\nIf  is an even number, compute the trend-cycle component  using a -\nMA. If  is an odd number, compute the trend-cycle component  using an -\nMA.\nStep 2Step 2\nCalculate the detrended series: .\nStep 3Step 3\nTo estimate the seasonal component for each season, simply average the\ndetrended values for that season. For example, with monthly data, the seasonal\nindex for March is the average of all the detrended March values in the data.\nThese seasonal indexes are then adjusted to ensure that they add to . The\nseasonal component is obtained by stringing together these monthly indexes,\nand then replicating the sequence for each year of data. This gives .\nStep 4Step 4\nThe remainder component is calculated by dividing out the estimated seasonal\nand trend-cycle components: .",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 408,
      "total_chunks": 873
    }
  },
  {
    "text": "and then replicating the sequence for each year of data. This gives .\nStep 4Step 4\nThe remainder component is calculated by dividing out the estimated seasonal\nand trend-cycle components: .\nFigure 6.8 shows a classical decomposition of the electrical equipment index.\nCompare this decomposition with that shown in Figure 6.1 . The run of remainder\nvalues below 1 in 2009 suggests that there is some “leakage” of the trend-cycle\ncomponent into the remainder component. The trend-cycle estimate has over-\nsmoothed the drop in the data, and the corresponding remainder values have been\naffected by the poor trend-cycle estimate.\nelecequip %>% decompose(type=\"multiplicative\") %>%\n  autoplot() + xlab(\"Year\") +\n  ggtitle(\"Classical multiplicative decomposition\n    of electrical equipment index\")\n222",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 409,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.8: A classical multiplicative decomposition of the new orders index for electrical\nequipment.\nComments on classical decompositionComments on classical decomposition\nWhile classical decomposition is still widely used, it is not recommended, as there\nare now several much better methods. Some of the problems with classical\ndecomposition are summarised below.\nThe estimate of the trend-cycle is unavailable for the first few and last few\nobservations. For example, if , there is no trend-cycle estimate for the\nfirst six or the last six observations. Consequently, there is also no estimate of\nthe remainder component for the same time periods.\nThe trend-cycle estimate tends to over-smooth rapid rises and falls in the data\n(as seen in the above example).\n223",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 410,
      "total_chunks": 873
    }
  },
  {
    "text": "Classical decomposition methods assume that the seasonal component repeats\nfrom year to year. For many series, this is a reasonable assumption, but for some\nlonger series it is not. For example, electricity demand patterns have changed\nover time as air conditioning has become more widespread. Specifically, in many\nlocations, the seasonal usage pattern from several decades ago had its maximum\ndemand in winter (due to heating), while the current seasonal pattern has its\nmaximum demand in summer (due to air conditioning). The classical\ndecomposition methods are unable to capture these seasonal changes over time.\nOccasionally, the values of the time series in a small number of periods may be\nparticularly unusual. For example, the monthly air passenger traffic may be\naffected by an industrial dispute, making the traffic during the dispute different\nfrom usual. The classical method is not robust to these kinds of unusual values.\n224",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 411,
      "total_chunks": 873
    }
  },
  {
    "text": "6.46.4  X11 decompositionX11 decomposition\nAnother popular method for decomposing quarterly and monthly data is the X11\nmethod which originated in the US Census Bureau and Statistics Canada.\nThis method is based on classical decomposition, but includes many extra steps and\nfeatures in order to overcome the drawbacks of classical decomposition that were\ndiscussed in the previous section. In particular, trend-cycle estimates are available\nfor all observations including the end points, and the seasonal component is allowed\nto vary slowly over time. X11 also has some sophisticated methods for handling\ntrading day variation, holiday effects and the effects of known predictors. It handles\nboth additive and multiplicative decomposition. The process is entirely automatic\nand tends to be highly robust to outliers and level shifts in the time series.\nThe details of the X11 method are described in Dagum & Bianconcini ( 2016). Here we",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 412,
      "total_chunks": 873
    }
  },
  {
    "text": "and tends to be highly robust to outliers and level shifts in the time series.\nThe details of the X11 method are described in Dagum & Bianconcini ( 2016). Here we\nwill only demonstrate how to use the automatic procedure in R.\nThe X11 method is available using the ıseas()ı function from the seasonalseasonal package\nfor R.\nlibrary(seasonal)\nelecequip %>% seas(x11=\"\") -> fit\nautoplot(fit) +\n  ggtitle(\"X11 decomposition of electrical equipment index\" )\n225",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 413,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.9: An X11 decomposition of the new orders index for electrical equipment.\nCompare this decomposition with the STL decomposition shown in Figure 6.1 and\nthe classical decomposition shown in Figure 6.8. The X11 trend-cycle has captured\nthe sudden fall in the data in early 2009 better than either of the other two methods,\nand the unusual observation at the end of 2009 is now more clearly seen in the\nremainder component.\nGiven the output from the ıseas()ı function, ıseasonal()ı will extract the seasonal\ncomponent, ıtrendcycle()ı will extract the trend-cycle component, ıremainder()ı\nwill extract the remainder component, and ıseasadj()ı will compute the seasonally\nadjusted time series.\nFor example, Figure 6.10 shows the trend-cycle component and the seasonally\nadjusted data, along with the original data.\n226",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 414,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.10: Electrical equipment orders: the original data (grey), the trend-cycle\ncomponent (red) and the seasonally adjusted data (blue).\nIt can be useful to use seasonal plots and seasonal sub-series plots of the seasonal\ncomponent. These help us to visualise the variation in the seasonal component over\ntime. Figure 6.11 shows a seasonal sub-series plot of the seasonal component from\nFigure 6.9. In this case, there are only small changes over time.\nautoplot(elecequip, series=\"Data\") +\n  autolayer(trendcycle(fit), series=\"Trend\") +\n  autolayer(seasadj(fit), series=\"Seasonally Adjusted\") +\n  xlab(\"Year\") + ylab(\"New orders index\") +\n  ggtitle(\"Electrical equipment manufacturing (Euro area)\" ) +\n  scale_colour_manual (values=c(\"gray\",\"blue\",\"red\"),\n             breaks=c (\"Data\",\"Seasonally Adjusted\", \"Trend\"))\nfit %>% seasonal() %>% ggsubseriesplot() + ylab(\"Seasonal\")\n227",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 415,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.11: Seasonal sub-series plot of the seasonal component from the X11\ndecomposition of the new orders index for electrical equipment.\nBibliographyBibliography\nDagum, E. B., & Bianconcini, S. (2016). Seasonal adjustment methods and real\ntime trend-cycle estimation . Springer. [Amazon]\n228",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 416,
      "total_chunks": 873
    }
  },
  {
    "text": "6.56.5  SEATS decompositionSEATS decomposition\n“SEATS” stands for “Seasonal Extraction in ARIMA Time Series” (ARIMA models are\ndiscussed in Chapter 8). This procedure was developed at the Bank of Spain, and is\nnow widely used by government agencies around the world. The procedure works\nonly with quarterly and monthly data. So seasonality of other kinds, such as daily\ndata, or hourly data, or weekly data, require an alternative approach.\nThe details are beyond the scope of this book. However, a complete discussion of the\nmethod is available in Dagum & Bianconcini ( 2016). Here we will only demonstrate\nhow to use it via the seasonalseasonal package.\nlibrary(seasonal)\nelecequip %>% seas() %>%\nautoplot() +\n  ggtitle(\"SEATS decomposition of electrical equipment index\" )\n229",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 417,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.12: A SEATS decomposition of the new orders index for electrical equipment.\nThe result is quite similar to the X11 decomposition shown in Figure 6.9.\nAs with the X11 method, we can use the ıseasonal()ı, ıtrendcycle()ı and\nıremainder()ı functions to extract the individual components, and ıseasadj()ı to\ncompute the seasonally adjusted time series.\nThe seasonalseasonal package has many options for handling variations of X11 and SEATS.\nSee the package website  for a detailed introduction to the options and features\navailable.\nBibliographyBibliography\nDagum, E. B., & Bianconcini, S. (2016). Seasonal adjustment methods and real\ntime trend-cycle estimation . Springer. [Amazon]\n230\n\n\n231",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 418,
      "total_chunks": 873
    }
  },
  {
    "text": "6.66.6  STL decompositionSTL decomposition\nSTL is a versatile and robust method for decomposing time series. STL is an acronym\nfor “Seasonal and Trend decomposition using Loess”, while Loess is a method for\nestimating nonlinear relationships. The STL method was developed by R. B.\nCleveland, Cleveland, McRae, & Terpenning ( 1990).\nSTL has several advantages over the classical, SEATS and X11 decomposition\nmethods:\nUnlike SEATS and X11, STL will handle any type of seasonality, not only monthly\nand quarterly data.\nThe seasonal component is allowed to change over time, and the rate of change\ncan be controlled by the user.\nThe smoothness of the trend-cycle can also be controlled by the user.\nIt can be robust to outliers (i.e., the user can specify a robust decomposition), so\nthat occasional unusual observations will not affect the estimates of the trend-\ncycle and seasonal components. They will, however, affect the remainder\ncomponent.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 419,
      "total_chunks": 873
    }
  },
  {
    "text": "that occasional unusual observations will not affect the estimates of the trend-\ncycle and seasonal components. They will, however, affect the remainder\ncomponent.\nOn the other hand, STL has some disadvantages. In particular, it does not handle\ntrading day or calendar variation automatically, and it only provides facilities for\nadditive decompositions.\nIt is possible to obtain a multiplicative decomposition by first taking logs of the data,\nthen back-transforming the components. Decompositions between additive and\nmultiplicative can be obtained using a Box-Cox transformation of the data with\n. A value of  corresponds to the multiplicative decomposition while\n is equivalent to an additive decomposition.\nThe best way to begin learning how to use STL is to see some examples and\nexperiment with the settings. Figure 6.2 showed an example of STL applied to the\nelectrical equipment orders data. Figure 6.13 shows an alternative STL",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 420,
      "total_chunks": 873
    }
  },
  {
    "text": "experiment with the settings. Figure 6.2 showed an example of STL applied to the\nelectrical equipment orders data. Figure 6.13 shows an alternative STL\ndecomposition where the trend-cycle is more flexible, the seasonal component does\nnot change over time, and the robust option has been used. Here, it is more obvious\n232",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 421,
      "total_chunks": 873
    }
  },
  {
    "text": "that there has been a down-turn at the end of the series, and that the orders in 2009\nwere unusually low (corresponding to some large negative values in the remainder\ncomponent).\nFigure 6.13: The electrical equipment orders (top) and its three additive components\nobtained from a robust STL decomposition with flexible trend-cycle and fixed\nseasonality.\nThe two main parameters to be chosen when using STL are the trend-cycle window\n( ıt.windowı) and the seasonal window ( ıs.windowı). These control how rapidly the\ntrend-cycle and seasonal components can change. Smaller values allow for more\nrapid changes. Both ıt.windowı and ıs.windowı should be odd numbers; ıt.windowı is\nelecequip %>%\n  stl(t.window=13, s.window=\"periodic\", robust=TRUE ) %>%\n  autoplot()\n233",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 422,
      "total_chunks": 873
    }
  },
  {
    "text": "the number of consecutive observations to be used when estimating the trend-cycle;\nıs.windowı is the number of consecutive years to be used in estimating each value in\nthe seasonal component. The user must specify ıs.windowı as there is no default.\nSetting it to be infinite is equivalent to forcing the seasonal component to be periodic\n(i.e., identical across years). Specifying ıt.windowı is optional, and a default value will\nbe used if it is omitted.\nThe ımstl()ı function provides a convenient automated STL decomposition using\nıs.window=13ı, and ıt.windowı also chosen automatically. This usually gives a good\nbalance between overfitting the seasonality and allowing it to slowly change over\ntime. But, as with any automated procedure, the default settings will need adjusting\nfor some time series.\nAs with the other decomposition methods discussed in this book, to obtain the\nseparate components plotted in Figure 6.8, use the ıseasonal()ı function for the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 423,
      "total_chunks": 873
    }
  },
  {
    "text": "for some time series.\nAs with the other decomposition methods discussed in this book, to obtain the\nseparate components plotted in Figure 6.8, use the ıseasonal()ı function for the\nseasonal component, the ıtrendcycle()ı function for trend-cycle component, and\nthe ıremainder()ı function for the remainder component. The ıseasadj()ı function\ncan be used to compute the seasonally adjusted series.\nBibliographyBibliography\nCleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. J. (1990). STL: A\nseasonal-trend decomposition procedure based on loess. Journal of Official\nStatistics , 6(1), 3–33. http://bit.ly/stl1990\n234",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 424,
      "total_chunks": 873
    }
  },
  {
    "text": "6.76.7  Measuring strength of trend and seasonalityMeasuring strength of trend and seasonality\nA time series decomposition can be used to measure the strength of trend and\nseasonality in a time series ( Wang, Smith, & Hyndman, 2006). Recall that the\ndecomposition is written as\nwhere  is the smoothed trend component,  is the seasonal component and  is a\nremainder component. For strongly trended data, the seasonally adjusted data\nshould have much more variation than the remainder component. Therefore Var\n/Var  should be relatively small. But for data with little or no trend, the two\nvariances should be approximately the same. So we define the strength of trend as:\nThis will give a measure of the strength of the trend between 0 and 1. Because the\nvariance of the remainder might occasionally be even larger than the variance of the\nseasonally adjusted data, we set the minimal possible value of  equal to zero.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 425,
      "total_chunks": 873
    }
  },
  {
    "text": "variance of the remainder might occasionally be even larger than the variance of the\nseasonally adjusted data, we set the minimal possible value of  equal to zero.\nThe strength of seasonality is defined similarly, but with respect to the detrended\ndata rather than the seasonally adjusted data:\nA series with seasonal strength  close to 0 exhibits almost no seasonality, while a\nseries with strong seasonality will have  close to 1 because Var  will be much\nsmaller than Var .\nThese measures can be useful, for example, when you have a large collection of time\nseries, and you need to find the series with the most trend or the most seasonality.\nBibliographyBibliography\nWang, X., Smith, K. A., & Hyndman, R. J. (2006). Characteristic-based clustering\nfor time series data. Data Mining and Knowledge Discovery , 13(3), 335–364.\n235",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 426,
      "total_chunks": 873
    }
  },
  {
    "text": "[DOI]\n236",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 427,
      "total_chunks": 873
    }
  },
  {
    "text": "6.86.8  Forecasting with decompositionForecasting with decomposition\nWhile decomposition is primarily useful for studying time series data, and exploring\nhistorical changes over time, it can also be used in forecasting.\nAssuming an additive decomposition, the decomposed time series can be written as\nwhere  is the seasonally adjusted component. Or, if a multiplicative\ndecomposition has been used, we can write\nwhere .\nTo forecast a decomposed time series, we forecast the seasonal component, , and\nthe seasonally adjusted component , separately. It is usually assumed that the\nseasonal component is unchanging, or changing extremely slowly, so it is forecast\nby simply taking the last year of the estimated component. In other words, a\nseasonal naïve method is used for the seasonal component.\nTo forecast the seasonally adjusted component, any non-seasonal forecasting\nmethod may be used. For example, a random walk with drift model, or Holt’s method",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 428,
      "total_chunks": 873
    }
  },
  {
    "text": "To forecast the seasonally adjusted component, any non-seasonal forecasting\nmethod may be used. For example, a random walk with drift model, or Holt’s method\n(discussed in the next chapter), or a non-seasonal ARIMA model (discussed in\nChapter 8), may be used.\nExample: Electrical equipment manufacturingExample: Electrical equipment manufacturing\nfit <- stl(elecequip, t.window=13, s.window=\"periodic\",\n  robust=TRUE)\nfit %>% seasadj() %>% naive() %>%\n  autoplot() + ylab(\"New orders index\") +\n  ggtitle(\"Naive forecasts of seasonally adjusted data\" )\n237",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 429,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.14: Naïve forecasts of the seasonally adjusted data obtained from an STL\ndecomposition of the electrical equipment orders data.\nFigure 6.14 shows naïve forecasts of the seasonally adjusted electrical equipment\norders data. These are then “reseasonalised” by adding in the seasonal naïve\nforecasts of the seasonal component.\nThis is made easy with the ıforecast()ı function applied to the ıstlı object. You need\nto specify the method being used on the seasonally adjusted data, and the function\nwill do the reseasonalising for you. The resulting forecasts of the original data are\nshown in Figure 6.15.\nfit %>% forecast(method=\"naive\") %>%\n  autoplot() + ylab(\"New orders index\")\n238",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 430,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.15: Forecasts of the electrical equipment orders data based on a naïve forecast of\nthe seasonally adjusted data and a seasonal naïve forecast of the seasonal component,\nafter an STL decomposition of the data.\nThe prediction intervals shown in this graph are constructed in the same way as the\npoint forecasts. That is, the upper and lower limits of the prediction intervals on the\nseasonally adjusted data are “reseasonalised” by adding in the forecasts of the\nseasonal component. In this calculation, the uncertainty in the forecasts of the\nseasonal component has been ignored. The rationale for this choice is that the\nuncertainty in the seasonal component is much smaller than that for the seasonally\nadjusted data, and so it is a reasonable approximation to ignore it.\nA short-cut approach is to use the ıstlf()ı function. The following code will\ndecompose the time series using STL, forecast the seasonally adjusted series, and\nreturn the reseasonalised forecasts.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 431,
      "total_chunks": 873
    }
  },
  {
    "text": "A short-cut approach is to use the ıstlf()ı function. The following code will\ndecompose the time series using STL, forecast the seasonally adjusted series, and\nreturn the reseasonalised forecasts.\nThe ıstlf()ı function uses ımstl()ı to carry out the decomposition, so there are\ndefault values for ıs.windowı and ıt.windowı.\nAs well as the naïve method, several other possible forecasting methods are available\nwith ıstlf()ı, as described in the corresponding help file. If ımethodı is not specified,\nit will use the ETS approach (discussed in the next chapter) applied to the seasonally\nfcast <- stlf(elecequip, method='naive')\n239",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 432,
      "total_chunks": 873
    }
  },
  {
    "text": "adjusted series. This usually produces quite good forecasts for seasonal time series,\nand some companies use it routinely for all their operational forecasts.\n240",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 433,
      "total_chunks": 873
    }
  },
  {
    "text": "6.96.9  ExercisesExercises\n1. Show that a  MA is equivalent to a 7-term weighted moving average with\nweights of 0.067, 0.133, 0.200, 0.200, 0.200, 0.133, and 0.067.\n2. The ıplasticsı data set consists of the monthly sales (in thousands) of product A\nfor a plastics manufacturer for five years.\na. Plot the time series of sales of product A. Can you identify seasonal\nfluctuations and/or a trend-cycle?\nb. Use a classical multiplicative decomposition to calculate the trend-cycle and\nseasonal indices.\nc. Do the results support the graphical interpretation from part a?\nd. Compute and plot the seasonally adjusted data.\ne. Change one observation to be an outlier (e.g., add 500 to one observation),\nand recompute the seasonally adjusted data. What is the effect of the outlier?\nf. Does it make any difference if the outlier is near the end rather than in the\nmiddle of the time series?\n3. Recall your retail time series data (from Exercise 3 in Section 2.10). Decompose",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 434,
      "total_chunks": 873
    }
  },
  {
    "text": "f. Does it make any difference if the outlier is near the end rather than in the\nmiddle of the time series?\n3. Recall your retail time series data (from Exercise 3 in Section 2.10). Decompose\nthe series using X11. Does it reveal any outliers, or unusual features that you had\nnot noticed previously?\n4. Figures 6.16 and 6.17  show the result of decomposing the number of persons in\nthe civilian labour force in Australia each month from February 1978 to August\n1995.\n241",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 435,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.16: Decomposition of the number of persons in the civilian labour force in\nAustralia each month from February 1978 to August 1995.\n242",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 436,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 6.17: Seasonal component from the decomposition shown in the previous\nfigure.\na. Write about 3–5 sentences describing the results of the decomposition. Pay\nparticular attention to the scales of the graphs in making your\ninterpretation.\nb. Is the recession of 1991/1992 visible in the estimated components?\n5. This exercise uses the ıcangası data (monthly Canadian gas production in\nbillions of cubic metres, January 1960 – February 2005).\na. Plot the data using ıautoplot()ı, ıggsubseriesplot() ı and ıggseasonplot() ı to\nlook at the effect of the changing seasonality over time. What do you think is\ncausing it to change so much?\nb. Do an STL decomposition of the data. You will need to choose ıs.windowı to\nallow for the changing shape of the seasonal component.\nc. Compare the results with those obtained using SEATS and X11. How are they\ndifferent?\n6. We will use the ıbricksqı data (Australian quarterly clay brick production. 1956–\n1994) for this exercise.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 437,
      "total_chunks": 873
    }
  },
  {
    "text": "c. Compare the results with those obtained using SEATS and X11. How are they\ndifferent?\n6. We will use the ıbricksqı data (Australian quarterly clay brick production. 1956–\n1994) for this exercise.\na. Use an STL decomposition to calculate the trend-cycle and seasonal indices.\n(Experiment with having fixed or changing seasonality.)\nb. Compute and plot the seasonally adjusted data.\nc. Use a naïve method to produce forecasts of the seasonally adjusted data.\n243",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 438,
      "total_chunks": 873
    }
  },
  {
    "text": "d. Use ıstlf()ı to reseasonalise the results, giving forecasts for the original\ndata.\ne. Do the residuals look uncorrelated?\nf. Repeat with a robust STL decomposition. Does it make much difference?\ng. Compare forecasts from ıstlf()ı with those from ısnaive()ı, using a test set\ncomprising the last 2 years of data. Which is better?\n7. Use ıstlf()ı to produce forecasts of the ıwritingı series with either\nımethod=\"naive\" ı or ımethod=\"rwdrift\" ı, whichever is most appropriate. Use the\nılambdaı argument if you think a Box-Cox transformation is required.\n8. Use ıstlf()ı to produce forecasts of the ıfancyı series with either\nımethod=\"naive\" ı or ımethod=\"rwdrift\" ı, whichever is most appropriate. Use the\nılambdaı argument if you think a Box-Cox transformation is required.\n244",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 439,
      "total_chunks": 873
    }
  },
  {
    "text": "6.106.10  Further readingFurther reading\nA detailed modern discussion of SEATS and X11 decomposition methods is\nprovided by Dagum & Bianconcini ( 2016).\nR. B. Cleveland et al. ( 1990) introduced STL, and still provides the best\ndescription of the algorithm.\nFor a discussion of forecasting using STL, see Theodosiou ( 2011).\nBibliographyBibliography\nCleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. J. (1990). STL: A\nseasonal-trend decomposition procedure based on loess. Journal of Official\nStatistics , 6(1), 3–33. http://bit.ly/stl1990\nDagum, E. B., & Bianconcini, S. (2016). Seasonal adjustment methods and real\ntime trend-cycle estimation . Springer. [Amazon]\nTheodosiou, M. (2011). Forecasting monthly and quarterly time series using STL\ndecomposition. International Journal of Forecasting , 27(4), 1178–1195. [DOI]\n245",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 440,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 7Chapter 7  Exponential smoothingExponential smoothing\nExponential smoothing was proposed in the late 1950s ( Brown, 1959 ; Holt, 1957;\nWinters, 1960), and has motivated some of the most successful forecasting methods.\nForecasts produced using exponential smoothing methods are weighted averages of\npast observations, with the weights decaying exponentially as the observations get\nolder. In other words, the more recent the observation the higher the associated\nweight. This framework generates reliable forecasts quickly and for a wide range of\ntime series, which is a great advantage and of major importance to applications in\nindustry.\nThis chapter is divided into two parts. In the first part (Sections 7.1– 7.4) we present\nthe mechanics of the most important exponential smoothing methods, and their\napplication in forecasting time series with various characteristics. This helps us\ndevelop an intuition to how these methods work. In this setting, selecting and using",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 441,
      "total_chunks": 873
    }
  },
  {
    "text": "application in forecasting time series with various characteristics. This helps us\ndevelop an intuition to how these methods work. In this setting, selecting and using\na forecasting method may appear to be somewhat ad hoc. The selection of the\nmethod is generally based on recognising key components of the time series (trend\nand seasonal) and the way in which these enter the smoothing method (e.g., in an\nadditive, damped or multiplicative manner).\nIn the second part of the chapter (Sections 7.5 – 7.7) we present the statistical models\nthat underlie exponential smoothing methods. These models generate identical\npoint forecasts to the methods discussed in the first part of the chapter, but also\ngenerate prediction intervals. Furthermore, this statistical framework allows for\ngenuine model selection between competing models.\nBibliographyBibliography\nBrown, R. G. (1959). Statistical forecasting for inventory control . McGraw/Hill.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 442,
      "total_chunks": 873
    }
  },
  {
    "text": "genuine model selection between competing models.\nBibliographyBibliography\nBrown, R. G. (1959). Statistical forecasting for inventory control . McGraw/Hill.\nHolt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted\naverages (O.N.R. Memorandum No. 52). Carnegie Institute of Technology,\nPittsburgh USA. [DOI]\n246",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 443,
      "total_chunks": 873
    }
  },
  {
    "text": "Winters, P. R. (1960). Forecasting sales by exponentially weighted moving\naverages. Management Science , 6(3), 324–342. [DOI]\n247",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 444,
      "total_chunks": 873
    }
  },
  {
    "text": "7.17.1  Simple exponential smoothingSimple exponential smoothing\nThe simplest of the exponentially smoothing methods is naturally called simplesimple\nexponential smoothingexponential smoothing  (SES) . This method is suitable for forecasting data with no\nclear trend or seasonal pattern. For example, the data in Figure 7.1  do not display any\nclear trending behaviour or any seasonality. (There is a rise in the last few years,\nwhich might suggest a trend. We will consider whether a trended method would be\nbetter for this series later in this chapter.) We have already considered the naïve and\nthe average as possible methods for forecasting such data (Section 3.1).\nFigure 7.1: Oil production in Saudi Arabia from 1996 to 2013.\nUsing the naïve method, all forecasts for the future are equal to the last observed\nvalue of the series,\n14\noildata <- window(oil, start=1996 )\nautoplot(oildata) +\n  ylab(\"Oil (millions of tonnes)\" ) + xlab(\"Year\")\n248",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 445,
      "total_chunks": 873
    }
  },
  {
    "text": "for . Hence, the naïve method assumes that the most recent observation\nis the only important one, and all previous observations provide no information for\nthe future. This can be thought of as a weighted average where all of the weight is\ngiven to the last observation.\nUsing the average method, all future forecasts are equal to a simple average of the\nobserved data,\nfor . Hence, the average method assumes that all observations are of\nequal importance, and gives them equal weights when generating forecasts.\nWe often want something between these two extremes. For example, it may be\nsensible to attach larger weights to more recent observations than to observations\nfrom the distant past. This is exactly the concept behind simple exponential\nsmoothing. Forecasts are calculated using weighted averages, where the weights\ndecrease exponentially as observations come from further in the past — the smallest\nweights are associated with the oldest observations:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 446,
      "total_chunks": 873
    }
  },
  {
    "text": "decrease exponentially as observations come from further in the past — the smallest\nweights are associated with the oldest observations:\nwhere  is the smoothing parameter. The one-step-ahead forecast for time\n is a weighted average of all of the observations in the series . The\nrate at which the weights decrease is controlled by the parameter .\nThe table below shows the weights attached to observations for four different values\nof  when forecasting using simple exponential smoothing. Note that the sum of the\nweights even for a small value of  will be approximately one for any reasonable\nsample size.\n0.2000 0.4000 0.6000 0.8000\n0.1600 0.2400 0.2400 0.1600\n0.1280 0.1440 0.0960 0.0320\n0.1024 0.0864 0.0384 0.0064\n0.0819 0.0518 0.0154 0.0013\n0.0655 0.0311 0.0061 0.0003\n/g24 /g1E /g12 /gD/g13 /gD/g9A\n/g3F/g35 /g16 /gC /g24 /g5D/g16 /g1E\n/g16\n/g11\n/g30/g1E/g12\n/g35 /g30 /gD/g12\n/g16\n/g24 /g1E /g12 /gD/g13 /gD/g9A",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 447,
      "total_chunks": 873
    }
  },
  {
    "text": "0.0655 0.0311 0.0061 0.0003\n/g24 /g1E /g12 /gD/g13 /gD/g9A\n/g3F/g35 /g16 /gC /g24 /g5D/g16 /g1E\n/g16\n/g11\n/g30/g1E/g12\n/g35 /g30 /gD/g12\n/g16\n/g24 /g1E /g12 /gD/g13 /gD/g9A\n/g3F/g35 /g16 /gC/g12/g5D /g16 /g1E /g43/g35 /g16 /gC /g43 /g9/g12 /gC3 /g43 /gA/g35 /g16 /gC3/g12 /gC /g43 /g9/g12 /gC3 /g43 /gA /g13 /g35 /g16 /gC3/g13 /gC /gFC /gD /g9/g18/gF/g12/gA\n/g11/gDE/g43 /gDE/g12\n/g16 /gC/g12 /g35 /g12 /gD/g9A/gD/g35 /g16\n/g43\n/g43\n/g43\n/g43 /g1E /g11/gF/g13 /g43 /g1E /g11/gF/g15 /g43 /g1E /g11/gF/g17 /g43 /g1E /g11/gF/g19\n/g35 /g16\n/g35 /g16 /gC3/g12\n/g35 /g16 /gC3/g13\n/g35 /g16 /gC3/g14\n/g35 /g16 /gC3/g15\n/g35 /g16 /gC3/g16\n249",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 448,
      "total_chunks": 873
    }
  },
  {
    "text": "For any  between 0 and 1, the weights attached to the observations decrease\nexponentially as we go back in time, hence the name “exponential smoothing”. If \nis small (i.e., close to 0), more weight is given to observations from the more distant\npast. If  is large (i.e., close to 1), more weight is given to the more recent\nobservations. For the extreme case where , , and the forecasts are\nequal to the naïve forecasts.\nWe present two equivalent forms of simple exponential smoothing, each of which\nleads to the forecast Equation (7.1).\nWeighted average formWeighted average form\nThe forecast at time  is equal to a weighted average between the most recent\nobservation  and the previous forecast :\nwhere  is the smoothing parameter. Similarly, we can write the fitted\nvalues as\nfor . (Recall that fitted values are simply one-step forecasts of the\ntraining data.)\nThe process has to start somewhere, so we let the first fitted value at time 1 be\ndenoted by  (which we will have to estimate). Then",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 449,
      "total_chunks": 873
    }
  },
  {
    "text": "training data.)\nThe process has to start somewhere, so we let the first fitted value at time 1 be\ndenoted by  (which we will have to estimate). Then\nSubstituting each equation into the following equation, we obtain\n250",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 450,
      "total_chunks": 873
    }
  },
  {
    "text": "The last term becomes tiny for large . So, the weighted average form leads to the\nsame forecast Equation (7.1).\nComponent formComponent form\nAn alternative representation is the component form. For simple exponential\nsmoothing, the only component included is the level, . (Other methods which are\nconsidered later in this chapter may also include a trend  and a seasonal\ncomponent .) Component form representations of exponential smoothing methods\ncomprise a forecast equation and a smoothing equation for each of the components\nincluded in the method. The component form of simple exponential smoothing is\ngiven by:\nwhere  is the level (or the smoothed value) of the series at time . Setting \ngives the fitted values, while setting  gives the true forecasts beyond the\ntraining data.\nThe forecast equation shows that the forecast value at time  is the estimated\nlevel at time . The smoothing equation for the level (usually referred to as the level",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 451,
      "total_chunks": 873
    }
  },
  {
    "text": "training data.\nThe forecast equation shows that the forecast value at time  is the estimated\nlevel at time . The smoothing equation for the level (usually referred to as the level\nequation) gives the estimated level of the series at each period .\nIf we replace  with  and  with  in the smoothing equation, we will\nrecover the weighted average form of simple exponential smoothing.\nThe component form of simple exponential smoothing is not particularly useful, but\nit will be the easiest form to use when we start adding other components.\n251",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 452,
      "total_chunks": 873
    }
  },
  {
    "text": "Flat forecastsFlat forecasts\nSimple exponential smoothing has a “flat” forecast function:\nThat is, all forecasts take the same value, equal to the last level component.\nRemember that these forecasts will only be suitable if the time series has no trend or\nseasonal component.\nOptimisationOptimisation\nThe application of every exponential smoothing method requires the smoothing\nparameters and the initial values to be chosen. In particular, for simple exponential\nsmoothing, we need to select the values of  and . All forecasts can be computed\nfrom the data once we know those values. For the methods that follow there is\nusually more than one smoothing parameter and more than one initial component to\nbe chosen.\nIn some cases, the smoothing parameters may be chosen in a subjective manner —\nthe forecaster specifies the value of the smoothing parameters based on previous\nexperience. However, a more reliable and objective way to obtain values for the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 453,
      "total_chunks": 873
    }
  },
  {
    "text": "the forecaster specifies the value of the smoothing parameters based on previous\nexperience. However, a more reliable and objective way to obtain values for the\nunknown parameters is to estimate them from the observed data.\nIn Section 5.2 , we estimated the coefficients of a regression model by minimising the\nsum of the squared residuals (usually known as SSE or “sum of squared errors”).\nSimilarly, the unknown parameters and the initial values for any exponential\nsmoothing method can be estimated by minimising the SSE. The residuals are\nspecified as  for . Hence, we find the values of the\nunknown parameters and the initial values that minimise\nUnlike the regression case (where we have formulas which return the values of the\nregression coefficients that minimise the SSE), this involves a non-linear\nminimisation problem, and we need to use an optimisation tool to solve it.\n252",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 454,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Oil productionExample: Oil production\nIn this example, simple exponential smoothing is applied to forecast oil production\nin Saudi Arabia.\nThis gives parameter estimates  and , obtained by minimising\nSSE over periods , subject to the restriction that .\nIn Table 7.1 we demonstrate the calculation using these parameters. The second last\ncolumn shows the estimated level for times  to ; the last few rows of the\nlast column show the forecasts for .\noildata <- window(oil, start=1996 )\n# Estimate parameters\nfc <- ses(oildata, h=5)\n# Accuracy of one-step-ahead training errors\nround(accuracy(fc),2)\n#>               ME  RMSE   MAE MPE MAPE MASE  ACF1\n#> Training set 6.4 28.12 22.26 1.1 4.61 0.93 -0.03\n253",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 455,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.1: Forecasting the total oil production in millions of tonnes for Saudi Arabia\nusing simple exponential smoothing.\nYearYear TimeTime ObservationObservation LevelLevel ForecastForecast\n1995 0 446.59\n1996 1 445.36 445.57 446.59\n1997 2 453.20 451.93 445.57\n1998 3 454.41 454.00 451.93\n1999 4 422.38 427.63 454.00\n2000 5 456.04 451.32 427.63\n2001 6 440.39 442.20 451.32\n2002 7 425.19 428.02 442.20\n2003 8 486.21 476.54 428.02\n2004 9 500.43 496.46 476.54\n2005 10 521.28 517.15 496.46\n2006 11 508.95 510.31 517.15\n2007 12 488.89 492.45 510.31\n2008 13 509.87 506.98 492.45\n2009 14 456.72 465.07 506.98\n2010 15 473.82 472.36 465.07\n2011 16 525.95 517.05 472.36\n2012 17 549.83 544.39 517.05\n2013 18 542.34 542.68 544.39\n2014 1 542.68\n2015 2 542.68\n2016 3 542.68\n2017 4 542.68\n2018 5 542.68\nThe black line in Figure 7.2 is a plot of the data, which shows a changing level over\ntime.\nautoplot(fc) +\n  autolayer(fitted(fc), series=\"Fitted\") +\n  ylab(\"Oil (millions of tonnes)\" ) + xlab(\"Year\")\n254",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 456,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 7.2: Simple exponential smoothing applied to oil production in Saudi Arabia (1996–\n2013).\nThe forecasts for the period 2014–2018 are plotted in Figure 7.2. Also plotted are\none-step-ahead fitted values alongside the data over the period 1996–2013. The\nlarge value of  in this example is reflected in the large adjustment that takes place\nin the estimated level  at each time. A smaller value of  would lead to smaller\nchanges over time, and so the series of fitted values would be smoother.\nThe prediction intervals shown here are calculated using the methods described in\nSection 7.7. The prediction intervals show that there is considerable uncertainty in\nthe future values of oil production over the five-year forecast period. So interpreting\nthe point forecasts without accounting for the large uncertainty can be very\nmisleading.\n14. In some books it is called “single exponential smoothing”. ↩ \n/g43\n/g9F /g30 /g43\n255",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 457,
      "total_chunks": 873
    }
  },
  {
    "text": "7.27.2  Trend methodsTrend methods\nHolt’s linear trend methodHolt’s linear trend method\nHolt (1957) extended simple exponential smoothing to allow the forecasting of data\nwith a trend. This method involves a forecast equation and two smoothing equations\n(one for the level and one for the trend):\nwhere  denotes an estimate of the level of the series at time ,  denotes an\nestimate of the trend (slope) of the series at time ,  is the smoothing parameter for\nthe level, , and  is the smoothing parameter for the trend, .\n(We denote this as  instead of  for reasons that will be explained in Section 7.5.)\nAs with simple exponential smoothing, the level equation here shows that  is a\nweighted average of observation  and the one-step-ahead training forecast for\ntime , here given by . The trend equation shows that  is a weighted\naverage of the estimated trend at time  based on  and , the previous\nestimate of the trend.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 458,
      "total_chunks": 873
    }
  },
  {
    "text": "time , here given by . The trend equation shows that  is a weighted\naverage of the estimated trend at time  based on  and , the previous\nestimate of the trend.\nThe forecast function is no longer flat but trending. The -step-ahead forecast is\nequal to the last estimated level plus  times the last estimated trend value. Hence\nthe forecasts are a linear function of .\nExample: Air PassengersExample: Air Passengers\nIn Table 7.2  we demonstrate the application of Holt’s method to annual passenger\nnumbers for Australian airlines. The smoothing parameters,  and , and the initial\nvalues  and  are estimated by minimising the SSE for the one-step training errors\nas in Section 7.1 .\nair <- window(ausair, start=1990 )\nfc <- holt(air, h=5)\n256",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 459,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.2: Applying Holt’s linear method with  and  to\nAustralian air passenger data (millions of passengers).\nYearYear TimeTime ObservationObservation LevelLevel SlopeSlope ForecastForecast\n1989 0 15.57 2.102\n1990 1 17.55 17.57 2.102 17.67\n1991 2 21.86 21.49 2.102 19.68\n1992 3 23.89 23.84 2.102 23.59\n1993 4 26.93 26.76 2.102 25.94\n1994 5 26.89 27.22 2.102 28.86\n1995 6 28.83 28.92 2.102 29.33\n1996 7 30.08 30.24 2.102 31.02\n1997 8 30.95 31.19 2.102 32.34\n1998 9 30.19 30.71 2.101 33.29\n1999 10 31.58 31.79 2.101 32.81\n2000 11 32.58 32.80 2.101 33.89\n2001 12 33.48 33.72 2.101 34.90\n2002 13 39.02 38.48 2.101 35.82\n2003 14 41.39 41.25 2.101 40.58\n2004 15 41.60 41.89 2.101 43.35\n2005 16 44.66 44.54 2.101 44.00\n2006 17 46.95 46.90 2.101 46.65\n2007 18 48.73 48.78 2.101 49.00\n2008 19 51.49 51.38 2.101 50.88\n2009 20 50.03 50.61 2.101 53.49\n2010 21 60.64 59.30 2.102 52.72\n2011 22 63.36 63.03 2.102 61.40\n2012 23 66.36 66.15 2.102 65.13\n2013 24 68.20 68.21 2.102 68.25",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 460,
      "total_chunks": 873
    }
  },
  {
    "text": "2008 19 51.49 51.38 2.101 50.88\n2009 20 50.03 50.61 2.101 53.49\n2010 21 60.64 59.30 2.102 52.72\n2011 22 63.36 63.03 2.102 61.40\n2012 23 66.36 66.15 2.102 65.13\n2013 24 68.20 68.21 2.102 68.25\n2014 25 68.12 68.49 2.102 70.31\n2015 26 69.78 69.92 2.102 70.60\n2016 27 72.60 72.50 2.102 72.02\n1 74.60\n2 76.70\n3 78.80\n4 80.91\n5 83.01\n257",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 461,
      "total_chunks": 873
    }
  },
  {
    "text": "The very small value of  means that the slope hardly changes over time.\nDamped trend methodsDamped trend methods\nThe forecasts generated by Holt’s linear method display a constant trend (increasing\nor decreasing) indefinitely into the future. Empirical evidence indicates that these\nmethods tend to over-forecast, especially for longer forecast horizons. Motivated by\nthis observation, Gardner & McKenzie ( 1985) introduced a parameter that\n“dampens” the trend to a flat line some time in the future. Methods that include a\ndamped trend have proven to be very successful, and are arguably the most popular\nindividual methods when forecasts are required automatically for many series.\nIn conjunction with the smoothing parameters  and  (with values between 0 and\n1 as in Holt’s method), this method also includes a damping parameter :\nIf , the method is identical to Holt’s linear method. For values between  and ,\n dampens the trend so that it approaches a constant some time in the future. In",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 462,
      "total_chunks": 873
    }
  },
  {
    "text": "If , the method is identical to Holt’s linear method. For values between  and ,\n dampens the trend so that it approaches a constant some time in the future. In\nfact, the forecasts converge to  as  for any value .\nThis means that short-run forecasts are trended while long-run forecasts are\nconstant.\nIn practice,  is rarely less than 0.8 as the damping has a very strong effect for\nsmaller values. Values of  close to 1 will mean that a damped model is not able to be\ndistinguished from a non-damped model. For these reasons, we usually restrict  to\na minimum of 0.8 and a maximum of 0.98.\nExample: Air Passengers (continued)Example: Air Passengers (continued)\nFigure 7.3  shows the forecasts for years 2017–2031 generated from Holt’s linear\ntrend method and the damped trend method.\n258",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 463,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 7.3: Forecasting to tal annual passengers of air carriers registered in Australia\n(millions of passeng ers, 1990–2016). For the damped trend method, .\nWe have set the damping parameter to a relatively low number  to\nexaggerate the effect of damping for comparison. Usually, we would estimate \nalong with the other parameters. We have also used a rather large forecast horizon (\n) to highlight the difference between a damped trend and a linear trend. In\npractice, we would not normally want to forecast so many years ahead with only 27\nyears of data.\nfc <- holt(air, h=15)\nfc2 <- holt(air, damped=TRUE, phi = 0.9, h=15)\nautoplot(air) +\n  autolayer(fc, series=\"Holt's method\", PI=FALSE ) +\n  autolayer(fc2, series=\"Damped Holt's method\", PI=FALSE ) +\n  ggtitle(\"Forecasts from Holt's method\" ) + xlab(\"Year\") +\n  ylab(\"Air passengers in Australia (millions)\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n/g5D /g1E /g11/gF/g1A/g11\n/g9/g5D /g1E /g11/gF/g1A/g11/gA\n/g5D",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 464,
      "total_chunks": 873
    }
  },
  {
    "text": "ylab(\"Air passengers in Australia (millions)\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n/g5D /g1E /g11/gF/g1A/g11\n/g9/g5D /g1E /g11/gF/g1A/g11/gA\n/g5D\n/g24 /g1E/g12 /g16\n259",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 465,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Sheep in AsiaExample: Sheep in Asia\nIn this example, we compare the forecasting performance of the three exponential\nsmoothing methods that we have considered so far in forecasting the sheep livestock\npopulation in Asia. The data spans the period 1961–2007 and is shown in Figure 7.4.\nFigure 7.4: Annual sheep livestock numbers in Asia (in million head)\nWe will use time series cross-validation to compare the one-step forecast accuracy\nof the three methods.\nautoplot(livestock) +\n  xlab(\"Year\") + ylab(\"Livestock, sheep in Asia (millions)\" )\n260",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 466,
      "total_chunks": 873
    }
  },
  {
    "text": "Damped Holt’s method is best whether you compare MAE or MSE values. So we will\nproceed with using the damped Holt’s method and apply it to the whole data set to\nget forecasts for future years.\ne1 <- tsCV(livestock, ses, h=1)\ne2 <- tsCV(livestock, holt, h=1)\ne3 <- tsCV(livestock, holt, damped=TRUE, h=1)\n# Compare MSE:\nmean(e1^2, na.rm=TRUE)\n#> [1] 178.3\nmean(e2^2, na.rm=TRUE)\n#> [1] 173.4\nmean(e3^2, na.rm=TRUE)\n#> [1] 162.6\n# Compare MAE:\nmean(abs(e1), na.rm=TRUE)\n#> [1] 8.532\nmean(abs(e2), na.rm=TRUE)\n#> [1] 8.803\nmean(abs(e3), na.rm=TRUE)\n#> [1] 8.024\n261",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 467,
      "total_chunks": 873
    }
  },
  {
    "text": "The smoothing parameter for the slope is estimated to be essentially zero, indicating\nthat the trend is not changing over time. The value of  is very close to one, showing\nthat the level reacts strongly to each new observation.\nfc <- holt(livestock, damped=TRUE )\n# Estimated parameters:\nfc[[\"model\"]]\n#> Damped Holt's method \n#> \n#> Call:\n#>  holt(y = livestock, damped = TRUE) \n#> \n#>   Smoothing parameters:\n#>     alpha = 0.9999 \n#>     beta  = 3e-04 \n#>     phi   = 0.9798 \n#> \n#>   Initial states:\n#>     l = 223.35 \n#>     b = 6.9046 \n#> \n#>   sigma:  12.84\n#> \n#>   AIC  AICc   BIC \n#> 427.6 429.7 438.7\n/g43\nautoplot(fc) +\n  xlab(\"Year\") + ylab(\"Livestock, sheep in Asia (millions)\" )\n262",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 468,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 7.5: Forecasting livestock, sheep in Asia: comparing forecasting performance of\nnon-seasonal method.\nThe resulting forecasts look sensible with increasing trend, and relatively wide\nprediction intervals reflecting the variation in the historical data. The prediction\nintervals are calculated using the methods described in Section 7.7.\nIn this example, the process of selecting a method was relatively easy as both MSE\nand MAE comparisons suggested the same method (damped Holt’s). However,\nsometimes different accuracy measures will suggest different forecasting methods,\nand then a decision is required as to which forecasting method we prefer to use. As\nforecasting tasks can vary by many dimensions (length of forecast horizon, size of\ntest set, forecast error measures, frequency of data, etc.), it is unlikely that one\nmethod will be better than all others for all forecasting scenarios. What we require",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 469,
      "total_chunks": 873
    }
  },
  {
    "text": "test set, forecast error measures, frequency of data, etc.), it is unlikely that one\nmethod will be better than all others for all forecasting scenarios. What we require\nfrom a forecasting method are consistently sensible forecasts, and these should be\nfrequently evaluated against the task at hand.\nBibliographyBibliography\nGardner, E. S., & McKenzie, E. (1985). Forecasting trends in time series.\nManagement Science , 31(10), 1237–1246. [DOI]\nHolt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted\naverages (O.N.R. Memorandum No. 52). Carnegie Institute of Technology,\n263",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 470,
      "total_chunks": 873
    }
  },
  {
    "text": "Pittsburgh USA. [DOI]\n264",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 471,
      "total_chunks": 873
    }
  },
  {
    "text": "7.37.3  Holt-Winters’ seasonal methodHolt-Winters’ seasonal method\nHolt (1957) and Winters ( 1960) extended Holt’s method to capture seasonality. The\nHolt-Winters seasonal method comprises the forecast equation and three smoothing\nequations — one for the level , one for the trend , and one for the seasonal\ncomponent , with corresponding smoothing parameters ,  and . We use  to\ndenote the frequency of the seasonality, i.e., the number of seasons in a year. For\nexample, for quarterly data , and for monthly data .\nThere are two variations to this method that differ in the nature of the seasonal\ncomponent. The additive method is preferred when the seasonal variations are\nroughly constant through the series, while the multiplicative method is preferred\nwhen the seasonal variations are changing proportional to the level of the series.\nWith the additive method, the seasonal component is expressed in absolute terms in",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 472,
      "total_chunks": 873
    }
  },
  {
    "text": "when the seasonal variations are changing proportional to the level of the series.\nWith the additive method, the seasonal component is expressed in absolute terms in\nthe scale of the observed series, and in the level equation the series is seasonally\nadjusted by subtracting the seasonal component. Within each year, the seasonal\ncomponent will add up to approximately zero. With the multiplicative method, the\nseasonal component is expressed in relative terms (percentages), and the series is\nseasonally adjusted by dividing through by the seasonal component. Within each\nyear, the seasonal component will sum up to approximately .\nHolt-Winters’ additive methodHolt-Winters’ additive method\nThe component form for the additive method is:\nwhere  is the integer part of , which ensures that the estimates of the\nseasonal indices used for forecasting come from the final year of the sample. The\nlevel equation shows a weighted average between the seasonally adjusted",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 473,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonal indices used for forecasting come from the final year of the sample. The\nlevel equation shows a weighted average between the seasonally adjusted\nobservation  and the non-seasonal forecast  for time . The\n265",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 474,
      "total_chunks": 873
    }
  },
  {
    "text": "trend equation is identical to Holt’s linear method. The seasonal equation shows a\nweighted average between the current seasonal index, , and the\nseasonal index of the same season last year (i.e.,  time periods ago).\nThe equation for the seasonal component is often expressed as\nIf we substitute  from the smoothing equation for the level of the component form\nabove, we get\nwhich is identical to the smoothing equation for the seasonal component we specify\nhere, with . The usual parameter restriction is , which\ntranslates to .\nHolt-Winters’ multiplicative methodHolt-Winters’ multiplicative method\nThe component form for the multiplicative method is:\nExample: International tourist visitor nights in AustraliaExample: International tourist visitor nights in Australia\nWe apply Holt-Winters’ method with both additive and multiplicative seasonality to\nforecast quarterly visitor nights in Australia spent by international tourists. Figure",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 475,
      "total_chunks": 873
    }
  },
  {
    "text": "We apply Holt-Winters’ method with both additive and multiplicative seasonality to\nforecast quarterly visitor nights in Australia spent by international tourists. Figure\n7.6 shows the data from 2005, and the forecasts for 2016–2017. The data show an\nobvious seasonal pattern, with peaks observed in the March quarter of each year,\ncorresponding to the Australian summer.\n266",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 476,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 7.6: Forecasting international visitor nights in Australia using the Holt-Winters\nmethod with both additive and multiplicative seasonality.\naust <- window(austourists,start=2005 )\nfit1 <- hw(aust,seasonal=\"additive\")\nfit2 <- hw(aust,seasonal=\"multiplicative\")\nautoplot(aust) +\n  autolayer(fit1, series=\"HW additive forecasts\" , PI=FALSE) +\n  autolayer(fit2, series=\"HW multiplicative forecasts\" ,\n    PI=FALSE) +\n  xlab(\"Year\") +\n  ylab(\"Visitor nights (millions)\") +\n  ggtitle(\"International visitors nights in Australia\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n267",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 477,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.3: Applying Holt-Winters’ method with additive seasonality for forecasting\ninternational visitor nights in Australia. Notice that the additive seasonal component\nsums to approximately zero. The smoothing parameters and initial estimates for the\ncomponents have been estimated by minimising RMSE ( , ,\n and RMSE ).\n2004 Q1 -3 9.70\n2004 Q2 -2 -9.31\n2004 Q3 -1 -1.69\n2004 Q4 0 32.26 0.70 1.31\n2005 Q1 1 42.21 32.82 0.70 9.50 42.66\n2005 Q2 2 24.65 33.66 0.70 -9.13 24.21\n2005 Q3 3 32.67 34.36 0.70 -1.69 32.67\n2005 Q4 4 37.26 35.33 0.70 1.69 36.37\n⋮⋮ ⋮⋮⋮ ⋮\n2015 Q1 41 73.26 59.96 0.70 12.18 69.05\n2015 Q2 42 47.70 60.69 0.70 -13.02 47.59\n2015 Q3 43 61.10 61.96 0.70 -1.35 59.24\n2015 Q4 44 66.06 63.22 0.70 2.35 64.22\n2016 Q1 1 76.10\n2016 Q2 2 51.60\n2016 Q3 3 63.97\n2016 Q4 4 68.37\n2017 Q1 5 78.90\n2017 Q2 6 54.41\n2017 Q3 7 66.77\n2017 Q4 8 71.18\n/g43 /g1E /g11/gF/g14/g11/g17 /g44 /gC7 /g1E /g11/gF/g11/g11/g11/g14\n/g45 /g1E /g11/gF/g15/g13/g17 /g1E /g12/gF/g18/g17/g14",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 478,
      "total_chunks": 873
    }
  },
  {
    "text": "2017 Q1 5 78.90\n2017 Q2 6 54.41\n2017 Q3 7 66.77\n2017 Q4 8 71.18\n/g43 /g1E /g11/gF/g14/g11/g17 /g44 /gC7 /g1E /g11/gF/g11/g11/g11/g14\n/g45 /g1E /g11/gF/g15/g13/g17 /g1E /g12/gF/g18/g17/g14\n/g30/g35 /g30 /g9F /g30 /g1E /g30 /g2F /g30 /g3F/g35 /g30\n/g24 /g3F/g35 /g16 /gC /g24/g5D /g16\n268",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 479,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.4: Applying Holt-Winters’ method with multiplicative seasonality for\nforecasting international visitor nights in Australia. Notice that the multiplicative\nseasonal component sums to approximately . The smoothing parameters and\ninitial estimates for the components have been estimated by minimising RMSE (\n, ,  and RMSE ).\n2004 Q1 -3 1.24\n2004 Q2 -2 0.77\n2004 Q3 -1 0.96\n2004 Q4 0 32.49 0.70 1.02\n2005 Q1 1 42.21 33.51 0.71 1.24 41.29\n2005 Q2 2 24.65 33.24 0.68 0.77 26.36\n2005 Q3 3 32.67 33.94 0.68 0.96 32.62\n2005 Q4 4 37.26 35.40 0.70 1.02 35.44\n⋮⋮ ⋮⋮⋮ ⋮\n2015 Q1 41 73.26 58.57 0.66 1.24 72.59\n2015 Q2 42 47.70 60.42 0.69 0.77 45.62\n2015 Q3 43 61.10 62.17 0.72 0.96 58.77\n2015 Q4 44 66.06 63.62 0.75 1.02 64.38\n2016 Q1 1 80.09\n2016 Q2 2 50.15\n2016 Q3 3 63.34\n2016 Q4 4 68.18\n2017 Q1 5 83.80\n2017 Q2 6 52.45\n2017 Q3 7 66.21\n2017 Q4 8 71.23\nThe applications of both methods (with additive and multiplicative seasonality) are",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 480,
      "total_chunks": 873
    }
  },
  {
    "text": "2016 Q2 2 50.15\n2016 Q3 3 63.34\n2016 Q4 4 68.18\n2017 Q1 5 83.80\n2017 Q2 6 52.45\n2017 Q3 7 66.21\n2017 Q4 8 71.23\nThe applications of both methods (with additive and multiplicative seasonality) are\npresented in Tables 7.3  and 7.4 respectively. Because both methods have exactly the\nsame number of parameters to estimate, we can compare the training RMSE from\nboth models. In this case, the method with multiplicative seasonality fits the data\nbest. This was to be expected, as the time plot shows that the seasonal variation in\nthe data increases as the level of the series increases. This is also reflected in the two\nsets of forecasts; the forecasts generated by the method with the multiplicative\nseasonality display larger and increasing seasonal variation as the level of the\nforecasts increases compared to the forecasts generated by the method with additive\nseasonality.\n/g29 /g1E/g15",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 481,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonality display larger and increasing seasonal variation as the level of the\nforecasts increases compared to the forecasts generated by the method with additive\nseasonality.\n/g29 /g1E/g15\n/g43 /g1E /g11/gF/g15/g15/g12 /g44 /gC7 /g1E /g11/gF/g11/g14/g11 /g45 /g1E /g11/gF/g11/g11/g13 /g1E /g12/gF/g16/g18/g17\n/g30/g35 /g30 /g9F /g30 /g1E /g30 /g2F /g30 /g3F/g35 /g30\n/g24 /g3F/g35 /g16 /gC /g24/g5D /g16\n269",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 482,
      "total_chunks": 873
    }
  },
  {
    "text": "The estimated states for both models are plotted in Figure 7.7. The small value of \nfor the multiplicative model means that the seasonal component hardly changes\nover time. The small value of  for the additive model means the slope component\nhardly changes over time (check the vertical scale). The increasing size of the\nseasonal component for the additive model suggests that the model is less\nappropriate than the multiplicative model.\nFigure 7.7: Estimated components for the Holt-Winters method with additive and\nmultiplicative seasonal components.\nHolt-Winters’ damped methodHolt-Winters’ damped method\nDamping is possible with both additive and multiplicative Holt-Winters’ methods. A\nmethod that often provides accurate and robust forecasts for seasonal data is the\nHolt-Winters method with a damped trend and multiplicative seasonality:\nhw(y, damped=TRUE, seasonal=\"multiplicative\")\n270",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 483,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Holt-Winters method with daily dataExample: Holt-Winters method with daily data\nThe Holt-Winters method can also be used for daily type of data, where the seasonal\nperiod is , and the appropriate unit of time for  is in days. Here, we generate\ndaily forecasts for the last five weeks for the ıhyndsightı data, which contains the\ndaily pageviews on the Hyndsight blog for one year starting April 30, 2014.\nFigure 7.8: Forecasts of daily pageviews on the Hyndsight blog.\nClearly the model has identified the weekly seasonal pattern and the increasing trend\nat the end of the data, and the forecasts are a close match to the test data.\nBibliographyBibliography\nHolt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted\naverages (O.N.R. Memorandum No. 52). Carnegie Institute of Technology,\nPittsburgh USA. [DOI]\nWinters, P. R. (1960). Forecasting sales by exponentially weighted moving\naverages. Management Science , 6(3), 324–342. [DOI]",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 484,
      "total_chunks": 873
    }
  },
  {
    "text": "Pittsburgh USA. [DOI]\nWinters, P. R. (1960). Forecasting sales by exponentially weighted moving\naverages. Management Science , 6(3), 324–342. [DOI]\nfc <- hw(subset(hyndsight,end= length(hyndsight)-35),\n         damped = TRUE, seasonal=\"multiplicative\", h=35)\nautoplot(hyndsight) +\n  autolayer(fc, series=\"HW multi damped\", PI=FALSE )+\n  guides(colour=guide_legend(title=\"Daily forecasts\"))\n271",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 485,
      "total_chunks": 873
    }
  },
  {
    "text": "272",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 486,
      "total_chunks": 873
    }
  },
  {
    "text": "7.47.4  A taxonomy of exponential smoothingA taxonomy of exponential smoothing\nmethodsmethods\nExponential smoothing methods are not restricted to those we have presented so far.\nBy considering variations in the combinations of the trend and seasonal\ncomponents, nine exponential smoothing methods are possible, listed in Table 7.5.\nEach method is labelled by a pair of letters (T,S) defining the type of ‘Trend’ and\n‘Seasonal’ components. For example, (A,M) is the method with an additive trend and\nmultiplicative seasonality; (A ,N) is the method with damped trend and no\nseasonality; and so on.\nTable 7.5: A two-way classification of exponential smoothing methods.\nTrend ComponentTrend Component Seasonal ComponentSeasonal Component\nNN AA MM\n(None)(None) (Additive)(Additive) (Multiplicative)(Multiplicative)\nN (None)N (None) (N,N) (N,A) (N,M)\nA (Additive)A (Additive) (A,N) (A,A) (A,M)\nAA  (Additive damped)(Additive damped) (A ,N) (A ,A) (A ,M)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 487,
      "total_chunks": 873
    }
  },
  {
    "text": "N (None)N (None) (N,N) (N,A) (N,M)\nA (Additive)A (Additive) (A,N) (A,A) (A,M)\nAA  (Additive damped)(Additive damped) (A ,N) (A ,A) (A ,M)\nSome of these methods we have already seen using other names:\nShort handShort hand MethodMethod\n(N,N) Simple exponential smoothing\n(A,N) Holt’s linear method\n(A ,N) Additive damped trend method\n(A,A) Additive Holt-Winters’ method\n(A,M) Multiplicative Holt-Winters’ method\n(A ,M) Holt-Winters’ damped method\nThis type of classification was first proposed by Pegels ( 1969), who also included a\nmethod with a multiplicative trend. It was later extended by Gardner ( 1985) to\ninclude methods with an additive damped trend and by Taylor ( 2003) to include\nmethods with a multiplicative damped trend. We do not consider the multiplicative\ntrend methods in this book as they tend to produce poor forecasts. See Hyndman,\nKoehler, Ord, & Snyder ( 2008) for a more thorough discussion of all exponential\nsmoothing methods.\n273",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 488,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.6 gives the recursive formulas for applying the nine exponential smoothing\nmethods in Table 7.5. Each cell includes the forecast equation for generating -step-\nahead forecasts, and the smoothing equations for applying the method.\nTable 7.6: Formulas for recursive calculations and point forecasts. In each case, \ndenotes the series level at time ,  denotes the slope at time ,  denotes the\nseasonal component of the series at time , and  denotes the number of seasons in\na year; , ,  and  are smoothing parameters, , and  is\nthe integer part of .\nBibliographyBibliography\nGardner, E. S. (1985). Exponential smoothing: The state of the art. Journal of\nForecasting , 4(1), 1–28. [DOI]\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). Forecasting with\nexponential smoothing: The state space approach . Berlin: Springer-Verlag.\nhttp://www.exponentialsmoothing.net\nPegels, C. C. (1969). Exponential forecasting: Some new variations. Management\nScience, 15(5), 311–315. [DOI]",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 489,
      "total_chunks": 873
    }
  },
  {
    "text": "http://www.exponentialsmoothing.net\nPegels, C. C. (1969). Exponential forecasting: Some new variations. Management\nScience, 15(5), 311–315. [DOI]\nTaylor, J. W. (2003). Exponential smoothing with a damped multiplicative trend.\nInternational Journal of Forecasting , 19(4), 715–725. [DOI]\n274",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 490,
      "total_chunks": 873
    }
  },
  {
    "text": "7.57.5  Innovations state space models for exponentialInnovations state space models for exponential\nsmoothingsmoothing\nIn the rest of this chapter, we study the statistical models that underlie the\nexponential smoothing methods we have considered so far. The exponential\nsmoothing methods presented in Table 7.6 are algorithms which generate point\nforecasts. The statistical models in this section generate the same point forecasts,\nbut can also generate prediction (or forecast) intervals. A statistical model is a\nstochastic (or random) data generating process that can produce an entire forecast\ndistribution. We will also describe how to use the model selection criteria introduced\nin Chapter 5 to choose the model in an objective manner.\nEach model consists of a measurement equation that describes the observed data,\nand some state equations that describe how the unobserved components or states",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 491,
      "total_chunks": 873
    }
  },
  {
    "text": "Each model consists of a measurement equation that describes the observed data,\nand some state equations that describe how the unobserved components or states\n(level, trend, seasonal) change over time. Hence, these are referred to as state spacestate space\nmodelsmodels.\nFor each method there exist two models: one with additive errors and one with\nmultiplicative errors. The point forecasts produced by the models are identical if\nthey use the same smoothing parameter values. They will, however, generate\ndifferent prediction intervals.\nTo distinguish between a model with additive errors and one with multiplicative\nerrors (and also to distinguish the models from the methods), we add a third letter to\nthe classification of Table 7.5. We label each state space model as ETS( ) for\n(Error, Trend, Seasonal). This label can also be thought of as ExponenTial\nSmoothing. Using the same notation as in Table 7.5, the possibilities for each\ncomponent are: Error A,M , Trend N,A,A  and Seasonal N,A,M .",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 492,
      "total_chunks": 873
    }
  },
  {
    "text": "Smoothing. Using the same notation as in Table 7.5, the possibilities for each\ncomponent are: Error A,M , Trend N,A,A  and Seasonal N,A,M .\n275",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 493,
      "total_chunks": 873
    }
  },
  {
    "text": "ETS(A,N,N): simple exponential smoothing with additiveETS(A,N,N): simple exponential smoothing with additive\nerrorserrors\nRecall the component form of simple exponential smoothing:\nIf we re-arrange the smoothing equation for the level, we get the “error correction”\nform:\nwhere  is the residual at time .\nThe training data errors lead to the adjustment of the estimated level throughout the\nsmoothing process for . For example, if the error at time  is negative,\nthen  and so the level at time  has been over-estimated. The new\nlevel  is then the previous level  adjusted downwards. The closer  is to one, the\n“rougher” the estimate of the level (large adjustments take place). The smaller the \n, the “smoother” the level (small adjustments take place).\nWe can also write , so that each observation can be represented by the\nprevious level plus an error. To make this into an innovations state space model, all\nwe need to do is specify the probability distribution for . For a model with additive",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 494,
      "total_chunks": 873
    }
  },
  {
    "text": "previous level plus an error. To make this into an innovations state space model, all\nwe need to do is specify the probability distribution for . For a model with additive\nerrors, we assume that residuals (the one-step training errors)  are normally\ndistributed white noise with mean 0 and variance . A short-hand notation for this\nis ; NID stands for “normally and independently distributed”.\nThen the equations of the model can be written as\nWe refer to (7.3) as the measurement  (or observation) equation and (7.4) as the state\n(or transition) equation. These two equations, together with the statistical\ndistribution of the errors, form a fully specified statistical model. Specifically, these\nconstitute an innovations state space model underlying simple exponential\nsmoothing.\n276",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 495,
      "total_chunks": 873
    }
  },
  {
    "text": "The term “innovations” comes from the fact that all equations use the same random\nerror process, . For the same reason, this formulation is also referred to as a\n“single source of error” model. There are alternative multiple source of error\nformulations which we do not present here.\nThe measurement equation shows the relationship between the observations and the\nunobserved states. In this case, observation  is a linear function of the level ,\nthe predictable part of , and the error , the unpredictable part of . For other\ninnovations state space models, this relationship may be nonlinear.\nThe state equation shows the evolution of the state through time. The influence of\nthe smoothing parameter  is the same as for the methods discussed earlier. For\nexample,  governs the amount of change in successive levels: high values of  allow\nrapid changes in the level; low values of  lead to smooth changes. If , the level\nof the series does not change over time; if , the model reduces to a random",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 496,
      "total_chunks": 873
    }
  },
  {
    "text": "rapid changes in the level; low values of  lead to smooth changes. If , the level\nof the series does not change over time; if , the model reduces to a random\nwalk model, . (See Section 8.1 for a discussion of this model.)\nETS(M,N,N): simple exponential smoothing with multiplicativeETS(M,N,N): simple exponential smoothing with multiplicative\nerrorserrors\nIn a similar fashion, we can specify models with multiplicative errors by writing the\none-step-ahead training errors as relative errors:\nwhere . Substituting  gives  and\n.\nThen we can write the multiplicative form of the state space model as\nETS(A,A,N): Holt’s linear method with additive errorsETS(A,A,N): Holt’s linear method with additive errors\nFor this model, we assume that the one-step-ahead training errors are given by\n. Substituting this into the error correction\nequations for Holt’s linear method we obtain\n277",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 497,
      "total_chunks": 873
    }
  },
  {
    "text": "where, for simplicity, we have set .\nETS(M,A,N): Holt’s linear method with multiplicative errorsETS(M,A,N): Holt’s linear method with multiplicative errors\nSpecifying one-step-ahead training errors as relative errors such that\nand following an approach similar to that used above, the innovations state space\nmodel underlying Holt’s linear method with multiplicative errors is specified as\nwhere again  and .\nOther ETS modelsOther ETS models\nIn a similar fashion, we can write an innovations state space model for each of the\nexponential smoothing methods of Table 7.6. Table 7.7 presents the equations for all\nof the models in the ETS framework.\n278\n\n\nTable 7.7: State space equations for each of the models in the ETS framework.\n279",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 498,
      "total_chunks": 873
    }
  },
  {
    "text": "7.67.6  Estimation and model selectionEstimation and model selection\nEstimating ETS modelsEstimating ETS models\nAn alternative to estimating the parameters by minimising the sum of squared errors\nis to maximise the “likelihood”. The likelihood is the probability of the data arising\nfrom the specified model. Thus, a large likelihood is associated with a good model.\nFor an additive error model, maximising the likelihood (assuming normally\ndistributed errors) gives the same results as minimising the sum of squared errors.\nHowever, different results will be obtained for multiplicative error models. In this\nsection, we will estimate the smoothing parameters , ,  and , and the initial\nstates , , , by maximising the likelihood.\nThe possible values that the smoothing parameters can take are restricted.\nTraditionally, the parameters have been constrained to lie between 0 and 1 so that\nthe equations can be interpreted as weighted averages. That is, .",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 499,
      "total_chunks": 873
    }
  },
  {
    "text": "Traditionally, the parameters have been constrained to lie between 0 and 1 so that\nthe equations can be interpreted as weighted averages. That is, .\nFor the state space models, we have set  and . Therefore, the\ntraditional restrictions translate to ,  and . In\npractice, the damping parameter  is usually constrained further to prevent\nnumerical difficulties in estimating the model. In R, it is restricted so that\n.\nAnother way to view the parameters is through a consideration of the mathematical\nproperties of the state space models. The parameters are constrained in order to\nprevent observations in the distant past having a continuing effect on current\nforecasts. This leads to some admissibility  constraints on the parameters, which are\nusually (but not always) less restrictive than the traditional constraints region\n(Hyndman et al., 2008, p. Ch10 ). For example, for the ETS(A,N,N) model, the\ntraditional parameter region is  but the admissible region is .",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 500,
      "total_chunks": 873
    }
  },
  {
    "text": "(Hyndman et al., 2008, p. Ch10 ). For example, for the ETS(A,N,N) model, the\ntraditional parameter region is  but the admissible region is .\nFor the ETS(A,A,N) model, the traditional parameter region is  and\n but the admissible region is  and .\n280",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 501,
      "total_chunks": 873
    }
  },
  {
    "text": "Model selectionModel selection\nA great advantage of the ETS statistical framework is that information criteria can be\nused for model selection. The AIC, AIC  and BIC, introduced in Section 5.5, can be\nused here to determine which of the ETS models is most appropriate for a given time\nseries.\nFor ETS models, Akaike’s Information Criterion (AIC) is defined as\nwhere  is the likelihood of the model and  is the total number of parameters and\ninitial states that have been estimated (including the residual variance).\nThe AIC corrected for small sample bias (AIC ) is defined as\nand the Bayesian Information Criterion (BIC) is\nThree of the combinations of (Error, Trend, Seasonal) can lead to numerical\ndifficulties. Specifically, the models that can cause such instabilities are ETS(A,N,M),\nETS(A,A,M), and ETS(A,A ,M), due to division by values potentially close to zero in\nthe state equations. We normally do not consider these particular combinations\nwhen selecting a model.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 502,
      "total_chunks": 873
    }
  },
  {
    "text": "ETS(A,A,M), and ETS(A,A ,M), due to division by values potentially close to zero in\nthe state equations. We normally do not consider these particular combinations\nwhen selecting a model.\nModels with multiplicative errors are useful when the data are strictly positive, but\nare not numerically stable when the data contain zeros or negative values. Therefore,\nmultiplicative error models will not be considered if the time series is not strictly\npositive. In that case, only the six fully additive models will be applied.\nThe The ĳets()ĳ function in R function in R\nThe models can be estimated in R using the ıets()ı function in the forecastforecast package.\nUnlike the ıses()ı, ıholt()ı and ıhw()ı functions, the ıets()ı function does not\nproduce forecasts. Rather, it estimates the model parameters and returns\ninformation about the fitted model. By default it uses the AICc to select an\nappropriate model, although other information criteria can be selected.\n281",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 503,
      "total_chunks": 873
    }
  },
  {
    "text": "The R code below shows the most important arguments that this function can take,\nand their default values. If only the time series is specified, and all other arguments\nare left at their default values, then an appropriate model will be selected\nautomatically. We explain the arguments below. See the help file for a complete\ndescription.\nıyı\nThe time series to be forecast.\nımodelı\nA three-letter code indicating the model to be estimated using the ETS\nclassification and notation. The possible inputs are “N” for none, “A” for\nadditive, “M” for multiplicative, or “Z” for automatic selection. If any of the\ninputs is left as “Z”, then this component is selected according to the\ninformation criterion. The default value of ıZZZı ensures that all components are\nselected using the information criterion.\nıdampedı\nIf ıdamped=TRUEı, then a damped trend will be used (either A or M). If\nıdamped=FALSEı, then a non-damped trend will used. If ıdamped=NULLı (the default),",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 504,
      "total_chunks": 873
    }
  },
  {
    "text": "ıdampedı\nIf ıdamped=TRUEı, then a damped trend will be used (either A or M). If\nıdamped=FALSEı, then a non-damped trend will used. If ıdamped=NULLı (the default),\nthen either a damped or a non-damped trend will be selected, depending on\nwhich model has the smallest value for the information criterion.\nıalphaı, , ıbetaı, , ıgammaı, , ıphiı\nThe values of the smoothing parameters can be specified using these arguments.\nIf they are set to ıNULLı (the default setting for each of them), the parameters are\nestimated.\nılambdaı\nBox-Cox transformation parameter. It will be ignored if ılambda=NULLı (the\ndefault value). Otherwise, the time series will be transformed before the model is\nestimated. When ılambdaı is not ıNULLı, ıadditive.onlyı is set to ıTRUEı.\nıbiasadjı\nIf ıTRUEı and ılambdaı is not ıNULLı, then the back-transformed fitted values and\nforecasts will be bias-adjusted.\nets(y, model=\"ZZZ\", damped=NULL, alpha=NULL, beta=NULL,\n    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 505,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasts will be bias-adjusted.\nets(y, model=\"ZZZ\", damped=NULL, alpha=NULL, beta=NULL,\n    gamma=NULL, phi=NULL, lambda=NULL, biasadj=FALSE,\n    additive.only=FALSE, restrict=TRUE,\n    allow.multiplicative.trend= FALSE)\n282",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 506,
      "total_chunks": 873
    }
  },
  {
    "text": "ıadditive.only ı\nOnly models with additive components will be considered if ıadditive.only=TRUEı.\nOtherwise, all models will be considered.\nırestrictı\nIf ırestrict=TRUEı (the default), the models that cause numerical difficulties are\nnot considered in model selection.\nıallow.multiplicative.trend ı\nMultiplicative trend models are also available, but not covered in this book. Set\nthis argument to ıTRUEı to allow these models to be considered.\nWorking with ĳetsĳ objects objects\nThe ıets()ı function will return an object of class ıetsı. There are many R functions\ndesigned to make working with ıetsı objects easy. A few of them are described\nbelow.\nıcoef()ı\nreturns all fitted parameters.\nıaccuracy()ı\nreturns accuracy measures computed on the training data.\nısummary()ı\nprints some summary information about the fitted model.\nıautoplot()ı and ıplot()ı\nproduce time plots of the components.\nıresiduals() ı\nreturns residuals from the estimated model.\nıfitted()ı",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 507,
      "total_chunks": 873
    }
  },
  {
    "text": "prints some summary information about the fitted model.\nıautoplot()ı and ıplot()ı\nproduce time plots of the components.\nıresiduals() ı\nreturns residuals from the estimated model.\nıfitted()ı\nreturns one-step forecasts for the training data.\nısimulate()ı\nwill simulate future sample paths from the fitted model.\nıforecast()ı\ncomputes point forecasts and prediction intervals, as described in the next\nsection.\n283",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 508,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: International tourist visitor nights in AustraliaExample: International tourist visitor nights in Australia\nWe now employ the ETS statistical framework to forecast tourist visitor nights in\nAustralia by international arrivals over the period 2016–2019. We let the ıets()ı\nfunction select the model by minimising the AICc.\naust <- window(austourists, start=2005 )\nfit <- ets(aust)\nsummary(fit)\n#> ETS(M,A,M) \n#> \n#> Call:\n#>  ets(y = aust) \n#> \n#>   Smoothing parameters:\n#>     alpha = 0.1908 \n#>     beta  = 0.0392 \n#>     gamma = 2e-04 \n#> \n#>   Initial states:\n#>     l = 32.3679 \n#>     b = 0.9281 \n#>     s = 1.022 0.9628 0.7683 1.247\n#> \n#>   sigma:  0.0383\n#> \n#>   AIC  AICc   BIC \n#> 224.9 230.2 240.9 \n#> \n#> Training set error measures:\n#>                   ME  RMSE  MAE     MPE  MAPE   MASE   ACF1\n#> Training set 0.04837 1.671 1.25 -0.1846 2.693 0.4095 0.2006\n284",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 509,
      "total_chunks": 873
    }
  },
  {
    "text": "The model selected is ETS(M,A,M):\nThe parameter estimates are , , and . The output\nalso returns the estimates for the initial states , , , ,  and  Compare\nthese with the values obtained for the equivalent Holt-Winters method with\nmultiplicative seasonality presented in Table 7.4. The ETS(M,A,M) model will give\ndifferent point forecasts to the multiplicative Holt-Winters’ method, because the\nparameters have been estimated differently. With the ıets()ı function, the default\nestimation method is maximum likelihood rather than minimum sum of squares.\nFigure 7.9 shows the states over time, while Figure 7.11 shows point forecasts and\nprediction intervals generated from the model. The small values of  and  mean\nthat the slope and seasonal components change very little over time. The narrow\nprediction intervals indicate that the series is relatively easy to forecast due to the\nstrong trend and seasonality.\nFigure 7.9: Graphical representation of the estimated states over time.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 510,
      "total_chunks": 873
    }
  },
  {
    "text": "prediction intervals indicate that the series is relatively easy to forecast due to the\nstrong trend and seasonality.\nFigure 7.9: Graphical representation of the estimated states over time.\n/g35 /g30 /g1E/g9 /g9F /g30/gC3/g12 /gC /g1E /g30/gC3/g12 /gA/g2F /g30/gC3 /g29 /g9/g12 /gC /g47 /g30 /gA\n/g9F /g30 /g1E/g9 /g9F /g30/gC3/g12 /gC /g1E /g30/gC3/g12 /gA/g9/g12 /gC /g43/g47 /g30 /gA\n/g1E /g30 /g1E /g1E /g30/gC3/g12 /gC /g44 /g9/g9F /g30/gC3/g12 /gC /g1E /g30/gC3/g12 /gA/g47 /g30\n/g2F /g30 /g1E /g2F /g30/gC3 /g29 /g9/g12 /gC /g45/g47 /g30 /gA/gF\n/g3F/g43 /g1E /g11/gF/g12/g1A/g11/g19 /g3F/g44 /g1E /g11/gF/g11/g14/g1A/g13 /g3F /g45 /g1E /g11/gF/g11/g11/g11/g13\n/g9F /g11 /g1E /g11 /g2F /g11 /g2F /gC3/g12 /g2F /gC3/g13 /g2F /gC3/g14 /gF\n/g44/g45\nautoplot(fit)\n285",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 511,
      "total_chunks": 873
    }
  },
  {
    "text": "Because this model has multiplicative errors, the residuals are not equivalent to the\none-step training errors. The residuals are given by , while the one-step training\nerrors are defined as . We can obtain both using the ıresiduals()ı\nfunction.\nFigure 7.10: Residuals and one-step forecast errors from the ETS(M,A,M) model.\nThe ıtypeı argument is used in the ıresiduals()ı function to distinguish between\nresiduals and forecast errors. The default is ıtype='innovation'ı which gives regular\nresiduals.\nBibliographyBibliography\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). Forecasting with\nexponential smoothing: The state space approach . Berlin: Springer-Verlag.\nhttp://www.exponentialsmoothing.net\ncbind('Residuals' = residuals(fit),\n      'Forecast errors' = residuals(fit,type='response')) %>%\n  autoplot(facet=TRUE) + xlab(\"Year\") + ylab(\"\")\n286",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 512,
      "total_chunks": 873
    }
  },
  {
    "text": "7.77.7  Forecasting with ETS modelsForecasting with ETS models\nPoint forecasts are obtained from the models by iterating the equations for\n and setting all  for .\nFor example, for model ETS(M,A,N),  Therefore\n Similarly,\nTherefore,  and so on. These forecasts are identical to the\nforecasts from Holt’s linear method, and also to those from model ETS(A,A,N). Thus,\nthe point forecasts obtained from the method and from the two models that underlie\nthe method are identical (assuming that the same parameter values are used).\nETS point forecasts are equal to the medians of the forecast distributions. For models\nwith only additive components, the forecast distributions are normal, so the\nmedians and means are equal. For ETS models with multiplicative errors, or with\nmultiplicative seasonality, the point forecasts will not be equal to the means of the\nforecast distributions.\nTo obtain forecasts from an ETS model, we use the ıforecast()ı function.\nfit %>% forecast(h=8) %>%\n  autoplot() +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 513,
      "total_chunks": 873
    }
  },
  {
    "text": "forecast distributions.\nTo obtain forecasts from an ETS model, we use the ıforecast()ı function.\nfit %>% forecast(h=8) %>%\n  autoplot() +\n  ylab(\"International visitor night in Australia (millions)\" )\n287",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 514,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 7.11: Forecasting international visitor nights in Australia using an ETS(M,A,M) model.\nPrediction intervalsPrediction intervals\nA big advantage of the models is that prediction intervals can also be generated —\nsomething that cannot be done using the methods. The prediction intervals will\ndiffer between models with additive and multiplicative methods.\nFor most ETS models, a prediction interval can be written as\nwhere  depends on the coverage probability, and  is the forecast variance. Values\nfor  were given in Table 3.1. For ETS models, formulas for  can be complicated; the\ndetails are given in Chapter 6 of Hyndman et al. ( 2008). In Table 7.8 we give the\nformulas for the additive ETS models, which are the simplest.\n288",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 515,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 7.8: Forecast variance expressions for each additive state space model, where\n is the residual variance,  is the seasonal period, and  is the integer part of\n (i.e., the number of complete years in the forecast period prior to time\n).\nModelModel Forecast variance: Forecast variance:\n(A,N,N)\n(A,A,N)\n(A,A ,N)\nÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\n(A,N,A)\n(A,A,A)\nÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\n(A,A ,A)\nÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\nÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\nÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖÖ\nFor a few ETS models, there are no known formulas for prediction intervals. In these\ncases, the ıforecast()ı function uses simulated future sample paths and computes\nprediction intervals from the percentiles of these simulated future paths.\nUsing Using ĳforecast()ĳ\nThe R code below shows the possible arguments that this function takes when\napplied to an ETS model. We explain each of the arguments in what follows.\nıobjectı\nThe object returned by the ıets()ı function.\nıhı\nforecast(object, h=ifelse(object$ m>1, 2*object$m, 10),",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 516,
      "total_chunks": 873
    }
  },
  {
    "text": "applied to an ETS model. We explain each of the arguments in what follows.\nıobjectı\nThe object returned by the ıets()ı function.\nıhı\nforecast(object, h=ifelse(object$ m>1, 2*object$m, 10),\nlevel=c(80,95), fan=FALSE, simulate=FALSE, bootstrap=FALSE,\nnpaths=5000, PI=TRUE, lambda=object$lambda, biasadj=NULL, ...)\n289",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 517,
      "total_chunks": 873
    }
  },
  {
    "text": "The forecast horizon — the number of periods to be forecast.\nılevelı\nThe confidence level for the prediction intervals.\nıfanı\nIf ıfan=TRUEı, ılevel=seq(50,99,by=1) ı. This is suitable for fan plots.\nısimulateı\nIf ısimulate=TRUEı, prediction intervals are produced by simulation rather than\nusing algebraic formulas. Simulation will also be used (even if ısimulate=FALSEı)\nwhere there are no algebraic formulas available for the particular model.\nıbootstrapı\nIf ıbootstrap=TRUEı and ısimulate=TRUEı, then the simulated prediction intervals\nuse re-sampled errors rather than normally distributed errors.\nınpathsı\nThe number of sample paths used in computing simulated prediction intervals.\nıPIı\nIf ıPI=TRUEı, then prediction intervals are produced; otherwise only point\nforecasts are calculated.\nılambdaı\nThe Box-Cox transformation parameter. This is ignored if ılambda=NULLı.\nOtherwise, the forecasts are back-transformed via an inverse Box-Cox\ntransformation.\nıbiasadjı",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 518,
      "total_chunks": 873
    }
  },
  {
    "text": "ılambdaı\nThe Box-Cox transformation parameter. This is ignored if ılambda=NULLı.\nOtherwise, the forecasts are back-transformed via an inverse Box-Cox\ntransformation.\nıbiasadjı\nIf ılambdaı is not ıNULLı, the back-transformed forecasts (and prediction\nintervals) are bias-adjusted.\nBibliographyBibliography\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). Forecasting with\nexponential smoothing: The state space approach . Berlin: Springer-Verlag.\nhttp://www.exponentialsmoothing.net\n290",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 519,
      "total_chunks": 873
    }
  },
  {
    "text": "7.87.8  ExercisesExercises\n1. Consider the ıpigsı series — the number of pigs slaughtered in Victoria each\nmonth.\na. Use the ıses()ı function in R to find the optimal values of  and , and\ngenerate forecasts for the next four months.\nb. Compute a 95% prediction interval for the first forecast using \nwhere  is the standard deviation of the residuals. Compare your interval\nwith the interval produced by R.\n2. Write your own function to implement simple exponential smoothing. The\nfunction should take arguments ıyı (the time series), ıalphaı (the smoothing\nparameter ) and ılevelı (the initial level ). It should return the forecast of the\nnext observation in the series. Does it give the same forecast as ıses()ı?\n3. Modify your function from the previous exercise to return the sum of squared\nerrors rather than the forecast of the next observation. Then use the ıoptim()ı\nfunction to find the optimal values of  and . Do you get the same values as the\nıses()ı function?",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 520,
      "total_chunks": 873
    }
  },
  {
    "text": "errors rather than the forecast of the next observation. Then use the ıoptim()ı\nfunction to find the optimal values of  and . Do you get the same values as the\nıses()ı function?\n4. Combine your previous two functions to produce a function which both finds the\noptimal values of  and , and produces a forecast of the next observation in the\nseries.\n5. Data set ıbooksı contains the daily sales of paperback and hardcover books at the\nsame store. The task is to forecast the next four days’ sales for paperback and\nhardcover books.\na. Plot the series and discuss the main features of the data.\nb. Use the ıses()ı function to forecast each series, and plot the forecasts.\nc. Compute the RMSE values for the training data in each case.\n6. We will continue with the daily sales of paperback and hardcover books in data\nset ıbooksı.\na. Apply Holt’s linear method to the ıpaperbackı and ıhardbackı series and\ncompute four-day forecasts in each case.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 521,
      "total_chunks": 873
    }
  },
  {
    "text": "set ıbooksı.\na. Apply Holt’s linear method to the ıpaperbackı and ıhardbackı series and\ncompute four-day forecasts in each case.\nb. Compare the RMSE measures of Holt’s method for the two series to those of\nsimple exponential smoothing in the previous question. (Remember that\n291",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 522,
      "total_chunks": 873
    }
  },
  {
    "text": "Holt’s method is using one more parameter than SES.) Discuss the merits of\nthe two forecasting methods for these data sets.\nc. Compare the forecasts for the two series using both methods. Which do you\nthink is best?\nd. Calculate a 95% prediction interval for the first forecast for each series,\nusing the RMSE values and assuming normal errors. Compare your intervals\nwith those produced using ısesı and ıholtı.\n7. For this exercise use data set ıeggsı, the price of a dozen eggs in the United\nStates from 1900–1993. Experiment with the various options in the ıholt()ı\nfunction to see how much the forecasts change with damped trend, or with a\nBox-Cox transformation. Try to develop an intuition of what each argument is\ndoing to the forecasts.\n[Hint: use ıh=100ı when calling ıholt()ı so you can clearly see the differences\nbetween the various options when plotting the forecasts.]\nWhich model gives the best RMSE?\n8. Recall your retail time series data (from Exercise 3 in Section 2.10).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 523,
      "total_chunks": 873
    }
  },
  {
    "text": "between the various options when plotting the forecasts.]\nWhich model gives the best RMSE?\n8. Recall your retail time series data (from Exercise 3 in Section 2.10).\na. Why is multiplicative seasonality necessary for this series?\nb. Apply Holt-Winters’ multiplicative method to the data. Experiment with\nmaking the trend damped.\nc. Compare the RMSE of the one-step forecasts from the two methods. Which\ndo you prefer?\nd. Check that the residuals from the best method look like white noise.\ne. Now find the test set RMSE, while training the model to the end of 2010. Can\nyou beat the seasonal naïve approach from Exercise 8 in Section 3.7 ?\n9. For the same retail data, try an STL decomposition applied to the Box-Cox\ntransformed series, followed by ETS on the seasonally adjusted data. How does\nthat compare with your best previous forecasts on the test set?\n10. For this exercise use data set ıukcarsı, the quarterly UK passenger vehicle\nproduction data from 1977Q1–2005Q1.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 524,
      "total_chunks": 873
    }
  },
  {
    "text": "that compare with your best previous forecasts on the test set?\n10. For this exercise use data set ıukcarsı, the quarterly UK passenger vehicle\nproduction data from 1977Q1–2005Q1.\na. Plot the data and describe the main features of the series.\nb. Decompose the series using STL and obtain the seasonally adjusted data.\nc. Forecast the next two years of the series using an additive damped trend\nmethod applied to the seasonally adjusted data. (This can be done in one step\nusing ıstlf()ı with arguments ıetsmodel=\"AAN\", damped=TRUE ı.)\n292",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 525,
      "total_chunks": 873
    }
  },
  {
    "text": "d. Forecast the next two years of the series using Holt’s linear method applied\nto the seasonally adjusted data (as before but with ıdamped=FALSEı).\ne. Now use ıets()ı to choose a seasonal model for the data.\nf. Compare the RMSE of the ETS model with the RMSE of the models you\nobtained using STL decompositions. Which gives the better in-sample fits?\ng. Compare the forecasts from the three approaches? Which seems most\nreasonable?\nh. Check the residuals of your preferred model.\n11. For this exercise use data set ıvisitorsı, the monthly Australian short-term\noverseas visitors data, May 1985–April 2005.\na. Make a time plot of your data and describe the main features of the series.\nb. Split your data into a training set and a test set comprising the last two years\nof available data. Forecast the test set using Holt-Winters’ multiplicative\nmethod.\nc. Why is multiplicative seasonality necessary here?\nd. Forecast the two-year test set using each of the following methods:\ni. an ETS model;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 526,
      "total_chunks": 873
    }
  },
  {
    "text": "method.\nc. Why is multiplicative seasonality necessary here?\nd. Forecast the two-year test set using each of the following methods:\ni. an ETS model;\nii. an additive ETS model applied to a Box-Cox transformed series;\niii. a seasonal naïve method;\niv. an STL decomposition applied to the Box-Cox transformed data followed\nby an ETS model applied to the seasonally adjusted (transformed) data.\ne. Which method gives the best forecasts? Does it pass the residual tests?\nf. Compare the same four methods using time series cross-validation with the\nıtsCV()ı function instead of using a training and test set. Do you come to the\nsame conclusions?\n12. The ıfets()ı function below returns ETS forecasts.\na. Apply ıtsCV()ı for a forecast horizon of , for both ETS and seasonal\nnaïve methods to the ıqcementı data, (Hint: use the newly created ıfets()ı\nand the existing ısnaive()ı functions as your forecast function arguments.)\nb. Compute the MSE of the resulting -step-ahead errors. (Hint: make sure you",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 527,
      "total_chunks": 873
    }
  },
  {
    "text": "and the existing ısnaive()ı functions as your forecast function arguments.)\nb. Compute the MSE of the resulting -step-ahead errors. (Hint: make sure you\nremove missing values.) Why are there missing values? Comment on which\nforecasts are more accurate. Is this what you expected?\nfets <- function(y, h) {\n  forecast(ets(y), h = h)\n}\n/g24 /g1E/g15\n/g15\n293",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 528,
      "total_chunks": 873
    }
  },
  {
    "text": "13. Compare ıets()ı, ısnaive()ı and ıstlf()ı on the following six time series. For\nıstlf()ı, you might need to use a Box-Cox transformation. Use a test set of three\nyears to decide what gives the best forecasts. ıausbeerı, ıbricksqı, ıdoleı, ıa10ı,\nıh02ı, ıusmelecı.\n14. a. Use ıets()ı on the following series: ıbicoalı, ıchickenı, ıdoleı, ıusdeathsı,\nılynxı, ıibmcloseı, ıeggsı. Does it always give good forecasts?\nb. Find an example where it does not work well. Can you figure out why?\n15. Show that the point forecasts from an ETS(M,A,M) model are the same as those\nobtained using Holt-Winters’ multiplicative method.\n16. Show that the forecast variance for an ETS(A,N,N) model is given by\n17. Write down 95% prediction intervals for an ETS(A,N,N) model as a function of \n, ,  and , assuming normally distributed errors.\n/g55 /g13 /g5/g12/gC/g43 /g13 /g9/g24 /gC3/g12 /gA /g7 /gF\n/g9F /g16\n/g43/g24 /g55\n294",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 529,
      "total_chunks": 873
    }
  },
  {
    "text": "7.97.9  Further readingFurther reading\nTwo articles by Ev Gardner ( Gardner, 1985 , 2006) provide a great overview of the\nhistory of exponential smoothing, and its many variations.\nA full book treatment of the subject providing the mathematical details is given\nby Hyndman et al. ( 2008).\nBibliography\nGardner, E. S. (1985). Exponential smoothing: The state of the art. Journal of\nForecasting , 4(1), 1–28. [DOI]\nGardner, E. S. (2006). Exponential smoothing: The state of the art — Part II.\nInternational Journal of Forecasting , 22, 637–666. [DOI]\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). Forecasting with\nexponential smoothing: The state space approach . Berlin: Springer-Verlag.\nhttp://www.exponentialsmoothing.net\n295",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 530,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 8Chapter 8  ARIMA modelsARIMA models\nARIMA models provide another approach to time series forecasting. Exponential\nsmoothing and ARIMA models are the two most widely used approaches to time\nseries forecasting, and provide complementary approaches to the problem. While\nexponential smoothing models are based on a description of the trend and\nseasonality in the data, ARIMA models aim to describe the autocorrelations in the\ndata.\nBefore we introduce ARIMA models, we must first discuss the concept of stationarity\nand the technique of differencing time series.\n296",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 531,
      "total_chunks": 873
    }
  },
  {
    "text": "8.18.1  Stationarity and differencingStationarity and differencing\nA stationary time series is one whose properties do not depend on the time at which\nthe series is observed.  Thus, time series with trends, or with seasonality, are not\nstationary — the trend and seasonality will affect the value of the time series at\ndifferent times. On the other hand, a white noise series is stationary — it does not\nmatter when you observe it, it should look much the same at any point in time.\nSome cases can be confusing — a time series with cyclic behaviour (but with no\ntrend or seasonality) is stationary. This is because the cycles are not of a fixed\nlength, so before we observe the series we cannot be sure where the peaks and\ntroughs of the cycles will be.\nIn general, a stationary time series will have no predictable patterns in the long-\nterm. Time plots will show the series to be roughly horizontal (although some cyclic\nbehaviour is possible), with constant variance.\n15\n297",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 532,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.1: Which of these series are stationary? (a) Google stock price for 200 consecutive\ndays; (b) Daily change in the Google stock price for 200 consecutive days; (c) Annual\nnumber of strikes in the US; (d) Monthly sales of new one-family houses sold in the US;\n(e) Annual price of a dozen eggs in the US (constant dollars); (f) Monthly total of pigs\nslaughtered in Victoria, Australia; (g) Annual total of lynx trapped in the McKenzie River\ndistrict of north-west Canada; (h) Monthly Australian beer production; (i) Monthly\nAustralian electricity production.\nConsider the nine series plotted in Figure 8.1. Which of these do you think are\nstationary?\nObvious seasonality rules out series (d), (h) and (i). Trends and changing levels rules\nout series (a), (c), (e), (f) and (i). Increasing variance also rules out (i). That leaves\nonly (b) and (g) as stationary series.\nAt first glance, the strong cycles in series (g) might appear to make it non-",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 533,
      "total_chunks": 873
    }
  },
  {
    "text": "only (b) and (g) as stationary series.\nAt first glance, the strong cycles in series (g) might appear to make it non-\nstationary. But these cycles are aperiodic — they are caused when the lynx\npopulation becomes too large for the available feed, so that they stop breeding and\nthe population falls to low numbers, then the regeneration of their food sources\nallows the population to grow again, and so on. In the long-term, the timing of these\ncycles is not predictable. Hence the series is stationary.\nDifferencingDifferencing\nIn Figure 8.1, note that the Google stock price was non-stationary in panel (a), but\nthe daily changes were stationary in panel (b). This shows one way to make a non-\nstationary time series stationary — compute the differences between consecutive\nobservations. This is known as differencingdifferencing .\nTransformations such as logarithms can help to stabilise the variance of a time\nseries. Differencing can help stabilise the mean of a time series by removing changes",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 534,
      "total_chunks": 873
    }
  },
  {
    "text": "Transformations such as logarithms can help to stabilise the variance of a time\nseries. Differencing can help stabilise the mean of a time series by removing changes\nin the level of a time series, and therefore eliminating (or reducing) trend and\nseasonality.\nAs well as looking at the time plot of the data, the ACF plot is also useful for\nidentifying non-stationary time series. For a stationary time series, the ACF will\ndrop to zero relatively quickly, while the ACF of non-stationary data decreases\nslowly. Also, for non-stationary data, the value of  is often large and positive.\n298",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 535,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.2: The ACF of the Google stock price (left) and of the daily changes in Google\nstock price (right).\nThe ACF of the differenced Google stock price looks just like that of a white noise\nseries. There are no autocorrelations lying outside the 95% limits, and the Ljung-\nBox  statistic has a p-value of 0.355 (for ). This suggests that the daily\nchange in the Google stock price is essentially a random amount which is\nuncorrelated with that of previous days.\nRandom walk modelRandom walk model\nThe differenced series is the change between consecutive observations in the\noriginal series, and can be written as\nThe differenced series will have only  values, since it is not possible to\ncalculate a difference  for the first observation.\nWhen the differenced series is white noise, the model for the original series can be\nwritten as\nwhere  denotes white noise. Rearranging this leads to the “random walk” model\nBox.test(diff(goog200), lag=10, type=\"Ljung-Box\")\n#> \n#>  Box-Ljung test\n#>",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 536,
      "total_chunks": 873
    }
  },
  {
    "text": "written as\nwhere  denotes white noise. Rearranging this leads to the “random walk” model\nBox.test(diff(goog200), lag=10, type=\"Ljung-Box\")\n#> \n#>  Box-Ljung test\n#> \n#> data:  diff(goog200)\n#> X-squared = 11, df = 10, p-value = 0.4\n299",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 537,
      "total_chunks": 873
    }
  },
  {
    "text": "Random walk models are widely used for non-stationary data, particularly financial\nand economic data. Random walks typically have:\nlong periods of apparent trends up or down\nsudden and unpredictable changes in direction.\nThe forecasts from a random walk model are equal to the last observation, as future\nmovements are unpredictable, and are equally likely to be up or down. Thus, the\nrandom walk model underpins naïve forecasts, first introduced in Section 3.1.\nA closely related model allows the differences to have a non-zero mean. Then\nThe value of  is the average of the changes between consecutive observations. If  is\npositive, then the average change is an increase in the value of . Thus,  will tend\nto drift upwards. However, if  is negative,  will tend to drift downwards.\nThis is the model behind the drift method, also discussed in Section 3.1.\nSecond-order differencingSecond-order differencing\nOccasionally the differenced data will not appear to be stationary and it may be",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 538,
      "total_chunks": 873
    }
  },
  {
    "text": "Second-order differencingSecond-order differencing\nOccasionally the differenced data will not appear to be stationary and it may be\nnecessary to difference the data a second time to obtain a stationary series:\nIn this case,  will have  values. Then, we would model the “change in the\nchanges” of the original data. In practice, it is almost never necessary to go beyond\nsecond-order differences.\nSeasonal differencingSeasonal differencing\nA seasonal difference is the difference between an observation and the previous\nobservation from the same season. So\n300",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 539,
      "total_chunks": 873
    }
  },
  {
    "text": "where  the number of seasons. These are also called “lag-  differences”, as we\nsubtract the observation after a lag of  periods.\nIf seasonally differenced data appear to be white noise, then an appropriate model\nfor the original data is\nForecasts from this model are equal to the last observation from the relevant season.\nThat is, this model gives seasonal naïve forecasts, introduced in Section 3.1.\nThe bottom panel in Figure 8.3 shows the seasonal differences of the logarithm of\nthe monthly scripts for A10 (antidiabetic) drugs sold in Australia. The\ntransformation and differencing have made the series look relatively stationary.\n/g29 /g1E /g29\n/g29\n/g35 /g30 /g1E /g35 /g30/gC3 /g29 /gC /g47 /g30 /gF\ncbind(\"Sales ($million)\" = a10,\n      \"Monthly log sales\" = log(a10),\n      \"Annual change in log sales\" = diff(log(a10),12)) %>%\n  autoplot(facets=TRUE) +\n    xlab(\"Year\") + ylab(\"\") +\n    ggtitle(\"Antidiabetic drug sales\" )\n301",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 540,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.3: Logs and seasonal differences of the A10 (antidiabetic) sales data. The\nlogarithms stabilise the variance, while the seasonal differences remove the seasonality\nand trend.\nTo distinguish seasonal differences from ordinary differences, we sometimes refer\nto ordinary differences as “first differences”, meaning differences at lag 1.\nSometimes it is necessary to take both a seasonal difference and a first difference to\nobtain stationary data, as is shown in Figure 8.4 . Here, the data are first transformed\nusing logarithms (second panel), then seasonal differences are calculated (third\npanel). The data still seem somewhat non-stationary, and so a further lot of first\ndifferences are computed (bottom panel).\n302",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 541,
      "total_chunks": 873
    }
  },
  {
    "text": "cbind(\"Billion kWh\" = usmelec,\n      \"Logs\" = log(usmelec),\n      \"Seasonally\\n differenced logs\" =\n        diff(log(usmelec),12),\n      \"Doubly\\n differenced logs\" =\n        diff(diff(log(usmelec),12),1)) %>%\n  autoplot(facets=TRUE) +\n    xlab(\"Year\") + ylab(\"\") +\n    ggtitle(\"Monthly US net electricity generation\" )\n303\n\n\nFigure 8.4: Top panel: US net electricity generation (billion kWh). Other panels show the\nsame data after transforming and differencing.\nThere is a degree of subjectivity in selecting which differences to apply. The\nseasonally differenced data in Figure 8.3 do not show substantially different\nbehaviour from the seasonally differenced data in Figure 8.4. In the latter case, we\ncould have decided to stop with the seasonally differenced data, and not done an\nextra round of differencing. In the former case, we could have decided that the data\n304",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 542,
      "total_chunks": 873
    }
  },
  {
    "text": "were not sufficiently stationary and taken an extra round of differencing. Some\nformal tests for differencing are discussed below, but there are always some choices\nto be made in the modelling process, and different analysts may make different\nchoices.\nIf  denotes a seasonally differenced series, then the twice-differenced\nseries is\nWhen both seasonal and first differences are applied, it makes no difference which is\ndone first—the result will be the same. However, if the data have a strong seasonal\npattern, we recommend that seasonal differencing be done first, because the\nresulting series will sometimes be stationary and there will be no need for a further\nfirst difference. If first differencing is done first, there will still be seasonality\npresent.\nIt is important that if differencing is used, the differences are interpretable. First\ndifferences are the change between one observation and the next. Seasonal",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 543,
      "total_chunks": 873
    }
  },
  {
    "text": "present.\nIt is important that if differencing is used, the differences are interpretable. First\ndifferences are the change between one observation and the next. Seasonal\ndifferences are the change between one year to the next. Other lags are unlikely to\nmake much interpretable sense and should be avoided.\nUnit root testsUnit root tests\nOne way to determine more objectively whether differencing is required is to use a\nunit root test . These are statistical hypothesis tests of stationarity that are designed\nfor determining whether differencing is required.\nA number of unit root tests are available, which are based on different assumptions\nand may lead to conflicting answers. In our analysis, we use the Kwiatkowski-\nPhillips-Schmidt-Shin (KPSS) test  (Kwiatkowski, Phillips, Schmidt, & Shin, 1992 ).\nIn this test, the null hypothesis is that the data are stationary, and we look for\nevidence that the null hypothesis is false. Consequently, small p-values (e.g., less",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 544,
      "total_chunks": 873
    }
  },
  {
    "text": "In this test, the null hypothesis is that the data are stationary, and we look for\nevidence that the null hypothesis is false. Consequently, small p-values (e.g., less\nthan 0.05) suggest that differencing is required. The test can be computed using the\nıur.kpss()ı function from the urcaurca package.\nFor example, let us apply it to the Google stock price data.\n305",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 545,
      "total_chunks": 873
    }
  },
  {
    "text": "The test statistic is much bigger than the 1% critical value, indicating that the null\nhypothesis is rejected. That is, the data are not stationary. We can difference the\ndata, and apply the test again.\nThis time, the test statistic is tiny, and well within the range we would expect for\nstationary data. So we can conclude that the differenced data are stationary.\nThis process of using a sequence of KPSS tests to determine the appropriate number\nof first differences is carried out by the function ındiffs()ı.\nlibrary(urca)\ngoog %>% ur.kpss() %>% summary()\n#> \n#> ####################### \n#> # KPSS Unit Root Test # \n#> ####################### \n#> \n#> Test is of type: mu with 7 lags. \n#> \n#> Value of test-statistic is: 10.72 \n#> \n#> Critical value for a significance level of: \n#>                 10pct  5pct 2.5pct  1pct\n#> critical values 0.347 0.463  0.574 0.739\ngoog %>% diff() %>% ur.kpss() %>% summary()\n#> \n#> ####################### \n#> # KPSS Unit Root Test #",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 546,
      "total_chunks": 873
    }
  },
  {
    "text": "#>                 10pct  5pct 2.5pct  1pct\n#> critical values 0.347 0.463  0.574 0.739\ngoog %>% diff() %>% ur.kpss() %>% summary()\n#> \n#> ####################### \n#> # KPSS Unit Root Test # \n#> ####################### \n#> \n#> Test is of type: mu with 7 lags. \n#> \n#> Value of test-statistic is: 0.0324 \n#> \n#> Critical value for a significance level of: \n#>                 10pct  5pct 2.5pct  1pct\n#> critical values 0.347 0.463  0.574 0.739\n306",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 547,
      "total_chunks": 873
    }
  },
  {
    "text": "As we saw from the KPSS tests above, one difference is required to make the ıgoogı\ndata stationary.\nA similar function for determining whether seasonal differencing is required is\nınsdiffs()ı, which uses the measure of seasonal strength introduced in Section 6.7\nto determine the appropriate number of seasonal differences required. No seasonal\ndifferences are suggested if , otherwise one seasonal difference is\nsuggested.\nWe can apply ınsdiffs()ı to the logged US monthly electricity data.\nBecause ınsdiffs()ı returns 1 (indicating one seasonal difference is required), we\napply the ındiffs()ı function to the seasonally differenced data. These functions\nsuggest we should do both a seasonal difference and a first difference.\nBibliographyBibliography\nKwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the null\nhypothesis of stationarity against the alternative of a unit root: How sure are",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 548,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nKwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the null\nhypothesis of stationarity against the alternative of a unit root: How sure are\nwe that economic time series have a unit root? Journal of Econometrics , 54(1-\n3), 159–178. [DOI]\n15. More precisely, if  is a stationarystationary  time series, then for all , the distribution\nof  does not depend on . ↩ \nndiffs(goog)\n#> [1] 1\nusmelec %>% log() %>% nsdiffs()\n#> [1] 1\nusmelec %>% log() %>% diff(lag=12) %>% ndiffs()\n#> [1] 1\n307",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 549,
      "total_chunks": 873
    }
  },
  {
    "text": "8.28.2  Backshift notationBackshift notation\nThe backward shift operator  is a useful notational device when working with time\nseries lags:\n(Some references use  for “lag” instead of  for “backshift”.) In other words, ,\noperating on , has the effect of shifting the data back one period. Two applications\nof  to  shifts the data back two periods:\nFor monthly data, if we wish to consider “the same month last year,” the notation is\n = .\nThe backward shift operator is convenient for describing the process of differencing .\nA first difference can be written as\nNote that a first difference is represented by . Similarly, if second-order\ndifferences have to be computed, then:\nIn general, a th-order difference can be written as\nBackshift notation is particularly useful when combining differences, as the operator\ncan be treated using ordinary algebraic rules. In particular, terms involving  can be\nmultiplied together.\n308",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 550,
      "total_chunks": 873
    }
  },
  {
    "text": "For example, a seasonal difference followed by a first difference can be written as\nthe same result we obtained earlier.\n/g9/g12 /gC3 /g4/gA/g9/g12 /gC3 /g4 /g29 /gA/g35 /g30 /g1E/g9 /g12 /gC3 /g4 /gC3 /g4 /g29 /gC /g4 /g29/gC/g12 /gA/g35 /g30\n/g1E /g35 /g30 /gC3 /g35 /g30/gC3/g12 /gC3 /g35 /g30/gC3 /g29 /gC /g35 /g30/gC3 /g29/gC3/g12 /gD\n309",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 551,
      "total_chunks": 873
    }
  },
  {
    "text": "8.38.3  Autoregressive modelsAutoregressive models\nIn a multiple regression model, we forecast the variable of interest using a linear\ncombination of predictors. In an autoregression model, we forecast the variable of\ninterest using a linear combination of past values of the variable . The term\nautoregression indicates that it is a regression of the variable against itself.\nThus, an autoregressive model of order  can be written as\nwhere  is white noise. This is like a multiple regression but with lagged values  of \nas predictors. We refer to this as an AR(AR( ) model) model, an autoregressive model of order .\nAutoregressive models are remarkably flexible at handling a wide range of different\ntime series patterns. The two series in Figure 8.5 show series from an AR(1) model\nand an AR(2) model. Changing the parameters  results in different time\nseries patterns. The variance of the error term  will only change the scale of the\nseries, not the patterns.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 552,
      "total_chunks": 873
    }
  },
  {
    "text": "and an AR(2) model. Changing the parameters  results in different time\nseries patterns. The variance of the error term  will only change the scale of the\nseries, not the patterns.\nFigure 8.5: Two examples of data from autoregressive models with different parameters.\nLeft: AR(1) with . Right: AR(2) with\n. In both cases,  is normally distributed white noise\nwith mean zero and variance one.\nFor an AR(1) model:\nhen is equi alent to hite noise\n310",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 553,
      "total_chunks": 873
    }
  },
  {
    "text": "when  and ,  is equivalent to a random walk;\nwhen  and ,  is equivalent to a random walk with drift;\nwhen ,  tends to oscillate around the mean.\nWe normally restrict autoregressive models to stationary data, in which case some\nconstraints on the values of the parameters are required.\nFor an AR(1) model: .\nFor an AR(2) model: , , .\nWhen , the restrictions are much more complicated. R takes care of these\nrestrictions when estimating a model.\n/g5D /g12 /g1E/g11 /g35 /g30/g5D /g12 /g1E/g12 /g1F /g1E/g11 /g35 /g30\n/g5D /g12 /g1E/g12 /g1F /gDC/g11 /g35 /g30\n/g5D /g12 /g1D/g11 /g35 /g30\n/gC3/g12 /g1D /g5D /g12 /g1D/g12\n/gC3/g12 /g1D /g5D /g13 /g1D/g12 /g5D /g12 /gC /g5D /g13 /g1D/g12 /g5D /g13 /gC3 /g5D /g12 /g1D/g12\n/g2C /gDF/g14\n311",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 554,
      "total_chunks": 873
    }
  },
  {
    "text": "8.48.4  Moving average modelsMoving average models\nRather than using past values of the forecast variable in a regression, a moving\naverage model uses past forecast errors in a regression-like model.\nwhere  is white noise. We refer to this as an MA(MA( ) model) model, a moving average model\nof order . Of course, we do not observe the values of , so it is not really a\nregression in the usual sense.\nNotice that each value of  can be thought of as a weighted moving average of the\npast few forecast errors. However, moving average models should not be confused\nwith the moving average smoothing we discussed in Chapter 6. A moving average\nmodel is used for forecasting future values, while moving average smoothing is used\nfor estimating the trend-cycle of past values.\nFigure 8.6: Two examples of data from moving average models with different parameters.\nLeft: MA(1) with . Right: MA(2) with . In\nboth cases,  is normally distributed white noise with mean zero and variance one.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 555,
      "total_chunks": 873
    }
  },
  {
    "text": "Left: MA(1) with . Right: MA(2) with . In\nboth cases,  is normally distributed white noise with mean zero and variance one.\nFigure 8.6 shows some data from an MA(1) model and an MA(2) model. Changing the\nparameters  results in different time series patterns. As with\nautoregressive models, the variance of the error term  will only change the scale of\nthe series, not the patterns.\n312",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 556,
      "total_chunks": 873
    }
  },
  {
    "text": "It is possible to write any stationary AR( ) model as an MA( ) model. For example,\nusing repeated substitution, we can demonstrate this for an AR(1) model:\nProvided , the value of  will get smaller as  gets larger. So eventually\nwe obtain\nan MA( ) process.\nThe reverse result holds if we impose some constraints on the MA parameters. Then\nthe MA model is called invertibleinvertible . That is, we can write any invertible MA( ) process\nas an AR( ) process. Invertible models are not simply introduced to enable us to\nconvert from MA models to AR models. They also have some desirable mathematical\nproperties.\nFor example, consider the MA(1) process, . In its AR( )\nrepresentation, the most recent error can be written as a linear function of current\nand past observations:\nWhen , the weights increase as lags increase, so the more distant the\nobservations the greater their influence on the current error. When , the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 557,
      "total_chunks": 873
    }
  },
  {
    "text": "and past observations:\nWhen , the weights increase as lags increase, so the more distant the\nobservations the greater their influence on the current error. When , the\nweights are constant in size, and the distant observations have the same influence as\nthe recent observations. As neither of these situations make much sense, we require\n, so the most recent observations have higher weight than observations from\nthe more distant past. Thus, the process is invertible when .\nThe invertibility constraints for other models are similar to the stationarity\nconstraints.\nFor an MA(1) model: .\nFor an MA(2) model:   .\n313",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 558,
      "total_chunks": 873
    }
  },
  {
    "text": "More complicated conditions hold for . Again, R will take care of these\nconstraints when estimating the models.\n/g2D /gDF/g14\n314",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 559,
      "total_chunks": 873
    }
  },
  {
    "text": "8.58.5  Non-seasonal ARIMA modelsNon-seasonal ARIMA models\nIf we combine differencing with autoregression and a moving average model, we obtain a\nnon-seasonal ARIMA model. ARIMA is an acronym for AutoRegressive Integrated Moving\nAverage (in this context, “integration” is the reverse of differencing). The full model can\nbe written as\nwhere  is the differenced series (it may have been differenced more than once). The\n“predictors” on the right hand side include both lagged values of  and lagged errors. We\ncall this an ARIMA(ARIMA( ) model) model, where\norder of the autoregressive part;\ndegree of first differencing involved;\norder of the moving average part.\nThe same stationarity and invertibility conditions that are used for autoregressive and\nmoving average models also apply to an ARIMA model.\nMany of the models we have already discussed are special cases of the ARIMA model, as\nshown in Table 8.1.\nTable 8.1: Special cases of ARIMA models.\nWhite noise ARIMA(0,0,0)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 560,
      "total_chunks": 873
    }
  },
  {
    "text": "Many of the models we have already discussed are special cases of the ARIMA model, as\nshown in Table 8.1.\nTable 8.1: Special cases of ARIMA models.\nWhite noise ARIMA(0,0,0)\nRandom walk ARIMA(0,1,0) with no constant\nRandom walk with drift ARIMA(0,1,0) with a constant\nAutoregression ARIMA( ,0,0)\nMoving average ARIMA(0,0, )\nOnce we start combining components in this way to form more complicated models, it is\nmuch easier to work with the backshift notation. For example, Equation (8.1) can be\nwritten in backshift notation as\n315",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 561,
      "total_chunks": 873
    }
  },
  {
    "text": "R uses a slightly different parameterisation:\nwhere  and  is the mean of . To convert to the form given by (8.2), set\n.\nSelecting appropriate values for ,  and  can be difficult. However, the ıauto.arima()ı\nfunction in R will do it for you automatically. In Section 8.7, we will learn how this\nfunction works, along with some methods for choosing these values yourself.\nUS consumption expenditureUS consumption expenditure\nFigure 8.7  shows quarterly percentage changes in US consumption expenditure. Although\nit is a quarterly series, there does not appear to be a seasonal pattern, so we will fit a non-\nseasonal ARIMA model.\nFigure 8.7: Quarterly percentage change in US consumption expenditure.\nThe following R code was used to select a model automatically.\nautoplot(uschange[,\"Consumption\"]) +\n  xlab(\"Year\") + ylab(\"Quarterly percentage change\" )\nfit <- auto.arima(uschange[, \"Consumption\"], seasonal=FALSE )\n316",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 562,
      "total_chunks": 873
    }
  },
  {
    "text": "#> Series: uschange[, \"Consumption\"] \n#> ARIMA(1,0,3) with non-zero mean \n#> \n#> Coefficients:\n#>         ar1     ma1    ma2    ma3   mean\n#>       0.589  -0.353  0.085  0.174  0.745\n#> s.e.  0.154   0.166  0.082  0.084  0.093\n#> \n#> sigma^2 = 0.35:  log likelihood = -164.8\n#> AIC=341.6   AICc=342.1   BIC=361\nThis is an ARIMA(1,0,3) model:\nwhere  and  is white noise with a standard deviation of\n. Forecasts from the model are shown in Figure 8.8.\nFigure 8.8: Forecasts of quarterly percentage changes in US consumption expenditure.\nUnderstanding ARIMA modelsUnderstanding ARIMA models\nThe ıauto.arima()ı function is useful, but anything automated can be a little dangerous,\nand it is worth understanding something of the behaviour of the models even when you\nrely on an automatic procedure to choose the model for you.\nfit %>% forecast(h=10) %>% autoplot(include=80)\n317",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 563,
      "total_chunks": 873
    }
  },
  {
    "text": "The constant  has an important effect on the long-term forecasts obtained from these\nmodels.\nIf  and , the long-term forecasts will go to zero.\nIf  and , the long-term forecasts will go to a non-zero constant.\nIf  and , the long-term forecasts will follow a straight line.\nIf  and , the long-term forecasts will go to the mean of the data.\nIf  and , the long-term forecasts will follow a straight line.\nIf  and , the long-term forecasts will follow a quadratic trend.\nThe value of  also has an effect on the prediction intervals — the higher the value of ,\nthe more rapidly the prediction intervals increase in size. For , the long-term\nforecast standard deviation will go to the standard deviation of the historical data, so the\nprediction intervals will all be essentially the same.\nThis behaviour is seen in Figure 8.8 where  and . In this figure, the prediction\nintervals are almost the same for the last few forecast horizons, and the point forecasts\nare equal to the mean of the data.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 564,
      "total_chunks": 873
    }
  },
  {
    "text": "intervals are almost the same for the last few forecast horizons, and the point forecasts\nare equal to the mean of the data.\nThe value of  is important if the data show cycles. To obtain cyclic forecasts, it is\nnecessary to have , along with some additional conditions on the parameters. For an\nAR(2) model, cyclic behaviour occurs if . In that case, the average period of\nthe cycles is \nACF and PACF plotsACF and PACF plots\nIt is usually not possible to tell, simply from a time plot, what values of  and  are\nappropriate for the data. However, it is sometimes possible to use the ACF plot, and the\nclosely related PACF plot, to determine appropriate values for  and .\nRecall that an ACF plot shows the autocorrelations which measure the relationship\nbetween  and  for different values of . Now if  and  are correlated, then \nand  must also be correlated. However, then  and  might be correlated, simply\nbecause they are both connected to , rather than because of any new information",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 565,
      "total_chunks": 873
    }
  },
  {
    "text": "and  must also be correlated. However, then  and  might be correlated, simply\nbecause they are both connected to , rather than because of any new information\ncontained in  that could be used in forecasting .\nTo overcome this problem, we can use partial autocorrelationspartial autocorrelations . These measure the\nrelationship between  and  after removing the effects of lags . So\nthe first partial autocorrelation is identical to the first autocorrelation, because there is\nnothing between them to remove. Each partial autocorrelation can be estimated as the last\n16\n318",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 566,
      "total_chunks": 873
    }
  },
  {
    "text": "coefficient in an autoregressive model. Specifically, , the th partial autocorrelation\ncoefficient, is equal to the estimate of  in an AR( ) model. In practice, there are more\nefficient algorithms for computing  than fitting all of these autoregressions, but they\ngive the same results.\nFigures 8.9 and 8.10 shows the ACF and PACF plots for the US consumption data shown in\nFigure 8.7 . The partial autocorrelations have the same critical values of  as for\nordinary autocorrelations, and these are typically shown on the plot as in Figure 8.9.\nFigure 8.9: ACF of quarterly percentage change in US consumption.\nFigure 8.10: PACF of quarterly percentage change in US consumption.\nIf the data are from an ARIMA( , ,0) or ARIMA(0, , ) model, then the ACF and PACF plots\ncan be helpful in determining the value of  or .  If  and  are both positive, then the\nplots do not help in finding suitable values of  and .\nThe data may follow an ARIMA( , ,0) model if the ACF and PACF plots of the differenced",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 567,
      "total_chunks": 873
    }
  },
  {
    "text": "plots do not help in finding suitable values of  and .\nThe data may follow an ARIMA( , ,0) model if the ACF and PACF plots of the differenced\ndata show the following patterns:\nthe ACF is exponentially decaying or sinusoidal;\n/g43 /g27 /g27\n/g5D /g27 /g27\n/g43 /g27\n/g65/g12/gF/g1A/g17/g10 /gCA /g16\nggAcf(uschange[,\"Consumption\"])\nggPacf(uschange[,\"Consumption\"])\n/g2C/g20 /g20/g2D\n/g2C/g2D 17 /g2C/g2D\n/g2C/g2D\n/g2C/g20\n319",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 568,
      "total_chunks": 873
    }
  },
  {
    "text": "there is a significant spike at lag  in the PACF, but none beyond lag .\nThe data may follow an ARIMA(0, , ) model if the ACF and PACF plots of the differenced\ndata show the following patterns:\nthe PACF is exponentially decaying or sinusoidal;\nthere is a significant spike at lag  in the ACF, but none beyond lag .\nIn Figure 8.9, we see that there are three spikes in the ACF, followed by an almost\nsignificant spike at lag 4. In the PACF, there are three significant spikes, and then no\nsignificant spikes thereafter (apart from one just outside the bounds at lag 22). We can\nignore one significant spike in each plot if it is just outside the limits, and not in the first\nfew lags. After all, the probability of a spike being significant by chance is about one in\ntwenty, and we are plotting 22 spikes in each plot. The pattern in the first three spikes is\nwhat we would expect from an ARIMA(3,0,0), as the PACF tends to decrease. So in this",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 569,
      "total_chunks": 873
    }
  },
  {
    "text": "twenty, and we are plotting 22 spikes in each plot. The pattern in the first three spikes is\nwhat we would expect from an ARIMA(3,0,0), as the PACF tends to decrease. So in this\ncase, the ACF and PACF lead us to think an ARIMA(3,0,0) model might be appropriate.\nThis model is actually slightly better than the model identified by ıauto.arima()ı (with an\nAICc value of 340.67 compared to 342.08). The ıauto.arima()ı function did not find this\nmodel because it does not consider all possible models in its search. You can make it work\nharder by using the arguments ıstepwise=FALSEı and ıapproximation=FALSEı:\n/g2C/g2C\n/g20/g2D\n/g2D/g2D\n(fit2 <- Arima(uschange[,\"Consumption\"], order=c (3,0,0)))\n#> Series: uschange[, \"Consumption\"] \n#> ARIMA(3,0,0) with non-zero mean \n#> \n#> Coefficients:\n#>         ar1    ar2    ar3   mean\n#>       0.227  0.160  0.203  0.745\n#> s.e.  0.071  0.072  0.071  0.103\n#> \n#> sigma^2 = 0.349:  log likelihood = -165.2\n#> AIC=340.3   AICc=340.7   BIC=356.5\n320",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 570,
      "total_chunks": 873
    }
  },
  {
    "text": "We also use the argument ıseasonal=FALSEı to prevent it searching for seasonal ARIMA\nmodels; we will consider these models in Section 8.9.\nThis time, ıauto.arima()ı has found the same model that we guessed from the ACF and\nPACF plots. The forecasts from this ARIMA(3,0,0) model are almost identical to those\nshown in Figure 8.8 for the ARIMA(1,0,3) model, so we do not produce the plot here.\n16. arc cos is the inverse cosine function. You should be able to find it on your calculator.\nIt may be labelled acos or cos . ↩ \n17. A convenient way to produce a time plot, ACF plot and PACF plot in one command is to\nuse the ıggtsdisplay()ı function.↩ \n(fit3 <- auto.arima(uschange[, \"Consumption\"], seasonal=FALSE ,\n  stepwise=FALSE, approximation=FALSE ))\n#> Series: uschange[, \"Consumption\"] \n#> ARIMA(3,0,0) with non-zero mean \n#> \n#> Coefficients:\n#>         ar1    ar2    ar3   mean\n#>       0.227  0.160  0.203  0.745\n#> s.e.  0.071  0.072  0.071  0.103\n#>",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 571,
      "total_chunks": 873
    }
  },
  {
    "text": "#> ARIMA(3,0,0) with non-zero mean \n#> \n#> Coefficients:\n#>         ar1    ar2    ar3   mean\n#>       0.227  0.160  0.203  0.745\n#> s.e.  0.071  0.072  0.071  0.103\n#> \n#> sigma^2 = 0.349:  log likelihood = -165.2\n#> AIC=340.3   AICc=340.7   BIC=356.5\n/gC3/g12\n321",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 572,
      "total_chunks": 873
    }
  },
  {
    "text": "8.68.6  Estimation and order selectionEstimation and order selection\nMaximum likelihood estimationMaximum likelihood estimation\nOnce the model order has been identified (i.e., the values of ,  and ), we need to\nestimate the parameters , , . When R estimates the ARIMA\nmodel, it uses maximum likelihood estimation  (MLE). This technique finds the\nvalues of the parameters which maximise the probability of obtaining the data that\nwe have observed. For ARIMA models, MLE is similar to the least squares  estimates\nthat would be obtained by minimising\n(For the regression models considered in Chapter 5 , MLE gives exactly the same\nparameter estimates as least squares estimation.) Note that ARIMA models are much\nmore complicated to estimate than regression models, and different software will\ngive slightly different answers as they use different methods of estimation, and\ndifferent optimisation algorithms.\nIn practice, R will report the value of the log likelihood  of the data; that is, the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 573,
      "total_chunks": 873
    }
  },
  {
    "text": "different optimisation algorithms.\nIn practice, R will report the value of the log likelihood  of the data; that is, the\nlogarithm of the probability of the observed data coming from the estimated model.\nFor given values of ,  and , R will try to maximise the log likelihood when finding\nparameter estimates.\nInformation CriteriaInformation Criteria\nAkaike’s Information Criterion (AIC), which was useful in selecting predictors for\nregression, is also useful for determining the order of an ARIMA model. It can be\nwritten as\nwhere  is the likelihood of the data,  if  and  if . Note that the\nlast term in parentheses is the number of parameters in the model (including , the\nvariance of the residuals).\n322",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 574,
      "total_chunks": 873
    }
  },
  {
    "text": "For ARIMA models, the corrected AIC can be written as\nand the Bayesian Information Criterion can be written as\nGood models are obtained by minimising the AIC, AICc or BIC. Our preference is to\nuse the AICc.\nIt is important to note that these information criteria tend not to be good guides to\nselecting the appropriate order of differencing ( ) of a model, but only for selecting\nthe values of  and . This is because the differencing changes the data on which the\nlikelihood is computed, making the AIC values between models with different orders\nof differencing not comparable. So we need to use some other approach to choose ,\nand then we can use the AICc to select  and .\n/g22/g2A/g24/g44 /g1E /g22/g2A/g24 /gC /gD/g13/g9/g2C /gC /g2D /gC /g27 /gC /g12/gA/g9/g2C /gC /g2D /gC /g27 /gC/g13 /gA\n/g16 /gC3 /g2C /gC3 /g2D /gC3 /g27 /gC3/g13\n/g23/g2A/g24 /g1E /g22/g2A/g24 /gC /g3C/g4D/g50/g48/g9/g16 /gA /gC3 /g13/g3E/g9/g2C /gC /g2D /gC /g27 /gC /g12/gA/gF\n/g20\n/g2C/g2D\n/g20\n/g2C/g2D\n323",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 575,
      "total_chunks": 873
    }
  },
  {
    "text": "8.78.7  ARIMA modelling in RARIMA modelling in R\nHow does How does ĳauto.arima()ĳ work?work?\nThe ıauto.arima()ı function in R uses a variation of the Hyndman-Khandakar algorithm\n(Hyndman & Khandakar, 2008 ), which combines unit root tests, minimisation of the\nAICc and MLE to obtain an ARIMA model. The arguments to ıauto.arima()ı provide for\nmany variations on the algorithm. What is described here is the default behaviour.\nHyndman-Khandakar algorithm for automatic ARIMA modelling\n1. The number of differences  is determined using repeated KPSS tests.\n2. The values of  and  are then chosen by minimising the AICc after differencing the data \ntimes. Rather than considering every possible combination of  and , the algorithm uses a\nstepwise search to traverse the model space.\na. Four initial models are fitted:\nARIMA ,\nARIMA ,\nARIMA ,\nARIMA .\nA constant is included unless . If , an additional model is also fitted:\nARIMA  without a constant.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 576,
      "total_chunks": 873
    }
  },
  {
    "text": "a. Four initial models are fitted:\nARIMA ,\nARIMA ,\nARIMA ,\nARIMA .\nA constant is included unless . If , an additional model is also fitted:\nARIMA  without a constant.\nb. The best model (with the smallest AICc value) fitted in step (a) is set to be the\n“current model”.\nc. Variations on the current model are considered:\nvary  and/or  from the current model by ;\ninclude/exclude  from the current model.\nThe best model considered so far (either the current model or one of these variations)\nbecomes the new current model.\nd. Repeat Step 2(c) until no lower AICc can be found.\nThe default procedure uses some approximations to speed up the search. These\napproximations can be avoided with the argument ıapproximation=FALSEı. It is possible\nthat the minimum AICc model will not be found due to these approximations, or because\nof the use of a stepwise procedure. A much larger set of models will be searched if the\nargument ıstepwise=FALSEı is used. See the help file for a full description of the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 577,
      "total_chunks": 873
    }
  },
  {
    "text": "of the use of a stepwise procedure. A much larger set of models will be searched if the\nargument ıstepwise=FALSEı is used. See the help file for a full description of the\narguments.\n324",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 578,
      "total_chunks": 873
    }
  },
  {
    "text": "Choosing your own modelChoosing your own model\nIf you want to choose the model yourself, use the ıArima()ı function in R. There is\nanother function ıarima()ı in R which also fits an ARIMA model. However, it does not\nallow for the constant  unless , and it does not return everything required for other\nfunctions in the forecast package to work. Finally, it does not allow the estimated model\nto be applied to new data (which is useful for checking forecast accuracy). Consequently,\nit is recommended that ıArima()ı be used instead.\nModelling procedureModelling procedure\nWhen fitting an ARIMA model to a set of (non-seasonal) time series data, the following\nprocedure provides a useful general approach.\n1. Plot the data and identify any unusual observations.\n2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the\nvariance.\n3. If the data are non-stationary, take first differences of the data until the data are\nstationary.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 579,
      "total_chunks": 873
    }
  },
  {
    "text": "2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the\nvariance.\n3. If the data are non-stationary, take first differences of the data until the data are\nstationary.\n4. Examine the ACF/PACF: Is an ARIMA( ) or ARIMA( ) model appropriate?\n5. Try your chosen model(s), and use the AICc to search for a better model.\n6. Check the residuals from your chosen model by plotting the ACF of the residuals, and\ndoing a portmanteau test of the residuals. If they do not look like white noise, try a\nmodified model.\n7. Once the residuals look like white noise, calculate forecasts.\nThe Hyndman-Khandakar algorithm only takes care of steps 3–5. So even if you use it,\nyou will still need to take care of the other steps yourself.\nThe process is summarised in Figure 8.11.\n325",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 580,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.11: General process for forecasting using an ARIMA model.\n326",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 581,
      "total_chunks": 873
    }
  },
  {
    "text": "Portmanteau tests of residuals for ARIMA modelsPortmanteau tests of residuals for ARIMA models\nWith ARIMA models, more accurate portmanteau tests are obtained if the degrees of\nfreedom of the test statistic are adjusted to take account of the number of parameters in\nthe model. Specifically, we use  degrees of freedom in the test, where  is the\nnumber of AR and MA parameters in the model. So for the non-seasonal models, we have\nconsidered so far, . The correct value of  is automatically determined in the\nıcheckresiduals()ı function.\nExample: Seasonally adjusted electrical equipment ordersExample: Seasonally adjusted electrical equipment orders\nWe will apply this procedure to the seasonally adjusted electrical equipment orders data\nshown in Figure 8.12.\nFigure 8.12: Seasonally adjusted electrical equipment orders index in the Euro area.\n1. The time plot shows some sudden changes, particularly the big drop in 2008/2009.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 582,
      "total_chunks": 873
    }
  },
  {
    "text": "shown in Figure 8.12.\nFigure 8.12: Seasonally adjusted electrical equipment orders index in the Euro area.\n1. The time plot shows some sudden changes, particularly the big drop in 2008/2009.\nThese changes are due to the global economic environment. Otherwise there is\nnothing unusual about the time plot and there appears to be no need to do any data\nadjustments.\n2. There is no evidence of changing variance, so we will not do a Box-Cox\ntransformation.\nelecequip %>% stl(s.window='periodic') %>%  seasadj() -> eeadj\nautoplot(eeadj)\n327",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 583,
      "total_chunks": 873
    }
  },
  {
    "text": "3. The data are clearly non-stationary, as the series wanders up and down for long\nperiods. Consequently, we will take a first difference of the data. The differenced data\nare shown in Figure 8.13. These look stationary, and so we will not consider further\ndifferences.\nFigure 8.13: Time plot and ACF and PACF plots for the differenced seasonally adjusted\nelectrical equipment data.\n4. The PACF shown in Figure 8.13 is suggestive of an AR(3) model. So an initial\ncandidate model is an ARIMA(3,1,0). There are no other obvious candidate models.\n5. We fit an ARIMA(3,1,0) model along with variations including ARIMA(4,1,0),\nARIMA(2,1,0), ARIMA(3,1,1), etc. Of these, the ARIMA(3,1,1) has a slightly smaller\nAICc value.\neeadj %>% diff() %>% ggtsdisplay(main=\"\")\n328",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 584,
      "total_chunks": 873
    }
  },
  {
    "text": "6. The ACF plot of the residuals from the ARIMA(3,1,1) model shows that all\nautocorrelations are within the threshold limits, indicating that the residuals are\nbehaving like white noise. A portmanteau test returns a large p-value, also\nsuggesting that the residuals are white noise.\nFigure 8.14: Residual plots for the ARIMA(3,1,1) model.\n(fit <- Arima(eeadj, order=c(3,1,1)))\n#> Series: eeadj \n#> ARIMA(3,1,1) \n#> \n#> Coefficients:\n#>         ar1    ar2    ar3     ma1\n#>       0.004  0.092  0.370  -0.392\n#> s.e.  0.220  0.098  0.067   0.243\n#> \n#> sigma^2 = 9.58:  log likelihood = -492.7\n#> AIC=995.4   AICc=995.7   BIC=1012\ncheckresiduals(fit)\n329",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 585,
      "total_chunks": 873
    }
  },
  {
    "text": "#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from ARIMA(3,1,1)\n#> Q* = 24, df = 20, p-value = 0.2\n#> \n#> Model df: 4.   Total lags used: 24\n7. Forecasts from the chosen model are shown in Figure 8.15.\nFigure 8.15: Forecasts for the seasonally adjusted electrical orders index.\nIf we had used the automated algorithm instead, we would have obtained an\nARIMA(3,1,0) model using the default settings, but the ARIMA(3,1,1) model if we had set\nıapproximation=FALSEı.\nautoplot(forecast(fit))\n330",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 586,
      "total_chunks": 873
    }
  },
  {
    "text": "Understanding constants in RUnderstanding constants in R\nA non-seasonal ARIMA model can be written as\nor equivalently as\nwhere  and  is the mean of . R uses the\nparameterisation of Equation (8.5).\nThus, the inclusion of a constant in a non-stationary ARIMA model is equivalent to\ninducing a polynomial trend of order  in the forecast function. (If the constant is\nomitted, the forecast function includes a polynomial trend of order .) When ,\nwe have the special case that  is the mean of .\nBy default, the ıArima()ı function sets  when  and provides an estimate of\n when . It will be close to the sample mean of the time series, but usually not\nidentical to it as the sample mean is not the maximum likelihood estimate when\n.\nThe argument ıinclude.meanı only has an effect when  and is ıTRUEı by default.\nSetting ıinclude.mean=FALSEı will force .\nThe argument ıinclude.driftı allows  when . For , no constant is",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 587,
      "total_chunks": 873
    }
  },
  {
    "text": ".\nThe argument ıinclude.meanı only has an effect when  and is ıTRUEı by default.\nSetting ıinclude.mean=FALSEı will force .\nThe argument ıinclude.driftı allows  when . For , no constant is\nallowed as a quadratic or higher order trend is particularly dangerous when forecasting.\nThe parameter  is called the “drift” in the R output when .\nThere is also an argument ıinclude.constantı which, if ıTRUEı, will set\nıinclude.mean=TRUEı if  and ıinclude.drift=TRUEı when . If\nıinclude.constant=FALSE ı, both ıinclude.meanı and ıinclude.driftı will be set to ıFALSEı. If\nıinclude.constantı is used, the values of ıinclude.mean=TRUEı and ıinclude.drift=TRUEı are\nignored.\nThe ıauto.arima()ı function automates the inclusion of a constant. By default, for \nor , a constant will be included if it improves the AICc value; for  the constant\nis always omitted. If ıallowdrift=FALSEı is specified, then the constant is only allowed\nwhen .\nPlotting the characteristic rootsPlotting the characteristic roots",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 588,
      "total_chunks": 873
    }
  },
  {
    "text": "is always omitted. If ıallowdrift=FALSEı is specified, then the constant is only allowed\nwhen .\nPlotting the characteristic rootsPlotting the characteristic roots\n(This is a more advanced section and can be skipped if desired.)\n331",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 589,
      "total_chunks": 873
    }
  },
  {
    "text": "We can re-write Equation (8.4) as\nwhere  is a th order polynomial in  and\n is a th order polynomial in .\nThe stationarity conditions for the model are that the  complex roots of  lie outside\nthe unit circle, and the invertibility conditions are that the  complex roots of  lie\noutside the unit circle. So we can see whether the model is close to invertibility or\nstationarity by a plot of the roots in relation to the complex unit circle.\nIt is easier to plot the inverse roots instead, as they should all lie within the unit circle.\nThis is easily done in R. For the ARIMA(3,1,1) model fitted to the seasonally adjusted\nelectrical equipment index, we obtain Figure 8.16 .\nFigure 8.16: Inverse characteristic roots for the ARIMA(3,1,1) model fitted to the seasonally\nadjusted electrical equipment index.\nThe three red dots in the left hand plot correspond to the roots of the polynomials ,\nwhile the red dot in the right hand plot corresponds to the root of . They are all",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 590,
      "total_chunks": 873
    }
  },
  {
    "text": "The three red dots in the left hand plot correspond to the roots of the polynomials ,\nwhile the red dot in the right hand plot corresponds to the root of . They are all\ninside the unit circle, as we would expect because R ensures the fitted model is both\nstationary and invertible. Any roots close to the unit circle may be numerically unstable,\nand the corresponding model will not be good for forecasting.\n/g5D/g9/g4/gA/g9/g12 /gC3 /g4/gA /g20 /g35 /g30 /g1E /g1F /gC /g4A /g9/g4/gA/g47 /g30\n/g5D/g9/g4/gA /g1E /g9/g12 /gC3 /g5D /g12 /g4 /gC3/gFC/gC3/g5D /g2C /g4 /g2C /gA /g2C/g4\n/g4A/g9/g4/gA /g1E /g9/g12 /gC /g4A /g12 /g4 /gC/gFC/gC/g4A /g2D /g4 /g2D /gA /g2D/g4\n/g2C/g5D /g9/g4/gA\n/g2D/g4A /g9/g4/gA\nautoplot(fit)\n/g5D/g9/g4/gA\n/g4A/g9/g4/gA\n332",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 591,
      "total_chunks": 873
    }
  },
  {
    "text": "The ıArima()ı function will never return a model with inverse roots outside the unit\ncircle. The ıauto.arima()ı function is even stricter, and will not select a model with roots\nclose to the unit circle either.\nBibliographyBibliography\nHyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The\nforecast package for R. Journal of Statistical Software , 27(1), 1–22. [DOI]\n333",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 592,
      "total_chunks": 873
    }
  },
  {
    "text": "8.88.8  ForecastingForecasting\nPoint forecastsPoint forecasts\nAlthough we have calculated forecasts from the ARIMA models in our examples, we have\nnot yet explained how they are obtained. Point forecasts can be calculated using the\nfollowing three steps.\n1. Expand the ARIMA equation so that  is on the left hand side and all other terms are\non the right.\n2. Rewrite the equation by replacing  with .\n3. On the right hand side of the equation, replace future observations with their\nforecasts, future errors with zero, and past errors with the corresponding residuals.\nBeginning with , these steps are then repeated for  until all forecasts\nhave been calculated.\nThe procedure is most easily understood via an example. We will illustrate it using the\nARIMA(3,1,1) model fitted in the previous section. The model can be written as follows:\nwhere , ,  and . Then we expand the\nleft hand side to obtain\nand applying the backshift operator gives",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 593,
      "total_chunks": 873
    }
  },
  {
    "text": "ARIMA(3,1,1) model fitted in the previous section. The model can be written as follows:\nwhere , ,  and . Then we expand the\nleft hand side to obtain\nand applying the backshift operator gives\nFinally, we move all terms other than  to the right hand side:\nThis completes the first step. While the equation now looks like an ARIMA(4,0,1), it is still\nthe same ARIMA(3,1,1) model we started with. It cannot be considered an ARIMA(4,0,1)\nbecause the coefficients do not satisfy the stationarity conditions.\n334",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 594,
      "total_chunks": 873
    }
  },
  {
    "text": "For the second step, we replace  with  in (8.6) :\nAssuming we have observations up to time , all values on the right hand side are known\nexcept for , which we replace with zero, and , which we replace with the last\nobserved residual :\nA forecast of  is obtained by replacing  with  in (8.6)  . All values on the right\nhand side will be known at time  except  which we replace with , and \nand , both of which we replace with zero:\nThe process continues in this manner for all future time periods. In this way, any number\nof point forecasts can be obtained.\nPrediction intervalsPrediction intervals\nThe calculation of ARIMA prediction intervals is more difficult, and the details are largely\nbeyond the scope of this book. We will only give some simple examples.\nThe first prediction interval is easy to calculate. If  is the standard deviation of the\nresiduals, then a 95% prediction interval is given by . This result is true\nfor all ARIMA models regardless of their parameters and orders.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 595,
      "total_chunks": 873
    }
  },
  {
    "text": "residuals, then a 95% prediction interval is given by . This result is true\nfor all ARIMA models regardless of their parameters and orders.\nMulti-step prediction intervals for ARIMA(0,0, ) models are relatively easy to calculate.\nWe can write the model as\nThen, the estimated forecast variance can be written as\nwhere  for , and a 95% prediction interval is given by .\nIn Section 8.4 , we showed that an AR(1) model can be written as an MA( ) model. Using\nthis equivalence, the above result for MA( ) models can also be used to obtain prediction\nintervals for AR(1) models.\n335",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 596,
      "total_chunks": 873
    }
  },
  {
    "text": "More general results, and other special cases of multi-step prediction intervals for an\nARIMA( , , ) model, are given in more advanced textbooks such as Brockwell & Davis\n(2016).\nThe prediction intervals for ARIMA models are based on assumptions that the residuals\nare uncorrelated and normally distributed. If either of these assumptions does not hold,\nthen the prediction intervals may be incorrect. For this reason, always plot the ACF and\nhistogram of the residuals to check the assumptions before producing prediction\nintervals.\nIn general, prediction intervals from ARIMA models increase as the forecast horizon\nincreases. For stationary models (i.e., with ) they will converge, so that prediction\nintervals for long horizons are all essentially the same. For , the prediction intervals\nwill continue to grow into the future.\nAs with most prediction interval calculations, ARIMA-based intervals tend to be too",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 597,
      "total_chunks": 873
    }
  },
  {
    "text": "will continue to grow into the future.\nAs with most prediction interval calculations, ARIMA-based intervals tend to be too\nnarrow. This occurs because only the variation in the errors has been accounted for.\nThere is also variation in the parameter estimates, and in the model order, that has not\nbeen included in the calculation. In addition, the calculation assumes that the historical\npatterns that have been modelled will continue into the forecast period.\nBibliographyBibliography\nBrockwell, P. J., & Davis, R. A. (2016). Introduction to time series and forecasting  (3rd\ned). New York, USA: Springer. [Amazon]\n336",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 598,
      "total_chunks": 873
    }
  },
  {
    "text": "8.98.9  Seasonal ARIMA modelsSeasonal ARIMA models\nSo far, we have restricted our attention to non-seasonal data and non-seasonal\nARIMA models. However, ARIMA models are also capable of modelling a wide range\nof seasonal data.\nA seasonal ARIMA model is formed by including additional seasonal terms in the\nARIMA models we have seen so far. It is written as follows:\nARIMA\nNon-seasonal part Seasonal part of\nof the model of the model\nwhere  number of observations per year. We use uppercase notation for the\nseasonal parts of the model, and lowercase notation for the non-seasonal parts of\nthe model.\nThe seasonal part of the model consists of terms that are similar to the non-seasonal\ncomponents of the model, but involve backshifts of the seasonal period. For\nexample, an ARIMA(1,1,1)(1,1,1)  model (without a constant) is for quarterly data (\n), and can be written as\nThe additional seasonal terms are simply multiplied by the non-seasonal terms.\nACF/PACFACF/PACF",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 599,
      "total_chunks": 873
    }
  },
  {
    "text": "), and can be written as\nThe additional seasonal terms are simply multiplied by the non-seasonal terms.\nACF/PACFACF/PACF\nThe seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF\nand ACF. For example, an ARIMA(0,0,0)(0,0,1)  model will show:\na spike at lag 12 in the ACF but no other significant spikes;\nexponential decay in the seasonal lags of the PACF (i.e., at lags 12, 24, 36, …).\nSimilarly, an ARIMA(0,0,0)(1,0,0)  model will show:\nexponential decay in the seasonal lags of the ACF;\na single significant spike at lag 12 in the PACF.\n337",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 600,
      "total_chunks": 873
    }
  },
  {
    "text": "In considering the appropriate seasonal orders for a seasonal ARIMA model, restrict\nattention to the seasonal lags.\nThe modelling procedure is almost the same as for non-seasonal data, except that\nwe need to select seasonal AR and MA terms as well as the non-seasonal components\nof the model. The process is best illustrated via examples.\nExample: European quarterly retail tradeExample: European quarterly retail trade\nWe will describe the seasonal ARIMA modelling procedure using quarterly European\nretail trade data from 1996 to 2011. The data are plotted in Figure 8.17.\nFigure 8.17: Quarterly retail trade index in the Euro area (17 countries), 1996–2011, covering\nwholesale and retail trade, and the repair of motor vehicles and motorcycles. (Index:\n2005 = 100).\nThe data are clearly non-stationary, with some seasonality, so we will first take a\nseasonal difference. The seasonally differenced data are shown in Figure 8.18. These",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 601,
      "total_chunks": 873
    }
  },
  {
    "text": "2005 = 100).\nThe data are clearly non-stationary, with some seasonality, so we will first take a\nseasonal difference. The seasonally differenced data are shown in Figure 8.18. These\nalso appear to be non-stationary, so we take an additional first difference, shown in\nFigure 8.19 .\nautoplot(euretail) + ylab(\"Retail index\") + xlab(\"Year\")\n338",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 602,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.18: Seasonally differenced European retail trade index.\nFigure 8.19: Double differenced European retail trade index.\neuretail %>% diff(lag=4) %>% ggtsdisplay()\neuretail %>% diff(lag=4) %>% diff() %>% ggtsdisplay()\n339",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 603,
      "total_chunks": 873
    }
  },
  {
    "text": "Our aim now is to find an appropriate ARIMA model based on the ACF and PACF\nshown in Figure 8.19. The significant spike at lag 1 in the ACF suggests a non-\nseasonal MA(1) component, and the significant spike at lag 4 in the ACF suggests a\nseasonal MA(1) component. Consequently, we begin with an ARIMA(0,1,1)(0,1,1)\nmodel, indicating a first and seasonal difference, and non-seasonal and seasonal\nMA(1) components. The residuals for the fitted model are shown in Figure 8.20. (By\nanalogous logic applied to the PACF, we could also have started with an ARIMA(1,1,0)\n(1,1,0)  model.)\nFigure 8.20: Residuals from the fitted ARIMA(0,1,1)(0,1,1)  model for the European retail\ntrade index data.\nBoth the ACF and PACF show significant spikes at lag 2, and almost significant spikes\nat lag 3, indicating that some additional non-seasonal terms need to be included in\nthe model. The AICc of the ARIMA(0,1,2)(0,1,1)  model is 74.36, while that for the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 604,
      "total_chunks": 873
    }
  },
  {
    "text": "at lag 3, indicating that some additional non-seasonal terms need to be included in\nthe model. The AICc of the ARIMA(0,1,2)(0,1,1)  model is 74.36, while that for the\nARIMA(0,1,3)(0,1,1)  model is 68.53. We tried other models with AR terms as well,\nbut none that gave a smaller AICc value. Consequently, we choose the ARIMA(0,1,3)\n(0,1,1)  model. Its residuals are plotted in Figure 8.21. All the spikes are now within\nthe significance limits, so the residuals appear to be white noise. The Ljung-Box test\nalso shows that the residuals have no remaining autocorrelations.\n/g15\n/g15\neuretail %>%\n  Arima(order=c(0,1,1), seasonal=c(0,1,1)) %>%\n  residuals() %>% ggtsdisplay()\n/g15\n/g15\n/g15\n/g15\n340",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 605,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.21: Residuals from the fitted ARIMA(0,1,3)(0,1,1)  model for the European retail\ntrade index data.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from ARIMA(0,1,3)(0,1,1)[4]\n#> Q* = 0.51, df = 4, p-value = 1\n#> \n#> Model df: 4.   Total lags used: 8\nThus, we now have a seasonal ARIMA model that passes the required checks and is\nready for forecasting. Forecasts from the model for the next three years are shown in\nFigure 8.22. The forecasts follow the recent trend in the data, because of the double\ndifferencing. The large and rapidly increasing prediction intervals show that the\nretail trade index could start increasing or decreasing at any time — while the point\nforecasts trend downwards, the prediction intervals allow for the data to trend\nupwards during the forecast period.\nfit3 <- Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1))\ncheckresiduals(fit3)\n/g15\nfit3 %>% forecast(h=12) %>% autoplot()\n341",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 606,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.22: Forecasts of the European retail trade index data using the ARIMA(0,1,3)(0,1,1)\nmodel. 80% and 95% prediction intervals are shown.\nWe could have used ıauto.arima()ı to do most of this work for us. It would have given\nthe same result.\nThe ıauto.arima()ı function uses ınsdiffs()ı to determine  (the number of\nseasonal differences to use), and ındiffs()ı to determine  (the number of ordinary\ndifferences to use). The selection of the other model parameters (  and ) are\nall determined by minimizing the AICc, as with non-seasonal ARIMA models.\n/g15\nauto.arima(euretail)\n#> Series: euretail \n#> ARIMA(0,1,3)(0,1,1)[4] \n#> \n#> Coefficients:\n#>         ma1    ma2    ma3    sma1\n#>       0.263  0.369  0.420  -0.664\n#> s.e.  0.124  0.126  0.129   0.155\n#> \n#> sigma^2 = 0.156:  log likelihood = -28.63\n#> AIC=67.26   AICc=68.39   BIC=77.65\n/g6\n/g20\n/g2C/gD /g2D /gD /g12/g13\n342",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 607,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Corticosteroid drug sales in AustraliaExample: Corticosteroid drug sales in Australia\nOur second example is more difficult. We will try to forecast monthly corticosteroid\ndrug sales in Australia. These are known as H02 drugs under the Anatomical\nTherapeutic Chemical classification scheme.\nFigure 8.23: Corticosteroid drug sales in Australia (in millions of scripts per month).\nLogged data shown in bottom panel.\nData from July 1991 to June 2008 are plotted in Figure 8.23. There is a small increase\nin the variance with the level, so we take logarithms to stabilise the variance.\nThe data are strongly seasonal and obviously non-stationary, so seasonal\ndifferencing will be used. The seasonally differenced data are shown in Figure 8.24. It\nis not clear at this point whether we should do another difference or not. We decide\nnot to, but the choice is not obvious.\nlh02 <- log(h02)\ncbind(\"H02 sales (million scripts)\"  = h02,\n      \"Log H02 sales\"= lh02) %>%",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 608,
      "total_chunks": 873
    }
  },
  {
    "text": "not to, but the choice is not obvious.\nlh02 <- log(h02)\ncbind(\"H02 sales (million scripts)\"  = h02,\n      \"Log H02 sales\"= lh02) %>%\n  autoplot(facets=TRUE) + xlab(\"Year\") + ylab(\"\")\n343",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 609,
      "total_chunks": 873
    }
  },
  {
    "text": "The last few observations appear to be different (more variable) from the earlier\ndata. This may be due to the fact that data are sometimes revised when earlier sales\nare reported late.\nFigure 8.24: Seasonally differenced corticosteroid drug sales in Australia (in millions of\nscripts per month).\nIn the plots of the seasonally differenced data, there are spikes in the PACF at lags 12\nand 24, but nothing at seasonal lags in the ACF. This may be suggestive of a seasonal\nAR(2) term. In the non-seasonal lags, there are three significant spikes in the PACF,\nsuggesting a possible AR(3) term. The pattern in the ACF is not indicative of any\nsimple model.\nConsequently, this initial analysis suggests that a possible model for these data is an\nARIMA(3,0,0)(2,1,0) . We fit this model, along with some variations on it, and\ncompute the AICc values shown in the following table.\nlh02 %>% diff(lag=12) %>%\n  ggtsdisplay(xlab=\"Year\",\n    main=\"Seasonally differenced H02 scripts\" )\n344",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 610,
      "total_chunks": 873
    }
  },
  {
    "text": "ModelModel AICcAICc\nARIMA(3,0,1)(0,1,2) -485.5\nARIMA(3,0,1)(1,1,1) -484.2\nARIMA(3,0,1)(0,1,1) -483.7\nARIMA(3,0,1)(2,1,0) -476.3\nARIMA(3,0,0)(2,1,0) -475.1\nARIMA(3,0,2)(2,1,0) -474.9\nARIMA(3,0,1)(1,1,0) -463.4\nOf these models, the best is the ARIMA(3,0,1)(0,1,2)  model (i.e., it has the smallest\nAICc value).\n(fit <- Arima(h02, order=c(3,0,1), seasonal=c(0,1,2),\n  lambda=0))\n#> Series: h02 \n#> ARIMA(3,0,1)(0,1,2)[12] \n#> Box Cox transformation: lambda= 0 \n#> \n#> Coefficients:\n#>          ar1    ar2    ar3    ma1    sma1    sma2\n#>       -0.160  0.548  0.568  0.383  -0.522  -0.177\n#> s.e.   0.164  0.088  0.094  0.190   0.086   0.087\n#> \n#> sigma^2 = 0.00428:  log likelihood = 250\n#> AIC=-486.1   AICc=-485.5   BIC=-463.3\ncheckresiduals(fit, lag= 36)\n345",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 611,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.25: Residuals from the ARIMA(3,0,1)(0,1,2)  model applied to the H02 monthly\nscript sales data.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from ARIMA(3,0,1)(0,1,2)[12]\n#> Q* = 51, df = 30, p-value = 0.01\n#> \n#> Model df: 6.   Total lags used: 36\nThe residuals from this model are shown in Figure 8.25. There are a few significant\nspikes in the ACF, and the model fails the Ljung-Box test. The model can still be used\nfor forecasting, but the prediction intervals may not be accurate due to the correlated\nresiduals.\nNext we will try using the automatic ARIMA algorithm. Running ıauto.arima()ı with\nall arguments left at their default values led to an ARIMA(2,1,1)(0,1,2)  model.\nHowever, the model still fails the Ljung-Box test for 36 lags. Sometimes it is just not\npossible to find a model that passes all of the tests.\n346",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 612,
      "total_chunks": 873
    }
  },
  {
    "text": "Test set evaluation:Test set evaluation:\nWe will compare some of the models fitted so far using a test set consisting of the\nlast two years of data. Thus, we fit the models using data from July 1991 to June\n2006, and forecast the script sales for July 2006 – June 2008. The results are\nsummarised in Table 8.2.\nTable 8.2: RMSE values for various ARIMA models applied to the H02 monthly script\nsales data.\nModelModel RMSERMSE\nARIMA(3,0,1)(0,1,2) 0.0622\nARIMA(3,0,1)(1,1,1) 0.0630\nARIMA(2,1,3)(0,1,1) 0.0634\nARIMA(2,1,1)(0,1,2) 0.0634\nARIMA(2,1,2)(0,1,2) 0.0635\nARIMA(3,0,3)(0,1,1) 0.0637\nARIMA(3,0,1)(0,1,1) 0.0644\nARIMA(3,0,2)(0,1,1) 0.0644\nARIMA(3,0,2)(2,1,0) 0.0645\nARIMA(3,0,1)(2,1,0) 0.0646\nARIMA(4,0,2)(0,1,1) 0.0648\nARIMA(4,0,3)(0,1,1) 0.0648\nARIMA(3,0,0)(2,1,0) 0.0661\nARIMA(3,0,1)(1,1,0) 0.0679\nThe models chosen manually and with ıauto.arima()ı are both in the top four models\nbased on their RMSE values.\nWhen models are compared using AICc values, it is important that all models have",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 613,
      "total_chunks": 873
    }
  },
  {
    "text": "The models chosen manually and with ıauto.arima()ı are both in the top four models\nbased on their RMSE values.\nWhen models are compared using AICc values, it is important that all models have\nthe same orders of differencing. However, when comparing models using a test set,\nit does not matter how the forecasts were produced — the comparisons are always\nvalid. Consequently, in the table above, we can include some models with only\nseasonal differencing and some models with both first and seasonal differencing,\nwhile in the earlier table containing AICc values, we only compared models with\nseasonal differencing but no first differencing.\nNone of the models considered here pass all of the residual tests. In practice, we\nwould normally use the best model we could find, even if it did not pass all of the\ntests.\n347",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 614,
      "total_chunks": 873
    }
  },
  {
    "text": "Forecasts from the ARIMA(3,0,1)(0,1,2)  model (which has the lowest RMSE value\non the test set, and the best AICc value amongst models with only seasonal\ndifferencing) are shown in Figure 8.26.\nFigure 8.26: Forecasts from the ARIMA(3,0,1)(0,1,2)  model applied to the H02 monthly\nscript sales data.\n/g12/g13\nh02 %>%\n  Arima(order=c(3,0,1), seasonal=c(0,1,2), lambda=0) %>%\n  forecast() %>%\n  autoplot() +\n    ylab(\"H02 sales (million scripts)\" ) + xlab(\"Year\")\n/g12/g13\n348",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 615,
      "total_chunks": 873
    }
  },
  {
    "text": "8.108.10  ARIMA vs ETSARIMA vs ETS\nIt is a commonly held myth that ARIMA models are more general than exponential\nsmoothing. While linear exponential smoothing models are all special cases of\nARIMA models, the non-linear exponential smoothing models have no equivalent\nARIMA counterparts. On the other hand, there are also many ARIMA models that\nhave no exponential smoothing counterparts. In particular, all ETS models are non-\nstationary, while some ARIMA models are stationary.\nThe ETS models with seasonality or non-damped trend or both have two unit roots\n(i.e., they need two levels of differencing to make them stationary). All other ETS\nmodels have one unit root (they need one level of differencing to make them\nstationary).\nTable 8.3 gives the equivalence relationships for the two classes of models. For the\nseasonal models, the ARIMA parameters have a large number of restrictions.\nTable 8.3: Equivalence relationships between ETS and ARIMA models.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 616,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonal models, the ARIMA parameters have a large number of restrictions.\nTable 8.3: Equivalence relationships between ETS and ARIMA models.\nETS model ARIMA modelARIMA model ParametersParameters\nETS(A,N,N) ARIMA(0,1,1)\nETS(A,A,N) ARIMA(0,2,2)\nETS(A,A ,N) ARIMA(1,1,2)\nETS(A,N,A) ARIMA(0,1, )(0,1,0)\nETS(A,A,A) ARIMA(0,1, )(0,1,0)\nETS(A,A ,A) ARIMA(0,1, )(0,1,0)\nThe AICc is useful for selecting between models in the same class. For example, we\ncan use it to select an ARIMA model between candidate ARIMA models  or an ETS\nmodel between candidate ETS models. However, it cannot be used to compare\nbetween ETS and ARIMA models because they are in different model classes, and the\nlikelihood is computed in different ways. The examples below demonstrate selecting\nbetween these classes of models.\n18\n349",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 617,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: Comparing Example: Comparing ĳauto.arima()ĳ and  and ĳets()ĳ on non-on non-\nseasonal dataseasonal data\nWe can use time series cross-validation to compare an ARIMA model and an ETS\nmodel. The code below provides functions that return forecast objects from\nıauto.arima()ı and ıets()ı respectively.\nThe returned objects can then be passed into ıtsCV()ı. Let’s consider ARIMA models\nand ETS models for the ıairı data as introduced in Section 7.2  where, ıair <-\nwindow(ausair, start=1990) ı.\nIn this case the ets model has a lower tsCV statistic based on MSEs. Below we\ngenerate and plot forecasts for the next 5 years generated from an ETS model.\nfets <- function(x, h) {\n  forecast(ets(x), h = h)\n}\nfarima <- function(x, h) {\n  forecast(auto.arima(x), h=h)\n}\n# Compute CV errors for ETS as e1\ne1 <- tsCV(air, fets, h=1)\n# Compute CV errors for ARIMA as e2\ne2 <- tsCV(air, farima, h=1)\n# Find MSE of each model class\nmean(e1^2, na.rm=TRUE)\n#> [1] 7.864\nmean(e2^2, na.rm=TRUE)\n#> [1] 9.622",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 618,
      "total_chunks": 873
    }
  },
  {
    "text": "e1 <- tsCV(air, fets, h=1)\n# Compute CV errors for ARIMA as e2\ne2 <- tsCV(air, farima, h=1)\n# Find MSE of each model class\nmean(e1^2, na.rm=TRUE)\n#> [1] 7.864\nmean(e2^2, na.rm=TRUE)\n#> [1] 9.622\nair %>% ets() %>% forecast() %>% autoplot()\n350",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 619,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.27: Forecasts from an ETS model fitted to monthly totals of air transport\npassengers in Australia.\nExample: Comparing Example: Comparing ĳauto.arima()ĳ and  and ĳets()ĳ onon\nseasonal dataseasonal data\nIn this case we want to compare seasonal ARIMA and ETS models applied to the\nquarterly cement production data ıqcementı. Because the series is relatively long, we\ncan afford to use a training and a test set rather than time series cross-validation.\nThe advantage is that this is much faster. We create a training set from the beginning\nof 1988 to the end of 2007 and select an ARIMA and an ETS model using the\nıauto.arima()ı and ıets()ı functions.\nThe output below shows the ARIMA model selected and estimated by ıauto.arima()ı.\nThe ARIMA model does well in capturing all the dynamics in the data as the residuals\nseem to be white noise.\n# Consider the qcement data beginning in 1988\ncement <- window(qcement, start=1988 )\n# Use 20 years of the data as the training set",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 620,
      "total_chunks": 873
    }
  },
  {
    "text": "seem to be white noise.\n# Consider the qcement data beginning in 1988\ncement <- window(qcement, start=1988 )\n# Use 20 years of the data as the training set\ntrain <- window(cement, end=c(2007,4))\n351",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 621,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.28: Residual diagnostic plots for the ARIMA model fitted to the quarterly cement\nproduction training data.\n(fit.arima <- auto.arima(train))\n#> Series: train \n#> ARIMA(1,0,1)(2,1,1)[4] with drift \n#> \n#> Coefficients:\n#>         ar1     ma1   sar1    sar2    sma1  drift\n#>       0.889  -0.237  0.081  -0.235  -0.898  0.010\n#> s.e.  0.084   0.133  0.157   0.139   0.178  0.003\n#> \n#> sigma^2 = 0.0115:  log likelihood = 61.47\n#> AIC=-109   AICc=-107.3   BIC=-92.63\ncheckresiduals(fit.arima)\n352",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 622,
      "total_chunks": 873
    }
  },
  {
    "text": "#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from ARIMA(1,0,1)(2,1,1)[4] with drift\n#> Q* = 0.78, df = 3, p-value = 0.9\n#> \n#> Model df: 5.   Total lags used: 8\nThe output below also shows the ETS model selected and estimated by ıets()ı. This\nmodel also does well in capturing all the dynamics in the data, as the residuals\nsimilarly appear to be white noise.\n(fit.ets <- ets(train))\n#> ETS(M,N,M) \n#> \n#> Call:\n#>  ets(y = train) \n#> \n#>   Smoothing parameters:\n#>     alpha = 0.7341 \n#>     gamma = 1e-04 \n#> \n#>   Initial states:\n#>     l = 1.6439 \n#>     s = 1.031 1.044 1.01 0.9148\n#> \n#>   sigma:  0.0581\n#> \n#>     AIC    AICc     BIC \n#> -2.1967 -0.6411 14.4775\ncheckresiduals(fit.ets)\n353",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 623,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.29: Residual diagnostic plots for the ETS model fitted to the quarterly cement\nproduction training data.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from ETS(M,N,M)\n#> Q* = 6.1, df = 8, p-value = 0.6\n#> \n#> Model df: 0.   Total lags used: 8\nThe output below evaluates the forecasting performance of the two competing\nmodels over the test set. In this case the ETS model seems to be the slightly more\naccurate model based on the test set RMSE, MAPE and MASE.\n354",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 624,
      "total_chunks": 873
    }
  },
  {
    "text": "Notice that the ARIMA model fits the training data slightly better than the ETS\nmodel, but that the ETS model provides more accurate forecasts on the test set. A\ngood fit to training data is never an indication that the model will forecast well.\nBelow we generate and plot forecasts from an ETS model for the next 3 years.\n# Generate forecasts and compare accuracy over the test set\na1 <- fit.arima %>% forecast(h = 4*(2013-2007)+1) %>%\n  accuracy(qcement)\na1[,c(\"RMSE\",\"MAE\",\"MAPE\",\"MASE\")]\n#>                RMSE     MAE  MAPE   MASE\n#> Training set 0.1001 0.07989 4.372 0.5458\n#> Test set     0.1996 0.16882 7.719 1.1534\na2 <- fit.ets %>% forecast(h = 4*(2013-2007)+1) %>%\n  accuracy(qcement)\na2[,c(\"RMSE\",\"MAE\",\"MAPE\",\"MASE\")]\n#>                RMSE     MAE  MAPE   MASE\n#> Training set 0.1022 0.07958 4.372 0.5437\n#> Test set     0.1839 0.15395 6.986 1.0518\n# Generate forecasts from an ETS model\ncement %>% ets() %>% forecast(h=12) %>% autoplot()\n355",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 625,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 8.30: Forecasts from an ETS model fitted to all of the available quarterly cement\nproduction data.\n18. As already noted, comparing information criteria is only valid for ARIMA models\nof the same orders of differencing. ↩ \n356",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 626,
      "total_chunks": 873
    }
  },
  {
    "text": "8.118.11  ExercisesExercises\n1. Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and\n1,000 random numbers.\na. Explain the differences among these figures. Do they all indicate that the\ndata are white noise?\nFigure 8.31: Left: ACF for a white noise series of 36 numbers. Middle: ACF for a white\nnoise series of 360 numbers. Right: ACF for a white noise series of 1,000 numbers.\nb. Why are the critical values at different distances from the mean of zero? Why\nare the autocorrelations different in each figure when they each refer to\nwhite noise?\n2. A classic example of a non-stationary series is the daily closing IBM stock price\nseries (data set ıibmcloseı). Use R to plot the daily closing prices for IBM stock\nand the ACF and PACF. Explain how each plot shows that the series is non-\nstationary and should be differenced.\n3. For the following series, find an appropriate Box-Cox transformation and order\nof differencing in order to obtain stationary data.\na. ıusnetelecı",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 627,
      "total_chunks": 873
    }
  },
  {
    "text": "stationary and should be differenced.\n3. For the following series, find an appropriate Box-Cox transformation and order\nof differencing in order to obtain stationary data.\na. ıusnetelecı\nb. ıusgdpı\nc. ımcopperı\nd. ıenplanementsı\ne. ıvisitorsı\n357",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 628,
      "total_chunks": 873
    }
  },
  {
    "text": "4. For the ıenplanementsı data, write down the differences you chose above using\nbackshift operator notation.\n5. For your retail data (from Exercise 3 in Section 2.10), find the appropriate order\nof differencing (after transformation if necessary) to obtain stationary data.\n6. Use R to simulate and plot some data from simple ARIMA models.\na. Use the following R code to generate data from an AR(1) model with \nand . The process starts with .\nb. Produce a time plot for the series. How does the plot change as you change \n?\nc. Write your own code to generate data from an MA(1) model with \nand .\nd. Produce a time plot for the series. How does the plot change as you change \n?\ne. Generate data from an ARMA(1,1) model with ,  and .\nf. Generate data from an AR(2) model with ,  and .\n(Note that these parameters will give a non-stationary series.)\ng. Graph the latter two series and compare them.\n7. Consider ıwmurdersı, the number of women murdered each year (per 100,000",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 629,
      "total_chunks": 873
    }
  },
  {
    "text": "(Note that these parameters will give a non-stationary series.)\ng. Graph the latter two series and compare them.\n7. Consider ıwmurdersı, the number of women murdered each year (per 100,000\nstandard population) in the United States.\na. By studying appropriate graphs of the series in R, find an appropriate\nARIMA( ) model for these data.\nb. Should you include a constant in the model? Explain.\nc. Write this model in terms of the backshift operator.\nd. Fit the model using R and examine the residuals. Is the model satisfactory?\ne. Forecast three times ahead. Check your forecasts by hand to make sure that\nyou know how they have been calculated.\nf. Create a plot of the series with forecasts and prediction intervals for the next\nthree periods shown.\n/g5D /g12 /g1E /g11/gF/g17\n/g55 /g13 /g1E/g12 /g35 /g12 /g1E/g11\ny <- ts(numeric(100))\ne <- rnorm(100)\nfor(i in 2:100)\n  y[i] <- 0.6*y[i-1] + e[i]\n/g5D /g12\n/g4A /g12 /g1E /g11/gF/g17\n/g55 /g13 /g1E/g12\n/g4A /g12",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 630,
      "total_chunks": 873
    }
  },
  {
    "text": "/g55 /g13 /g1E/g12 /g35 /g12 /g1E/g11\ny <- ts(numeric(100))\ne <- rnorm(100)\nfor(i in 2:100)\n  y[i] <- 0.6*y[i-1] + e[i]\n/g5D /g12\n/g4A /g12 /g1E /g11/gF/g17\n/g55 /g13 /g1E/g12\n/g4A /g12\n/g5D /g12 /g1E /g11/gF/g17 /g4A /g12 /g1E /g11/gF/g17 /g55 /g13 /g1E/g12\n/g5D /g12 /g1E /gC3/g11/gF/g19 /g5D /g13 /g1E /g11/gF/g14 /g55 /g13 /g1E/g12\n/g2C/gD /g20 /gD /g2D\n358",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 631,
      "total_chunks": 873
    }
  },
  {
    "text": "g. Does ıauto.arima()ı give the same model you have chosen? If not, which\nmodel do you think is better?\n8. Consider ıaustaı, the total international visitors to Australia (in millions) for the\nperiod 1980-2015.\na. Use ıauto.arima()ı to find an appropriate ARIMA model. What model was\nselected. Check that the residuals look like white noise. Plot forecasts for the\nnext 10 periods.\nb. Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these\nto part a. Remove the MA term and plot again.\nc. Plot forecasts from an ARIMA(2,1,3) model with drift. Remove the constant\nand see what happens.\nd. Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA\nterm and plot again.\ne. Plot forecasts from an ARIMA(0,2,1) model with no constant.\n9. For the ıusgdpı series:\na. if necessary, find a suitable Box-Cox transformation for the data;\nb. fit a suitable ARIMA model to the transformed data using ıauto.arima()ı;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 632,
      "total_chunks": 873
    }
  },
  {
    "text": "9. For the ıusgdpı series:\na. if necessary, find a suitable Box-Cox transformation for the data;\nb. fit a suitable ARIMA model to the transformed data using ıauto.arima()ı;\nc. try some other plausible models by experimenting with the orders chosen;\nd. choose what you think is the best model and check the residual diagnostics;\ne. produce forecasts of your fitted model. Do the forecasts look reasonable?\nf. compare the results with what you would obtain using ıets()ı (with no\ntransformation).\n10. Consider ıaustouristsı, the quarterly visitor nights (in millions) spent by\ninternational tourists to Australia for the period 1999–2015.\na. Describe the time plot.\nb. What can you learn from the ACF graph?\nc. What can you learn from the PACF graph?\nd. Produce plots of the seasonally differenced data . What model do\nthese graphs suggest?\ne. Does ıauto.arima()ı give the same model that you chose? If not, which model\ndo you think is better?",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 633,
      "total_chunks": 873
    }
  },
  {
    "text": "d. Produce plots of the seasonally differenced data . What model do\nthese graphs suggest?\ne. Does ıauto.arima()ı give the same model that you chose? If not, which model\ndo you think is better?\nf. Write the model in terms of the backshift operator, then without using the\nbackshift operator.\n11. Consider ıusmelecı, the total net generation of electricity (in billion kilowatt\nhours) by the U.S. electric industry (monthly for the period January 1973 – June\n2013). In general there are two peaks per year: in mid-summer and mid-winter.\n/g9/g12 /gC3 /g4 /g15 /gA/g1B /g30\n359",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 634,
      "total_chunks": 873
    }
  },
  {
    "text": "a. Examine the 12-month moving average of this series to see what kind of\ntrend is involved.\nb. Do the data need transforming? If so, find a suitable transformation.\nc. Are the data stationary? If not, find an appropriate differencing which yields\nstationary data.\nd. Identify a couple of ARIMA models that might be useful in describing the\ntime series. Which of your models is the best according to their AIC values?\ne. Estimate the parameters of your best model and do diagnostic testing on the\nresiduals. Do the residuals resemble white noise? If not, try to find another\nARIMA model which fits better.\nf. Forecast the next 15 years of electricity generation by the U.S. electric\nindustry. Get the latest figures from the EIA to check the accuracy of your\nforecasts.\ng. Eventually, the prediction intervals are so wide that the forecasts are not\nparticularly useful. How many years of forecasts do you think are sufficiently\naccurate to be usable?\n12. For the ımcopperı data:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 635,
      "total_chunks": 873
    }
  },
  {
    "text": "particularly useful. How many years of forecasts do you think are sufficiently\naccurate to be usable?\n12. For the ımcopperı data:\na. if necessary, find a suitable Box-Cox transformation for the data;\nb. fit a suitable ARIMA model to the transformed data using ıauto.arima()ı;\nc. try some other plausible models by experimenting with the orders chosen;\nd. choose what you think is the best model and check the residual diagnostics;\ne. produce forecasts of your fitted model. Do the forecasts look reasonable?\nf. compare the results with what you would obtain using ıets()ı (with no\ntransformation).\n13. Choose one of the following seasonal time series: ıhsalesı, ıauscafeı, ıqauselecı,\nıqcementı, ıqgası.\na. Do the data need transforming? If so, find a suitable transformation.\nb. Are the data stationary? If not, find an appropriate differencing which yields\nstationary data.\nc. Identify a couple of ARIMA models that might be useful in describing the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 636,
      "total_chunks": 873
    }
  },
  {
    "text": "b. Are the data stationary? If not, find an appropriate differencing which yields\nstationary data.\nc. Identify a couple of ARIMA models that might be useful in describing the\ntime series. Which of your models is the best according to their AIC values?\nd. Estimate the parameters of your best model and do diagnostic testing on the\nresiduals. Do the residuals resemble white noise? If not, try to find another\nARIMA model which fits better.\ne. Forecast the next 24 months of data using your preferred model.\nf. Compare the forecasts obtained using ıets()ı.\n360",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 637,
      "total_chunks": 873
    }
  },
  {
    "text": "14. For the same time series you used in the previous exercise, try using a non-\nseasonal model applied to the seasonally adjusted data obtained from STL. The\nıstlf()ı function will make the calculations easy (with ımethod=\"arima\"ı).\nCompare the forecasts with those obtained in the previous exercise. Which do\nyou think is the best approach?\n15. For your retail time series (Exercise 5 above):\na. develop an appropriate seasonal ARIMA model;\nb. compare the forecasts with those you obtained in earlier chapters;\nc. Obtain up-to-date retail data from the ABS website (Cat 8501.0, Table 11),\nand compare your forecasts with the actual numbers. How good were the\nforecasts from the various models?\n16. Consider ısheepı, the sheep population of England and Wales from 1867–1939.\na. Produce a time plot of the time series.\nb. Assume you decide to fit the following model:\nwhere  is a white noise series. What sort of ARIMA model is this (i.e., what\nare , , and )?",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 638,
      "total_chunks": 873
    }
  },
  {
    "text": "a. Produce a time plot of the time series.\nb. Assume you decide to fit the following model:\nwhere  is a white noise series. What sort of ARIMA model is this (i.e., what\nare , , and )?\nc. By examining the ACF and PACF of the differenced data, explain why this\nmodel is appropriate.\nd. The last five values of the series are given below:\nYearYear 19351935 19361936 19371937 19381938 19391939\nMillions of sheep 1648 1665 1627 1791 1797\nThe estimated parameters are , , and .\nWithout using the ıforecastı function, calculate forecasts for the next three\nyears (1940–1942).\ne. Now fit the model in R and obtain the forecasts using ıforecastı. How are\nthey different from yours? Why?\n17. The annual bituminous coal production in the United States from 1920 to 1968 is\nin data set ıbicoalı.\na. Produce a time plot of the data.\n361",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 639,
      "total_chunks": 873
    }
  },
  {
    "text": "b. You decide to fit the following model to the series:\nwhere  is the coal production in year  and  is a white noise series. What\nsort of ARIMA model is this (i.e., what are , , and )?\nc. Explain why this model was chosen using the ACF and PACF.\nd. The last five values of the series are given below.\nYearYear 19641964 19651965 19661966 19671967 19681968\nMillions of tons 467 512 534 552 545\nThe estimated parameters are , , ,\n, and . Without using the ıforecastı function, calculate\nforecasts for the next three years (1969–1971).\ne. Now fit the model in R and obtain the forecasts from the same model. How\nare they different from yours? Why?\n18. Before doing this exercise, you will need to install the QuandlQuandl package in R using\na. Select a time series from Quandl. Then copy its short URL and import the\ndata using\n(Replace each ı?????ı with the appropriate values.)\nb. Plot graphs of the data, and try to identify an appropriate ARIMA model.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 640,
      "total_chunks": 873
    }
  },
  {
    "text": "data using\n(Replace each ı?????ı with the appropriate values.)\nb. Plot graphs of the data, and try to identify an appropriate ARIMA model.\nc. Do residual diagnostic checking of your ARIMA model. Are the residuals\nwhite noise?\nd. Use your chosen ARIMA model to forecast the next four years.\ne. Now try to identify an appropriate ETS model.\nf. Do residual diagnostic checking of your ETS model. Are the residuals white\nnoise?\ng. Use your chosen ETS model to forecast the next four years.\nh. Which of the two models do you prefer?\ninstall.packages(\"Quandl\")\ny <- Quandl(\"?????\", api_key=\"?????\", type= \"ts\")\n362",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 641,
      "total_chunks": 873
    }
  },
  {
    "text": "363",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 642,
      "total_chunks": 873
    }
  },
  {
    "text": "8.128.12  Further readingFurther reading\nThe classic text which popularised ARIMA modelling was Box & Jenkins ( 1970).\nThe most recent edition is Box, Jenkins, Reinsel, & Ljung ( 2015), and it is still an\nexcellent reference for all things ARIMA.\nBrockwell & Davis ( 2016) provides a good introduction to the mathematical\nbackground to the models.\nPeña, Tiao, & Tsay (2001) describes some alternative automatic algorithms to the\none used by ıauto.arima() ı.\nBibliographyBibliography\nBox, G. E. P., & Jenkins, G. M. (1970). Time series analysis: Forecasting and\ncontrol. San Francisco: Holden-Day.\nBox, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time series\nanalysis: Forecasting and control  (5th ed). Hoboken, New Jersey: John Wiley &\nSons. [Amazon]\nBrockwell, P. J., & Davis, R. A. (2016). Introduction to time series and forecasting\n(3rd ed). New York, USA: Springer. [Amazon]\nPeña, D., Tiao, G. C., & Tsay, R. S. (Eds.). (2001). A course in time series analysis .",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 643,
      "total_chunks": 873
    }
  },
  {
    "text": "(3rd ed). New York, USA: Springer. [Amazon]\nPeña, D., Tiao, G. C., & Tsay, R. S. (Eds.). (2001). A course in time series analysis .\nNew York, USA: John Wiley & Sons. [Amazon]\n364",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 644,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 9Chapter 9  Dynamic regression modelsDynamic regression models\nThe time series models in the previous two chapters allow for the inclusion of\ninformation from past observations of a series, but not for the inclusion of other\ninformation that may also be relevant. For example, the effects of holidays,\ncompetitor activity, changes in the law, the wider economy, or other external\nvariables, may explain some of the historical variation and may lead to more\naccurate forecasts. On the other hand, the regression models in Chapter 5 allow for\nthe inclusion of a lot of relevant information from predictor variables, but do not\nallow for the subtle time series dynamics that can be handled with ARIMA models. In\nthis chapter, we consider how to extend ARIMA models in order to allow other\ninformation to be included in the models.\nIn Chapter 5  we considered regression models of the form\nwhere  is a linear function of the  predictor variables ( ), and  is",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 645,
      "total_chunks": 873
    }
  },
  {
    "text": "information to be included in the models.\nIn Chapter 5  we considered regression models of the form\nwhere  is a linear function of the  predictor variables ( ), and  is\nusually assumed to be an uncorrelated error term (i.e., it is white noise). We\nconsidered tests such as the Breusch-Godfrey test for assessing whether the\nresulting residuals were significantly correlated.\nIn this chapter, we will allow the errors from a regression to contain autocorrelation.\nTo emphasise this change in perspective, we will replace  with  in the equation.\nThe error series  is assumed to follow an ARIMA model. For example, if  follows\nan ARIMA(1,1,1) model, we can write\nwhere  is a white noise series.\nNotice that the model has two error terms here — the error from the regression\nmodel, which we denote by , and the error from the ARIMA model, which we\ndenote by . Only the ARIMA model errors are assumed to be white noise.\n365",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 646,
      "total_chunks": 873
    }
  },
  {
    "text": "9.19.1  EstimationEstimation\nWhen we estimate the parameters from the model, we need to minimise the sum of\nsquared  values. If we minimise the sum of squared  values instead (which is\nwhat would happen if we estimated the regression model ignoring the\nautocorrelations in the errors), then several problems arise.\n1. The estimated coefficients  are no longer the best estimates, as some\ninformation has been ignored in the calculation;\n2. Any statistical tests associated with the model (e.g., t-tests on the coefficients)\nwill be incorrect.\n3. The AICc values of the fitted models are no longer a good guide as to which is the\nbest model for forecasting.\n4. In most cases, the -values associated with the coefficients will be too small,\nand so some predictor variables will appear to be important when they are not.\nThis is known as “spurious regression”.\nMinimising the sum of squared  values avoids these problems. Alternatively,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 647,
      "total_chunks": 873
    }
  },
  {
    "text": "and so some predictor variables will appear to be important when they are not.\nThis is known as “spurious regression”.\nMinimising the sum of squared  values avoids these problems. Alternatively,\nmaximum likelihood estimation can be used; this will give similar estimates of the\ncoefficients.\nAn important consideration when estimating a regression with ARMA errors is that\nall of the variables in the model must first be stationary. Thus, we first have to check\nthat  and all of the predictors  appear to be stationary. If we estimate\nthe model when any of these are non-stationary, the estimated coefficients will not\nbe consistent estimates (and therefore may not be meaningful). One exception to\nthis is the case where non-stationary variables are co-integrated. If there exists a\nlinear combination of the non-stationary  and the predictors that is stationary,\nthen the estimated coefficients will be consistent.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 648,
      "total_chunks": 873
    }
  },
  {
    "text": "linear combination of the non-stationary  and the predictors that is stationary,\nthen the estimated coefficients will be consistent.\nWe therefore first difference the non-stationary variables in the model. It is often\ndesirable to maintain the form of the relationship between  and the predictors, and\nconsequently it is common to difference all of the variables if any of them need\ndifferencing. The resulting model is then called a “model in differences”, as distinct\nfrom a “model in levels”, which is what is obtained when the original data are used\nwithout differencing.\n19\n366",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 649,
      "total_chunks": 873
    }
  },
  {
    "text": "If all of the variables in the model are stationary, then we only need to consider\nARMA errors for the residuals. It is easy to see that a regression model with ARIMA\nerrors is equivalent to a regression model in differences with ARMA errors. For\nexample, if the above regression model with ARIMA(1,1,1) errors is differenced we\nobtain the model\nwhere ,  and , which is a regression\nmodel in differences with ARMA errors.\nBibliography\nHarris, R., & Sollis, R. (2003). Applied time series modelling and forecasting .\nChichester, UK: John Wiley & Sons. [Amazon]\n19. Forecasting with cointegrated models is discussed by Harris & Sollis ( 2003).↩ \n367",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 650,
      "total_chunks": 873
    }
  },
  {
    "text": "9.29.2  Regression with ARIMA errors in RRegression with ARIMA errors in R\nThe R function ıArima()ı will fit a regression model with ARIMA errors if the\nargument ıxregı is used. The ıorderı argument specifies the order of the ARIMA\nerror model. If differencing is specified, then the differencing is applied to all\nvariables in the regression model before the model is estimated. For example, the R\ncommand\nwill fit the model , where  is an AR(1) error. This is\nequivalent to the model\nwhere  is an ARIMA(1,1,0) error. Notice that the constant term disappears due to\nthe differencing. To include a constant in the differenced model, specify\nıinclude.drift=TRUEı.\nThe ıauto.arima()ı function will also handle regression terms via the ıxregı\nargument. The user must specify the predictor variables to include, but\nıauto.arima()ı will select the best ARIMA model for the errors. If differencing is\nrequired, then all variables are differenced during the estimation process, although",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 651,
      "total_chunks": 873
    }
  },
  {
    "text": "ıauto.arima()ı will select the best ARIMA model for the errors. If differencing is\nrequired, then all variables are differenced during the estimation process, although\nthe final model will be expressed in terms of the original variables.\nThe AICc is calculated for the final model, and this value can be used to determine\nthe best predictors. That is, the procedure should be repeated for all subsets of\npredictors to be considered, and the model with the lowest AICc value selected.\nExample: US Personal Consumption and IncomeExample: US Personal Consumption and Income\nFigure 9.1  shows the quarterly changes in personal consumption expenditure and\npersonal disposable income from 1970 to 2016 Q3. We would like to forecast changes\nin expenditure based on changes in income. A change in income does not necessarily\ntranslate to an instant change in consumption (e.g., after the loss of a job, it may\ntake a few months for expenses to be reduced to allow for the new circumstances).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 652,
      "total_chunks": 873
    }
  },
  {
    "text": "translate to an instant change in consumption (e.g., after the loss of a job, it may\ntake a few months for expenses to be reduced to allow for the new circumstances).\nfit <- Arima(y, xreg=x, order=c(1,1,0))\n368",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 653,
      "total_chunks": 873
    }
  },
  {
    "text": "However, we will ignore this complexity in this example and try to measure the\ninstantaneous effect of the average change of income on the average change of\nconsumption expenditure.\nFigure 9.1: Percentage changes in quarterly personal consumption expenditure and\npersonal disposable income for the USA, 1970 to 2016 Q3.\nautoplot(uschange[,1:2], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Quarterly changes in US consumption\n    and personal income\")\n(fit <- auto.arima(uschange[,\"Consumption\"],\n  xreg=uschange[,\"Income\"]))\n#> Series: uschange[, \"Consumption\"] \n#> Regression with ARIMA(1,0,2) errors \n#> \n#> Coefficients:\n#>         ar1     ma1    ma2  intercept   xreg\n#>       0.692  -0.576  0.198      0.599  0.203\n#> s.e.  0.116   0.130  0.076      0.088  0.046\n#> \n#> sigma^2 = 0.322:  log likelihood = -156.9\n#> AIC=325.9   AICc=326.4   BIC=345.3\n369",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 654,
      "total_chunks": 873
    }
  },
  {
    "text": "The data are clearly already stationary (as we are considering percentage changes\nrather than raw expenditure and income), so there is no need for any differencing.\nThe fitted model is\nWe can recover estimates of both the  and  series using the ıresiduals()ı\nfunction.\nFigure 9.2: Regression errors ( ) and ARIMA errors ( ) from the fitted model.\nIt is the ARIMA errors that should resemble a white noise series.\n/g35 /g30 /g1E /g11/gF/g16/g1A/g1A /gC /g11/gF/g13/g11/g14/g34 /g30 /gC /g49 /g30 /gD\n/g49 /g30 /g1E /g11/gF/g17/g1A/g13/g49 /g30/gC3/g12 /gC /g47 /g30 /gC3 /g11/gF/g16/g18/g17/g47 /g30/gC3/g12 /gC /g11/gF/g12/g1A/g19/g47 /g30/gC3/g13 /gD\n/g47 /g30 /gD5 /g2F/g2A/g25/g9/g11/gD /g11/gF/g14/g13/g13/gA/gF\n/g49 /g30 /g47 /g30\ncbind(\"Regression Errors\" = residuals(fit, type=\"regression\"),\n      \"ARIMA errors\" = residuals(fit, type=\"innovation\")) %>%\n  autoplot(facets=TRUE)\n/g49 /g30 /g47 /g30\ncheckresiduals(fit)\n370",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 655,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.3: The residuals (i.e., the ARIMA errors) are not significantly different from white\nnoise.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from Regression with ARIMA(1,0,2) errors\n#> Q* = 5.89, df = 5, p-value = 0.32\n#> \n#> Model df: 3.   Total lags used: 8\n371",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 656,
      "total_chunks": 873
    }
  },
  {
    "text": "9.39.3  ForecastingForecasting\nTo forecast using a regression model with ARIMA errors, we need to forecast the\nregression part of the model and the ARIMA part of the model, and combine the\nresults. As with ordinary regression models, in order to obtain forecasts we first need\nto forecast the predictors. When the predictors are known into the future (e.g.,\ncalendar-related variables such as time, day-of-week, etc.), this is straightforward.\nBut when the predictors are themselves unknown, we must either model them\nseparately, or use assumed future values for each predictor.\nExample: US Personal Consumption and IncomeExample: US Personal Consumption and Income\nWe will calculate forecasts for the next eight quarters assuming that the future\npercentage changes in personal disposable income will be equal to the mean\npercentage change from the last forty years.\nFigure 9.4: Forecasts obtained from regressing the percentage change in consumption",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 657,
      "total_chunks": 873
    }
  },
  {
    "text": "percentage change from the last forty years.\nFigure 9.4: Forecasts obtained from regressing the percentage change in consumption\nexpenditure on the percentage change in disposable income, with an ARIMA(1,0,2) error\nmodel.\nfcast <- forecast(fit, xreg=rep(mean(uschange[,2]),8))\nautoplot(fcast) + xlab(\"Year\") +\n  ylab(\"Percentage change\")\n372",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 658,
      "total_chunks": 873
    }
  },
  {
    "text": "The prediction intervals for this model are narrower than those for the model\ndeveloped in Section 8.5  because we are now able to explain some of the variation in\nthe data using the income predictor.\nIt is important to realise that the prediction intervals from regression models (with\nor without ARIMA errors) do not take into account the uncertainty in the forecasts of\nthe predictors. So they should be interpreted as being conditional on the assumed (or\nestimated) future values of the predictor variables.\nExample: Forecasting electricity demandExample: Forecasting electricity demand\nDaily electricity demand can be modelled as a function of temperature. As can be\nobserved on an electricity bill, more electricity is used on cold days due to heating\nand hot days due to air conditioning. The higher demand on cold and hot days is\nreflected in the u-shape of Figure 9.5, where daily demand is plotted versus daily\nmaximum temperature.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 659,
      "total_chunks": 873
    }
  },
  {
    "text": "and hot days due to air conditioning. The higher demand on cold and hot days is\nreflected in the u-shape of Figure 9.5, where daily demand is plotted versus daily\nmaximum temperature.\nFigure 9.5: Daily electricity demand versus maximum daily temperature for the state of\nVictoria in Australia for 2014.\nThe data are stored as ıelecdailyı including total daily demand, an indicator variable\nfor workdays (a workday is represented with 1, and a non-workday is represented\nwith 0), and daily maximum temperatures. Because there is weekly seasonality, the\n373",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 660,
      "total_chunks": 873
    }
  },
  {
    "text": "frequency has been set to 7. Figure 9.6 shows the time series of both daily demand\nand daily maximum temperatures. The plots highlight the need for both a non-linear\nand a dynamic model.\nFigure 9.6: Daily electricity demand and maximum daily temperature for the state of\nVictoria in Australia for 2014.\nIn this example, we fit a quadratic regression model with ARMA errors using the\nıauto.arima()ı function.\nxreg <- cbind(MaxTemp = elecdaily[, \"Temperature\"],\n              MaxTempSq = elecdaily[, \"Temperature\"] ^2,\n              Workday = elecdaily[, \"WorkDay\"])\nfit <- auto.arima(elecdaily[, \"Demand\"], xreg = xreg)\ncheckresiduals(fit)\n374",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 661,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.7: Residuals diagnostics for a dynamic regression model for daily electricity\ndemand with workday and quadratic temperature effects.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from Regression with ARIMA(2,1,2)(2,0,0)[7] errors\n#> Q* = 28.2, df = 8, p-value = 0.00043\n#> \n#> Model df: 6.   Total lags used: 14\nThe model has some significant autocorrelation in the residuals, which means the\nprediction intervals may not provide accurate coverage. Also, the histogram of the\nresiduals shows one positive outlier, which will also affect the coverage of the\nprediction intervals.\nUsing the estimated model we forecast 14 days ahead starting from Thursday 1\nJanuary 2015 (a non-work-day being a public holiday for New Years Day). In this\ncase, we could obtain weather forecasts from the weather bureau for the next 14\ndays. But for the sake of illustration, we will use scenario based forecasting (as\nintroduced in Section 5.6) where we set the temperature for the next 14 days to a",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 662,
      "total_chunks": 873
    }
  },
  {
    "text": "days. But for the sake of illustration, we will use scenario based forecasting (as\nintroduced in Section 5.6) where we set the temperature for the next 14 days to a\nconstant 26 degrees.\n375",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 663,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.8: Forecasts from the dynamic regression model for daily electricity demand. All\nfuture temperatures have been set to 26 degrees, and the working day dummy variable\nhas been set to known future values.\nThe point forecasts look reasonable for the first two weeks of 2015. The slow down in\nelectricity demand at the end of 2014 (due to many people taking summer vacations)\nhas caused the forecasts for the next two weeks to show similarly low demand\nvalues.\nfcast <- forecast(fit,\n  xreg = cbind(MaxTemp=rep(26,14), MaxTempSq=rep(26^2,14),\n    Workday=c(0,1,0,0,1,1,1,1,1,0,0,1,1,1)))\nautoplot(fcast) + ylab(\"Electricity demand (GW)\" )\n376",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 664,
      "total_chunks": 873
    }
  },
  {
    "text": "9.49.4  Stochastic and deterministic trendsStochastic and deterministic trends\nThere are two different ways of modelling a linear trend. A deterministic trend  is\nobtained using the regression model\nwhere  is an ARMA process. A stochastic trend  is obtained using the model\nwhere  is an ARIMA process with . In the latter case, we can difference both\nsides so that , where  is an ARMA process. In other words,\nThis is similar to a random walk with drift (introduced in Section 8.1), but here the\nerror term is an ARMA process rather than simply white noise.\nAlthough these models appear quite similar (they only differ in the number of\ndifferences that need to be applied to ), their forecasting characteristics are quite\ndifferent.\nExample: International visitors to AustraliaExample: International visitors to Australia\nautoplot(austa) + xlab(\"Year\") +\n  ylab(\"millions of people\") +\n  ggtitle(\"Total annual international visitors to Australia\" )\n377",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 665,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.9: Annual international visitors to Australia, 1980–2015.\nFigure 9.9 shows the total number of international visitors to Australia each year\nfrom 1980 to 2015. We will fit both a deterministic and a stochastic trend model to\nthese data.\nThe deterministic trend model is obtained as follows:\nThis model can be written as\nThe estimated growth in visitor numbers is 0.17 million people per year.\ntrend <- seq_along(austa)\n(fit1 <- auto.arima(austa, d=0 , xreg=trend))\n#> Series: austa \n#> Regression with ARIMA(2,0,0) errors \n#> \n#> Coefficients:\n#>         ar1     ar2  intercept   xreg\n#>       1.113  -0.380      0.416  0.171\n#> s.e.  0.160   0.158      0.190  0.009\n#> \n#> sigma^2 = 0.0298:  log likelihood = 13.6\n#> AIC=-17.2   AICc=-15.2   BIC=-9.28\n/g35 /g30 /g1E /g11/gF/g15/g12/g17 /gC /g11/gF/g12/g18/g12/g30 /gC /g49 /g30\n/g49 /g30 /g1E /g12/gF/g12/g12/g14/g49 /g30/gC3/g12 /gC3 /g11/gF/g14/g19/g11/g49 /g30/gC3/g13 /gC /g47 /g30",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 666,
      "total_chunks": 873
    }
  },
  {
    "text": "/g35 /g30 /g1E /g11/gF/g15/g12/g17 /gC /g11/gF/g12/g18/g12/g30 /gC /g49 /g30\n/g49 /g30 /g1E /g12/gF/g12/g12/g14/g49 /g30/gC3/g12 /gC3 /g11/gF/g14/g19/g11/g49 /g30/gC3/g13 /gC /g47 /g30\n/g47 /g30 /gD5 /g2F/g2A/g25/g9/g11/gD /g11/gF/g11/g14/g11/gA/gF\n378",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 667,
      "total_chunks": 873
    }
  },
  {
    "text": "Alternatively, the stochastic trend model can be estimated.\nThis model can be written as , or equivalently\nIn this case, the estimated growth in visitor numbers is also 0.17 million people per\nyear. Although the growth estimates are similar, the prediction intervals are not, as\nFigure 9.10 shows. In particular, stochastic trends have much wider prediction\nintervals because the errors are non-stationary.\n(fit2 <- auto.arima(austa, d=1 ))\n#> Series: austa \n#> ARIMA(0,1,1) with drift \n#> \n#> Coefficients:\n#>         ma1  drift\n#>       0.301  0.173\n#> s.e.  0.165  0.039\n#> \n#> sigma^2 = 0.0338:  log likelihood = 10.62\n#> AIC=-15.24   AICc=-14.46   BIC=-10.57\n/g35 /g30 /gC3 /g35 /g30/gC3/g12 /g1E /g11/gF/g12/g18/g14 /gC /g49 /g9B\n/g30\n/g35 /g30 /g1E /g35 /g11 /gC /g11/gF/g12/g18/g14/g30 /gC /g49 /g30\n/g49 /g30 /g1E /g49 /g30/gC3/g12 /gC /g11/gF/g14/g11/g12/g47 /g30/gC3/g12 /gC /g47 /g30\n/g47 /g30 /gD5 /g2F/g2A/g25/g9/g11/gD /g11/gF/g11/g14/g15/gA/gF\nfc1 <- forecast(fit1,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 668,
      "total_chunks": 873
    }
  },
  {
    "text": "/g49 /g30 /g1E /g49 /g30/gC3/g12 /gC /g11/gF/g14/g11/g12/g47 /g30/gC3/g12 /gC /g47 /g30\n/g47 /g30 /gD5 /g2F/g2A/g25/g9/g11/gD /g11/gF/g11/g14/g15/gA/gF\nfc1 <- forecast(fit1,\n  xreg = length(austa) + 1:10)\nfc2 <- forecast(fit2, h=10)\nautoplot(austa) +\n  autolayer(fc2, series=\"Stochastic trend\" ) +\n  autolayer(fc1, series=\"Deterministic trend\") +\n  ggtitle(\"Forecasts from trend models\" ) +\n  xlab(\"Year\") + ylab(\"Visitors to Australia (millions)\" ) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n379",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 669,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.10: Forecasts of annual international visitors to Australia using a deterministic\ntrend model and a stochastic trend model.\nThere is an implicit assumption with deterministic trends that the slope of the trend\nis not going to change over time. On the other hand, stochastic trends can change,\nand the estimated growth is only assumed to be the average growth over the\nhistorical period, not necessarily the rate of growth that will be observed into the\nfuture. Consequently, it is safer to forecast with stochastic trends, especially for\nlonger forecast horizons, as the prediction intervals allow for greater uncertainty in\nfuture growth.\n380",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 670,
      "total_chunks": 873
    }
  },
  {
    "text": "9.59.5  Dynamic harmonic regressionDynamic harmonic regression\nWhen there are long seasonal periods, a dynamic regression with Fourier terms is\noften better than other models we have considered in this book.\nFor example, daily data can have annual seasonality of length 365, weekly data has\nseasonal period of approximately 52, while half-hourly data can have several\nseasonal periods, the shortest of which is the daily pattern of period 48.\nSeasonal versions of ARIMA and ETS models are designed for shorter periods such as\n12 for monthly data or 4 for quarterly data. The ıets()ı function restricts seasonality\nto be a maximum period of 24 to allow hourly data but not data with a larger\nseasonal frequency. The problem is that there are  parameters to be estimated\nfor the initial seasonal states where  is the seasonal period. So for large , the\nestimation becomes almost impossible.\nThe ıArima()ı and ıauto.arima()ı functions will allow a seasonal period up to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 671,
      "total_chunks": 873
    }
  },
  {
    "text": "for the initial seasonal states where  is the seasonal period. So for large , the\nestimation becomes almost impossible.\nThe ıArima()ı and ıauto.arima()ı functions will allow a seasonal period up to\n, but in practice will usually run out of memory whenever the seasonal\nperiod is more than about 200. In any case, seasonal differencing of high order does\nnot make a lot of sense — for daily data it involves comparing what happened today\nwith what happened exactly a year ago and there is no constraint that the seasonal\npattern is smooth.\nSo for such time series, we prefer a harmonic regression approach where the\nseasonal pattern is modelled using Fourier terms with short-term time series\ndynamics handled by an ARMA error.\nThe advantages of this approach are:\nit allows any length seasonality;\nfor data with more than one seasonal period, Fourier terms of different\nfrequencies can be included;\nthe smoothness of the seasonal pattern can be controlled by , the number of",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 672,
      "total_chunks": 873
    }
  },
  {
    "text": "for data with more than one seasonal period, Fourier terms of different\nfrequencies can be included;\nthe smoothness of the seasonal pattern can be controlled by , the number of\nFourier sin and cos pairs – the seasonal pattern is smoother for smaller values of\n;\nthe short-term dynamics are easily handled with a simple ARMA error.\n20\n381",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 673,
      "total_chunks": 873
    }
  },
  {
    "text": "The only real disadvantage (compared to a seasonal ARIMA model) is that the\nseasonality is assumed to be fixed — the seasonal pattern is not allowed to change\nover time. But in practice, seasonality is usually remarkably constant so this is not a\nbig disadvantage except for long time series.\nExample: Australian eating out expenditureExample: Australian eating out expenditure\nIn this example we demonstrate combining Fourier terms for capturing seasonality\nwith ARIMA errors capturing other dynamics in the data. For simplicity, we will use\nan example with monthly data. The same modelling approach using weekly data is\ndiscussed in Section 12.1.\nWe use ıauscafeı, the total monthly expenditure on cafes, restaurants and takeaway\nfood services in Australia ($billion), starting in 2004 up to November 2016 and we\nforecast 24 months ahead. We vary , the number of Fourier sin and cos pairs, from\n to  (which is equivalent to including seasonal dummies). Figure 9.11",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 674,
      "total_chunks": 873
    }
  },
  {
    "text": "forecast 24 months ahead. We vary , the number of Fourier sin and cos pairs, from\n to  (which is equivalent to including seasonal dummies). Figure 9.11\nshows the seasonal pattern projected forward as  increases. Notice that as \nincreases the Fourier terms capture and project a more “wiggly” seasonal pattern\nand simpler ARIMA models are required to capture other dynamics. The AICc value is\nminimised for , with a significant jump going from  to , hence\nthe forecasts generated from this model would be the ones used.\ncafe04 <- window(auscafe, start=2004 )\nplots <- list()\nfor (i in seq(6)) {\n  fit <- auto.arima(cafe04, xreg = fourier(cafe04, K = i),\n    seasonal = FALSE, lambda = 0)\n  plots[[i]] <- autoplot(forecast(fit,\n      xreg=fourier(cafe04, K=i, h=24))) +\n    xlab(paste(\"K=\",i,\"   AICC=\",round(fit[[\"aicc\"]],2))) +\n    ylab(\"\") + ylim(1.5,4.7)\n}\ngridExtra::grid.arrange(\n  plots[[1]],plots[[2]],plots[[3]],\n  plots[[4]],plots[[5]],plots[[6]], nrow=3)\n382",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 675,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.11: Using Fourier terms and ARIMA errors for forecasting monthly expenditure on\neating out in Australia.\nBibliographyBibliography\nYoung, P. C., Pedregal, D. J., & Tych, W. (1999). Dynamic harmonic regression.\nJournal of Forecasting , 18, 369–394. [DOI]\n20. The term “dynamic harmonic regression” is also used for a harmonic regression\nwith time-varying parameters ( Young, Pedregal, & Tych, 1999 ).↩ \n383",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 676,
      "total_chunks": 873
    }
  },
  {
    "text": "9.69.6  Lagged predictorsLagged predictors\nSometimes, the impact of a predictor which is included in a regression model will not\nbe simple and immediate. For example, an advertising campaign may impact sales\nfor some time beyond the end of the campaign, and sales in one month will depend\non the advertising expenditure in each of the past few months. Similarly, a change in\na company’s safety policy may reduce accidents immediately, but have a diminishing\neffect over time as employees take less care when they become familiar with the new\nworking conditions.\nIn these situations, we need to allow for lagged effects of the predictor. Suppose that\nwe have only one predictor in our model. Then a model which allows for lagged\neffects can be written as\nwhere  is an ARIMA process. The value of  can be selected using the AICc, along\nwith the values of  and  for the ARIMA error.\nExample: TV advertising and insurance quotations",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 677,
      "total_chunks": 873
    }
  },
  {
    "text": "where  is an ARIMA process. The value of  can be selected using the AICc, along\nwith the values of  and  for the ARIMA error.\nExample: TV advertising and insurance quotations\nA US insurance company advertises on national television in an attempt to increase\nthe number of insurance quotations provided (and consequently the number of new\npolicies). Figure 9.12 shows the number of quotations and the expenditure on\ntelevision advertising for the company each month from January 2002 to April 2005.\nautoplot(insurance, facets=TRUE ) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"Insurance advertising and quotations\" )\n384",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 678,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 9.12: Numbers of insurance quotations provided per month and the expenditure on\nadvertising per month.\nWe will consider including advertising expenditure for up to four months; that is, the\nmodel may include advertising expenditure in the current month, and the three\nmonths before that. When comparing models, it is important that they all use the\nsame training set. In the following code, we exclude the first three months in order\nto make fair comparisons.\n385",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 679,
      "total_chunks": 873
    }
  },
  {
    "text": "Next we choose the optimal lag length for advertising based on the AICc.\nThe best model (with the smallest AICc value) has two lagged predictors; that is, it\nincludes advertising only in the current month and the previous month. So we now\nre-estimate that model, but using all the available data.\n# Lagged predictors. Test 0, 1, 2 or 3 lags.\nAdvert <- cbind(\n    AdLag0 = insurance[,\"TV.advert\" ],\n    AdLag1 = stats::lag(insurance[,\"TV.advert\"],- 1),\n    AdLag2 = stats::lag(insurance[,\"TV.advert\"],- 2),\n    AdLag3 = stats::lag(insurance[,\"TV.advert\"],- 3)) %>%\n  head(NROW(insurance))\n# Restrict data so models use same fitting period\nfit1 <- auto.arima(insurance[4 :40,1], xreg=Advert[4:40,1],\n  stationary=TRUE)\nfit2 <- auto.arima(insurance[4 :40,1], xreg=Advert[4:40,1:2],\n  stationary=TRUE)\nfit3 <- auto.arima(insurance[ 4:40,1], xreg=Advert[4:40,1:3],\n  stationary=TRUE)\nfit4 <- auto.arima(insurance[ 4:40,1], xreg=Advert[4:40,1:4],\n  stationary=TRUE)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 680,
      "total_chunks": 873
    }
  },
  {
    "text": "stationary=TRUE)\nfit3 <- auto.arima(insurance[ 4:40,1], xreg=Advert[4:40,1:3],\n  stationary=TRUE)\nfit4 <- auto.arima(insurance[ 4:40,1], xreg=Advert[4:40,1:4],\n  stationary=TRUE)\nc(fit1[[\"aicc\"]],fit2[[\"aicc\"]],fit3[[\"aicc\"]],fit4[[\"aicc\"]])\n#> [1] 68.500 60.024 62.833 65.457\n(fit <- auto.arima(insurance[,1 ], xreg=Advert[,1:2],\n  stationary=TRUE))\n#> Series: insurance[, 1] \n#> Regression with ARIMA(3,0,0) errors \n#> \n#> Coefficients:\n#>         ar1     ar2    ar3  intercept  AdLag0  AdLag1\n#>       1.412  -0.932  0.359      2.039   1.256   0.162\n#> s.e.  0.170   0.255  0.159      0.993   0.067   0.059\n#> \n#> sigma^2 = 0.217:  log likelihood = -23.89\n#> AIC=61.78   AICc=65.4   BIC=73.43\n386",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 681,
      "total_chunks": 873
    }
  },
  {
    "text": "The chosen model has AR(3) errors. The model can be written as\nwhere  is the number of quotations provided in month ,  is the advertising\nexpenditure in month ,\nand  is white noise.\nWe can calculate forecasts using this model if we assume future values for the\nadvertising variable. If we set the future monthly advertising to 8 units, we get the\nforecasts in Figure 9.13.\nFigure 9.13: Forecasts of monthly insurance quotes, assuming that the future advertising\nexpenditure is 8 units in each future month.\n/g35 /g30 /g1E /g13/gF/g11/g14/g1A /gC /g12/gF/g13/g16/g17/g34 /g30 /gC /g11/gF/g12/g17/g13/g34 /g30/gC3/g12 /gC /g49 /g30 /gD\n/g35 /g30 /g30/g34 /g30\n/g30\n/g49 /g30 /g1E /g12/gF/g15/g12/g13/g49 /g30/gC3/g12 /gC3 /g11/gF/g1A/g14/g13/g49 /g30/gC3/g13 /gC /g11/gF/g14/g16/g1A/g49 /g30/gC3/g14 /gC /g47 /g30 /gD\n/g47 /g30\nfc8 <- forecast(fit, h=20,\n  xreg=cbind(AdLag0 = rep(8,20),\n             AdLag1 = c(Advert[40,1], rep(8,19))))\nautoplot(fc8) + ylab(\"Quotes\") +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 682,
      "total_chunks": 873
    }
  },
  {
    "text": "/g47 /g30\nfc8 <- forecast(fit, h=20,\n  xreg=cbind(AdLag0 = rep(8,20),\n             AdLag1 = c(Advert[40,1], rep(8,19))))\nautoplot(fc8) + ylab(\"Quotes\") +\n  ggtitle(\"Forecast quotes with future advertising set to 8\" )\n387",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 683,
      "total_chunks": 873
    }
  },
  {
    "text": "9.79.7  ExercisesExercises\n1. Consider monthly sales and advertising data for an automotive parts company\n(data set ıadvertı).\na. Plot the data using ıautoplotı. Why is it useful to set ıfacets=TRUEı?\nb. Fit a standard regression model  where  denotes sales\nand  denotes advertising using the ıtslm()ı function.\nc. Show that the residuals have significant autocorrelation.\nd. What difference does it make you use the ıArimaı function instead:\ne. Refit the model using ıauto.arima()ı. How much difference does the error\nmodel make to the estimated parameters? What ARIMA model for the errors\nis selected?\nf. Check the residuals of the fitted model.\ng. Assuming the advertising budget for the next six months is exactly 10 units\nper month, produce and plot sales forecasts with prediction intervals for the\nnext six months.\n2. This exercise uses data set ıhuronı giving the level of Lake Huron from 1875–\n1972.\na. Fit a piecewise linear trend model to the Lake Huron data with a knot at 1920",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 684,
      "total_chunks": 873
    }
  },
  {
    "text": "next six months.\n2. This exercise uses data set ıhuronı giving the level of Lake Huron from 1875–\n1972.\na. Fit a piecewise linear trend model to the Lake Huron data with a knot at 1920\nand an ARMA error structure.\nb. Forecast the level for the next 30 years.\n3. This exercise concerns ımotelı: the total monthly takings from accommodation\nand the total room nights occupied at hotels, motels, and guest houses in\nVictoria, Australia, between January 1980 and June 1995. Total monthly takings\nare in thousands of Australian dollars; total room nights occupied are in\nthousands.\nArima(advert[,\"sales\"], xreg= advert[,\"advert\"],\n  order=c(0,0,0))\n388",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 685,
      "total_chunks": 873
    }
  },
  {
    "text": "a. Use the data to calculate the average cost of a night’s accommodation in\nVictoria each month.\nb. Use ıcpimelı to estimate the monthly CPI.\nc. Produce time series plots of both variables and explain why logarithms of\nboth variables need to be taken before fitting any models.\nd. Fit an appropriate regression model with ARIMA errors. Explain your\nreasoning in arriving at the final model.\ne. Forecast the average price per room for the next twelve months using your\nfitted model. (Hint: You will need to produce forecasts of the CPI figures\nfirst.)\n4. We fitted a harmonic regression model to part of the ıgasolineı series in Exercise\n6 in Section 5.10. We will now revisit this model, and extend it to include more\ndata and ARMA errors.\na. Using ıtslm()ı, fit a harmonic regression with a piecewise linear time trend\nto the full ıgasolineı series. Select the position of the knots in the trend and\nthe appropriate number of Fourier terms to include by minimising the AICc\nor CV value.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 686,
      "total_chunks": 873
    }
  },
  {
    "text": "to the full ıgasolineı series. Select the position of the knots in the trend and\nthe appropriate number of Fourier terms to include by minimising the AICc\nor CV value.\nb. Now refit the model using ıauto.arima()ı to allow for correlated errors,\nkeeping the same predictor variables as you used with ıtslm()ı.\nc. Check the residuals of the final model using the ıcheckresiduals()ı function.\nDo they look sufficiently like white noise to continue? If not, try modifying\nyour model, or removing the first few years of data.\nd. Once you have a model with white noise residuals, produce forecasts for the\nnext year.\n5. Electricity consumption is often modelled as a function of temperature.\nTemperature is measured by daily heating degrees and cooling degrees. Heating\ndegrees is C minus the average daily temperature when the daily average is\nbelow C; otherwise it is zero. This provides a measure of our need to heat\nourselves as temperature falls. Cooling degrees measures our need to cool",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 687,
      "total_chunks": 873
    }
  },
  {
    "text": "below C; otherwise it is zero. This provides a measure of our need to heat\nourselves as temperature falls. Cooling degrees measures our need to cool\nourselves as the temperature rises. It is defined as the average daily temperature\nminus C when the daily average is above C; otherwise it is zero. Let \ndenote the monthly total of kilowatt-hours of electricity used, let  denote the\nmonthly total of heating degrees, and let  denote the monthly total of cooling\ndegrees.\n/g12/g19 /gC8\n/g12/g19 /gC8\n/g12/g19 /gC8 /g12/g19 /gC8 /g35 /g30\n/g34 /g12/gD /g30\n/g34 /g13/gD /g30\n389",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 688,
      "total_chunks": 873
    }
  },
  {
    "text": "An analyst fits the following model to a set of such data:\nwhere\nand ,  and .\na. What sort of ARIMA model is identified for ?\nb. The estimated coefficients are\nParameterParameter EstimateEstimate s.e.s.e. -value-value\n0.0077 0.0015 4.98 0.000\n0.0208 0.0023 9.23 0.000\n0.5830 0.0720 8.10 0.000\n-0.5373 0.0856 -6.27 0.000\n-0.4667 0.0862 -5.41 0.000\nExplain what the estimates of  and  tell us about electricity consumption.\nc. Write the equation in a form more suitable for forecasting.\nd. Describe how this model could be used to forecast electricity demand for the\nnext 12 months.\ne. Explain why the  term should be modelled with an ARIMA model rather\nthan modelling the data using a standard regression package. In your\ndiscussion, comment on the properties of the estimates, the validity of the\nstandard regression results, and the importance of the  model in producing\nforecasts.\n6. For the retail time series considered in earlier chapters:",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 689,
      "total_chunks": 873
    }
  },
  {
    "text": "standard regression results, and the importance of the  model in producing\nforecasts.\n6. For the retail time series considered in earlier chapters:\na. Develop an appropriate dynamic regression model with Fourier terms for the\nseasonality. Use the AIC to select the number of Fourier terms to include in\nthe model. (You will probably need to use the same Box-Cox transformation\nyou identified previously.)\nb. Check the residuals of the fitted model. Does the residual series look like\nwhite noise?\nc. Compare the forecasts with those you obtained earlier using alternative\nmodels.\n390",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 690,
      "total_chunks": 873
    }
  },
  {
    "text": "391\n\n\n9.89.8  Further readingFurther reading\nA detailed discussion of dynamic regression models is provided in Pankratz\n(1991).\nA generalisation of dynamic regression models, known as “transfer function\nmodels”, is discussed in Box et al. ( 2015).\nBibliographyBibliography\nBox, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time series\nanalysis: Forecasting and control  (5th ed). Hoboken, New Jersey: John Wiley &\nSons. [Amazon]\nPankratz, A. E. (1991). Forecasting with dynamic regression models . New York,\nUSA: John Wiley & Sons. [Amazon]\n392",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 691,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 10Chapter 10  Forecasting hierarchical orForecasting hierarchical or\ngroupedßtime seriesgroupedßtime series\nWarning: this is a more advanced chapter and assumes a knowledge of some basic\nmatrix algebra.\nTime series can often be naturally disaggregated by various attributes of interest. For\nexample, the total number of bicycles sold by a cycling manufacturer can be\ndisaggregated by product type such as road bikes, mountain bikes, children’s bikes\nand hybrids. Each of these can be disaggregated into finer categories. For example\nhybrid bikes can be divided into city, commuting, comfort, and trekking bikes; and\nso on. These categories are nested within the larger group categories, and so the\ncollection of time series follow a hierarchical aggregation structure. Therefore we\nrefer to these as “hierarchical time series”, the topic of Section 10.1.\nHierarchical time series often arise due to geographic divisions. For example, the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 692,
      "total_chunks": 873
    }
  },
  {
    "text": "refer to these as “hierarchical time series”, the topic of Section 10.1.\nHierarchical time series often arise due to geographic divisions. For example, the\ntotal bicycle sales can be disaggregated by country, then within each country by\nstate, within each state by region, and so on down to the outlet level.\nOur bicycle manufacturer may disaggregate sales by both product type and by\ngeographic location. Then we have a more complicated aggregation structure where\nthe product hierarchy and the geographic hierarchy can both be used together. We\nusually refer to these as “grouped time series”, and discuss them in Section 10.2.\nIt is common to produce disaggregated forecasts based on disaggregated time series,\nand we usually require the forecasts to add up in the same way as the data. For\nexample, forecasts of regional sales should add up to give forecasts of state sales,\nwhich should in turn add up to give a forecast for the national sales.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 693,
      "total_chunks": 873
    }
  },
  {
    "text": "example, forecasts of regional sales should add up to give forecasts of state sales,\nwhich should in turn add up to give a forecast for the national sales.\nIn this chapter we discuss forecasting large collections of time series that must add\nup in some way. The challenge is that we require forecasts that are coherentcoherent across\nthe aggregation structure. That is, we require forecasts to add up in a manner that is\n393",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 694,
      "total_chunks": 873
    }
  },
  {
    "text": "consistent with the aggregation structure of the collection of time series. In Sections\n10.3– 10.7 we discuss several methods for producing coherent forecasts for both\nhierarchical and grouped time series.\n394",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 695,
      "total_chunks": 873
    }
  },
  {
    "text": "10.110.1  Hierarchical time seriesHierarchical time series\nFigure 10.1  shows a -level hierarchical structure. At the top of the hierarchy\n(which we call level 0) is the “Total”, the most aggregate level of the data. The th\nobservation of the Total series is denoted by  for . The Total is\ndisaggregated into two series at level 1, which in turn are divided into three and two\nseries respectively at the bottom-level of the hierarchy. Below the top level, we use\n to denote the th observation of the series corresponding to node . For example,\n denotes the th observation of the series corresponding to node A at level 1, \ndenotes the th observation of the series corresponding to node AB at level 2, and so\non.\nFigure 10.1: A two level hierarchical tree diagram.\nIn this small example, the total number of series in the hierarchy is\n, while the number of series at the bottom-level is . Note\nthat  in all hierarchies.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 696,
      "total_chunks": 873
    }
  },
  {
    "text": "In this small example, the total number of series in the hierarchy is\n, while the number of series at the bottom-level is . Note\nthat  in all hierarchies.\nFor any time , the observations at the bottom-level of the hierarchy will sum to the\nobservations of the series above. For example,\nand\nSubstituting (10.2) into (10.1) , we also get . These equations can be\n395",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 697,
      "total_chunks": 873
    }
  },
  {
    "text": "thought of as aggregation constraints or summing equalities, and can be more\nefficiently represented using matrix notation. We construct an  matrix \n(referred to as the “summing matrix”) which dictates the way in which the bottom-\nlevel series are aggregated.\nFor the hierarchical structure in Figure 10.1, we can write\nor in more compact notation\nwhere  is an -dimensional vector of all the observations in the hierarchy at time \n,  is the summing matrix, and  is an -dimensional vector of all the observations\nin the bottom-level of the hierarchy at time . Note that the first row in the summing\nmatrix  represents Equation (10.1) above, the second and third rows represent\n(10.2). The rows below these comprise an -dimensional identity matrix  so that\neach bottom-level observation on the right hand side of the equation is equal to\nitself on the left hand side.\nExample: Australian tourism hierarchyExample: Australian tourism hierarchy",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 698,
      "total_chunks": 873
    }
  },
  {
    "text": "each bottom-level observation on the right hand side of the equation is equal to\nitself on the left hand side.\nExample: Australian tourism hierarchyExample: Australian tourism hierarchy\nAustralia is divided into eight geographic areas (some called states and others called\nterritories) with each one having its own government and some economic and\nadministrative autonomy. Each of these can be further subdivided into smaller areas\nof interest, referred to as zones. Business planners and tourism authorities are\ninterested in forecasts for the whole of Australia, for the states and the territories,\nand also for the zones. In this example we concentrate on quarterly domestic\ntourism demand, measured as the number of visitor nights Australians spend away\nfrom home. To simplify our analysis, we combine the two territories and Tasmania\n396",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 699,
      "total_chunks": 873
    }
  },
  {
    "text": "into an “Other” state. So we have six states: New South Wales (NSW), Queensland\n(QLD), South Australia (SAU), Victoria (VIC), Western Australia (WAU) and Other\n(OTH). For each of these we consider visitor nights within the following zones.\nStateState ZonesZones\nNSW\nMetro (NSWMetro), North Coast (NSWNthCo), South Coast (NSWSthCo), South\nInner (NSWSthIn), North Inner (NSWNthIn)\nQLD Metro (QLDMetro), Central (QLDCntrl), North Coast (QLDNthCo)\nSAU Metro (SAUMetro), Coastal (SAUCoast), Inner (SAUInner)\nVIC Metro (VICMetro), West Coast (VICWstCo), East Coast (VICEstCo), Inner (VICInner)\nWAU Metro (WAUMetro), Coastal (WAUCoast), Inner (WAUInner)\nOTH Metro (OTHMetro), Non-Metro (OTHNoMet)\nWe consider five zones for NSW, four zones for VIC, and three zones each for QLD,\nSAU and WAU. Note that Metro zones contain the capital cities and surrounding\nareas. For further details on these geographic areas, please refer to Appendix C in\nWickramasuriya, Athanasopoulos, & Hyndman (2019 ).",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 700,
      "total_chunks": 873
    }
  },
  {
    "text": "areas. For further details on these geographic areas, please refer to Appendix C in\nWickramasuriya, Athanasopoulos, & Hyndman (2019 ).\nTo create a hierarchical time series, we use the ıhts()ı function as shown in the code\nbelow. The function requires two inputs: the bottom-level time series and\ninformation about the hierarchical structure. ıvisnightsı is a time series matrix\ncontaining the bottom-level series. There are several ways to input the structure of\nthe hierarchy. In this case we are using the ıcharactersı argument. The first three\ncharacters of each column name of ıvisnightsı capture the categories at the first\nlevel of the hierarchy (States). The following five characters capture the bottom-\nlevel categories (Zones).\nlibrary(hts)\ntourism.hts <- hts(visnights, characters = c(3, 5))\ntourism.hts %>% aggts(levels=0:1) %>%\n  autoplot(facet=TRUE) +\n  xlab(\"Year\") + ylab(\"millions\") + ggtitle(\"Visitor nights\")\n397",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 701,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.2: Australian domestic visitor nights over the period 1998 Q1 to 2016 Q4\ndisaggregated by State.\nThe top plot in Figure 10.2 shows the total number of visitor nights for the whole of\nAustralia, while the plots below show the data disaggregated by state. These reveal\ndiverse and rich dynamics at the aggregate national level, and the first level of\ndisaggregation for each state. The ıaggts()ı function extracts time series from an\nıhtsı object for any level of aggregation.\nThe plots in Figure 10.3 show the bottom-level time series, namely the visitor nights\nfor each zone. These help us visualise the diverse individual dynamics within each\nzone, and assist in identifying unique and important time series. Notice, for\nexample, the coastal WAU zone which shows significant growth over the last few\nyears.\n398",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 702,
      "total_chunks": 873
    }
  },
  {
    "text": "library(tidyverse)\ncols <- sample(scales::hue_pal( h=c(15,375),\n  c=100,l=65,h.start=0,direction = 1)(NCOL(visnights)))\nas_tibble(visnights) %>%\n  gather(Zone) %>%\n  mutate(Date = rep(time(visnights), NCOL(visnights)),\n         State = str_sub(Zone,1,3)) %>%\n  ggplot(aes(x=Date, y=value, group=Zone, colour=Zone)) +\n    geom_line() +\n    facet_grid(State~., scales=\"free_y\") +\n    xlab(\"Year\") + ylab(\"millions\") +\n    ggtitle(\"Visitor nights by Zone\" ) +\n    scale_colour_manual(values = cols)\n399",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 703,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.3: Australian domestic visitor nights over the period 1998 Q1 to 2016 Q4\ndisaggregated by Zones.\nTo produce this figure, we are using various functions from the tidyversetidyverse collection\nof packages . The details are beyond the scope of this book, but there are many good\nonline resources available to learn how to use these packages.\nBibliographyBibliography\nWickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal\nforecast reconciliation for hierarchical and grouped time series through trace\nminimization. Journal of the American Statistical Association , 114(526), 804–\n819. [DOI]\n400\n\n\n401",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 704,
      "total_chunks": 873
    }
  },
  {
    "text": "10.210.2  Grouped time seriesGrouped time series\nGrouped time series involve more general aggregation structures than hierarchical\ntime series. With grouped time series, the structure does not naturally disaggregate\nin a unique hierarchical manner, and often the disaggregating factors are both\nnested and crossed. For example, we could further disaggregate all geographic levels\nof the Australian tourism data by purpose of travel (such as holidays, business, etc.).\nSo we could consider visitors nights split by purpose of travel for the whole of\nAustralia, and for each state, and for each zone. Then we describe the structure as\ninvolving the purpose of travel “crossed” with the geographic hierarchy.\nFigure 10.4  shows a -level grouped structure. At the top of the grouped\nstructure is the Total, the most aggregate level of the data, again represented by .\nThe Total can be disaggregated by attributes (A, B) forming series  and , or by",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 705,
      "total_chunks": 873
    }
  },
  {
    "text": "structure is the Total, the most aggregate level of the data, again represented by .\nThe Total can be disaggregated by attributes (A, B) forming series  and , or by\nattributes (X, Y) forming series  and . At the bottom level, the data are\ndisaggregated by both attributes.\nFigure 10.4: Alternative representations of a two level grouped structure.\nThis example shows that there are alternative aggregation paths for grouped\nstructures. For any time , as with the hierarchical structure,\nHowever, for the first level of the grouped structure,\n402",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 706,
      "total_chunks": 873
    }
  },
  {
    "text": "but also\nThese equalities can again be represented by the  summing matrix . The total\nnumber of series is  with  series at the bottom-level. For the grouped\nstructure in Figure 10.4  we write\nor\nwhere the second and third rows of  represent (10.4) and the fourth and fifth rows\nrepresent (10.5) .\nGrouped time series can sometimes be thought of as hierarchical time series that do\nnot impose a unique hierarchical structure, in the sense that the order by which the\nseries can be grouped is not unique.\nExample: Australian prison populationExample: Australian prison population\nThe top row of Figure 10.5 shows the total number of prisoners in Australia over the\nperiod 2005 Q1 to 2016 Q4. This represents the top-level series in the grouping\nstructure. The rest of the panels show the prison population disaggregated by (i)\nstate  (ii) legal status, whether prisoners have already been sentenced or are in\nremand waiting for a sentence, and (iii) gender. In this example, the three factors are",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 707,
      "total_chunks": 873
    }
  },
  {
    "text": "state  (ii) legal status, whether prisoners have already been sentenced or are in\nremand waiting for a sentence, and (iii) gender. In this example, the three factors are\ncrossed, but none are nested within the others.\n21\n403",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 708,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.5: Total Australian quarterly adult prison population, disaggregated by state, by\nlegal status and by gender.\nTo create a grouped time series, we use the ıgts()ı function. Similar to the ıhts()ı\nfunction, inputs to the ıgts()ı function are the bottom-level time series and\ninformation about the grouping structure. ıprisonı is a time series matrix containing\nthe bottom-level time series. The information about the grouping structure can be\npassed in using the ıcharactersı input. (An alternative is to be more explicit about\nthe labelling of the series and use the ıgroupsı input.)\nOne way to plot the main groups is as follows.\nBut with a little more work, we can construct Figure 10.5 using the following code.\nprison.gts <- gts(prison/1e3, characters = c(3,1,9),\n  gnames = c(\"State\", \"Gender\", \"Legal\",\n             \"State*Gender\", \"State*Legal\",\n             \"Gender*Legal\"))\nprison.gts %>% aggts(level=0:3) %>% autoplot()\n404",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 709,
      "total_chunks": 873
    }
  },
  {
    "text": "Plots of other group combinations can be obtained similarly. Figure 10.6 shows the\nAustralian prison population disaggregated by all possible combinations of two\nattributes at a time. The top plot shows the prison population disaggregated by state\nand legal status, the middle panel shows the disaggregation by state and gender and\nthe bottom panel shows the disaggregation by legal status and gender.\np1 <- prison.gts %>% aggts(level=0) %>%\n  autoplot() + ggtitle(\"Australian prison population\" ) +\n    xlab(\"Year\") + ylab(\"Total number of prisoners ('000)\")\ngroups <- aggts(prison.gts, level=1:3)\ncols <- sample(scales::hue_pal( h=c(15,375),\n          c=100,l=65,h.start=0,direction = 1)(NCOL(groups)))\np2 <- as_tibble(groups) %>%\n  gather(Series) %>%\n  mutate(Date = rep(time(groups), NCOL(groups)),\n         Group = str_extract(Series, \"([A-Za-z ]*)\")) %>%\n  ggplot(aes(x=Date, y=value, group=Series, colour=Series)) +\n    geom_line() +\n    xlab(\"Year\") + ylab(\"Number of prisoners ('000)\") +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 710,
      "total_chunks": 873
    }
  },
  {
    "text": "Group = str_extract(Series, \"([A-Za-z ]*)\")) %>%\n  ggplot(aes(x=Date, y=value, group=Series, colour=Series)) +\n    geom_line() +\n    xlab(\"Year\") + ylab(\"Number of prisoners ('000)\") +\n    scale_colour_manual(values = cols) +\n    facet_grid(.~Group, scales=\"free_y\") +\n    scale_x_continuous(breaks=seq(2006,2016,by=2)) +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\ngridExtra::grid.arrange(p1, p2, ncol= 1)\n405",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 711,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.6: Australian adult prison population disaggregated by pairs of attributes.\nFigure 10.7 shows the Australian adult prison population disaggregated by all three\nattributes: state, legal status and gender. These form the bottom-level series of the\ngrouped structure.\nFigure 10.7: Bottom-level time series for the Australian adult prison population, grouped\nby state, legal status and gender.\n21. Australia comprises eight geographic areas six states and two territories:\nAustralian Capital Territory, New South Wales, Northern Territory, Queensland,\nSouth Australia, Tasmania, Victoria, Western Australia. In this example we\n406\n\n\nconsider all eight areas. ↩ \n407",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 712,
      "total_chunks": 873
    }
  },
  {
    "text": "10.310.3  The bottom-up approachThe bottom-up approach\nA simple method for generating coherent forecasts is the bottom-up approach. This\napproach involves first generating forecasts for each series at the bottom-level, and\nthen summing these to produce forecasts for all the series in the structure.\nFor example, for the hierarchy of Figure 10.1 , we first generate -step-ahead\nforecasts for each of the bottom-level series:\n(We have simplified the previously used notation of  for brevity.) Summing\nthese, we get -step-ahead coherent forecasts for the rest of the series:\n(In this chapter, we will use the “tilde” notation to indicate coherent forecasts.) As\nin Equation (10.3), we can employ the summing matrix here and write\nUsing more compact notation, the bottom-up approach can be represented as\nwhere  is an -dimensional vector of coherent -step-ahead forecasts, and  is\nan -dimensional vector of -step-ahead forecasts for each of the bottom-level\nseries.\n408",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 713,
      "total_chunks": 873
    }
  },
  {
    "text": "An advantage of this approach is that we are forecasting at the bottom-level of a\nstructure, and therefore no information is lost due to aggregation. On the other\nhand, bottom-level data can be quite noisy and more challenging to model and\nforecast.\nThe hts package for RThe hts package for R\nForecasts can be produced using the ıforecast()ı function applied to objects created\nby ıhts()ı or ıgts()ı. The hts package has three in-built options to produce\nforecasts: ETS models, ARIMA models or random walks; these are controlled by the\nıfmethodı argument. It also use several methods for producing coherent forecasts,\ncontrolled by the ımethodı argument.\nFor example, suppose we wanted bottom-up forecasts using ARIMA models applied\nto the prison data. Then we would use\nwhich will apply the ıauto.arima()ı function to every bottom-level series in our\ncollection of time series. Similarly, ETS models would be used if ıfmethod=\"ets\"ı was\nused.\nforecast(prison.gts, method=\"bu\" , fmethod=\"arima\")",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 714,
      "total_chunks": 873
    }
  },
  {
    "text": "collection of time series. Similarly, ETS models would be used if ıfmethod=\"ets\"ı was\nused.\nforecast(prison.gts, method=\"bu\" , fmethod=\"arima\")\n409",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 715,
      "total_chunks": 873
    }
  },
  {
    "text": "10.410.4  Top-down approachesTop-down approaches\nTop-down approaches only work with strictly hierarchical aggregation structures,\nand not with grouped structures. They involve first generating forecasts for the Total\nseries , and then disaggregating these down the hierarchy.\nWe let  be a set of disaggregation proportions which dictate how the\nforecasts of the Total series are to be distributed to obtain forecasts for each series at\nthe bottom-level of the structure. For example, for the hierarchy of Figure 10.1 using\nproportions  we get,\nUsing matrix notation we can stack the set of proportions in a -dimensional vector\n and write\nOnce the bottom-level -step-ahead forecasts have been generated, these are\naggregated to generate coherent forecasts for the rest of the series. In general, for a\nspecified set of proportions, top-down approaches can be represented as\nThe two most common top-down approaches specify disaggregation proportions",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 716,
      "total_chunks": 873
    }
  },
  {
    "text": "specified set of proportions, top-down approaches can be represented as\nThe two most common top-down approaches specify disaggregation proportions\nbased on the historical proportions of the data. These performed well in the study of\nGross & Sohl (1990).\nAverage historical proportionsAverage historical proportions\nfor . Each proportion  reflects the average of the historical\nproportions of the bottom-level series  over the period  relative to\nthe total aggregate .\n410",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 717,
      "total_chunks": 873
    }
  },
  {
    "text": "This approach is implemented in the ıforecast()ı function by setting\nımethod=\"tdgsa\" ı, where ıtdgsaı stands for “top-down Gross-Sohl method A”.\nProportions of the historical averagesProportions of the historical averages\nfor . Each proportion  captures the average historical value of the\nbottom-level series  relative to the average value of the total aggregate .\nThis approach is implemented in the ıforecast()ı function by setting\nımethod=\"tdgsf\" ı, where ıtdgsfı stands for “top-down Gross-Sohl method F”.\nA convenient attribute of such top-down approaches is their simplicity. One only\nneeds to model and generate forecasts for the most aggregated top-level series. In\ngeneral, these approaches seem to produce quite reliable forecasts for the aggregate\nlevels and they are useful with low count data. On the other hand, one disadvantage\nis the loss of information due to aggregation. Using such top-down approaches, we",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 718,
      "total_chunks": 873
    }
  },
  {
    "text": "levels and they are useful with low count data. On the other hand, one disadvantage\nis the loss of information due to aggregation. Using such top-down approaches, we\nare unable to capture and take advantage of individual series characteristics such as\ntime dynamics, special events, etc.\nForecast proportionsForecast proportions\nBecause historical proportions used for disaggregation do not take account of how\nthose proportions may change over time, top-down approaches based on historical\nproportions tend to produce less accurate forecasts at lower levels of the hierarchy\nthan bottom-up approaches. To address this issue, proportions based on forecasts\nrather than historical data can be used ( Athanasopoulos, Ahmed, & Hyndman, 2009).\nConsider a one level hierarchy. We first generate -step-ahead forecasts for all of\nthe series. We don’t use these forecasts directly, and they are not coherent (they\ndon’t add up correctly). Let’s call these “initial” forecasts. We calculate the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 719,
      "total_chunks": 873
    }
  },
  {
    "text": "the series. We don’t use these forecasts directly, and they are not coherent (they\ndon’t add up correctly). Let’s call these “initial” forecasts. We calculate the\nproportion of each -step-ahead initial forecast at the bottom level, to the\naggregate of all the -step-ahead initial forecasts at this level. We refer to these as\nthe forecast proportions, and we use them to disaggregate the top-level -step-\nahead initial forecast in order to generate coherent forecasts for the whole of the\nhierarchy.\n411",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 720,
      "total_chunks": 873
    }
  },
  {
    "text": "For a -level hierarchy, this process is repeated for each node, going from the top to\nthe bottom level. Applying this process leads to the following general rule for\nobtaining the forecast proportions:\nwhere ,  is the -step-ahead initial forecast of the series that\ncorresponds to the node which is  levels above , and  is the sum of the -step-\nahead initial forecasts below the node that is  levels above node  and are directly\nconnected to that node. These forecast proportions disaggregate the -step-ahead\ninitial forecast of the Total series to get -step-ahead coherent forecasts of the\nbottom-level series.\nWe will use the hierarchy of Figure 10.1  to explain this notation and to demonstrate\nhow this general rule is reached. Assume we have generated initial forecasts for each\nseries in the hierarchy. Recall that for the top-level “Total” series, , for any\ntop-down approach. Here are some examples using the above notation:\n;\n;\n;\n;\n.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 721,
      "total_chunks": 873
    }
  },
  {
    "text": "series in the hierarchy. Recall that for the top-level “Total” series, , for any\ntop-down approach. Here are some examples using the above notation:\n;\n;\n;\n;\n.\nMoving down the farthest left branch of the hierarchy, coherent forecasts are given\nby\nand\n/gD\n/g2C /g26 /g1E\n/gD /gC3/g12\n/gF\n/g9F/g1E/g11\n/g3F/g35 /g9/g9F/gA\n/g26/gD/g24\n/g3F/g15\n/g9/g9F/gC/g12/gA\n/g26/gD/g24\n/g26 /g1E /g12 /gD/g13 /gD/g9A/gD/g29 /g3F/g35 /g9/g9F/gA\n/g26/gD/g24 /g24\n/g9F /g26 /g3F/g15\n/g9/g9F/gA\n/g26/gD/g24 /g24\n/g9F /g26\n/g24\n/g24\n/g5F/g35 /g24 /g1E/g3F /g35 /g24\n/g3F/g35 /g9/g12/gA\n/g22/gD/g24 /g1E/g3F /g35 /g9/g12/gA\n/g23/gD/g24 /g1E/g3F /g35 /g24 /g1E /g5F/g35 /g24\n/g3F/g35\n/g9/g12/gA\n/g22/g22/gD/g24 /g1E/g3F /g35\n/g9/g12/gA\n/g22/g23/gD/g24 /g1E/g3F /g35\n/g9/g12/gA\n/g22/g24/gD /g24 /g1E/g3F /g35 /g22/gD/g24\n/g3F/g35\n/g9/g13/gA\n/g22/g22/gD/g24 /g1E /g3F/g35\n/g9/g13/gA\n/g22/g23/gD/g24 /g1E /g3F/g35\n/g9/g13/gA\n/g22/g24/gD /g24 /g1E /g3F/g35\n/g9/g13/gA\n/g23/g22/gD/g24 /g1E /g3F/g35 /g9/g13/gA",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 722,
      "total_chunks": 873
    }
  },
  {
    "text": "/g3F/g35\n/g9/g13/gA\n/g22/g22/gD/g24 /g1E /g3F/g35\n/g9/g13/gA\n/g22/g23/gD/g24 /g1E /g3F/g35\n/g9/g13/gA\n/g22/g24/gD /g24 /g1E /g3F/g35\n/g9/g13/gA\n/g23/g22/gD/g24 /g1E /g3F/g35 /g9/g13/gA\n/g23/g23/gD/g24 /g1E /g3F/g35 /g24 /g1E /g5F/g35 /g24\n/g3F/g15\n/g9/g12/gA\n/g22/g22/gD/g24 /g1E /g3F/g15\n/g9/g12/gA\n/g22/g23/gD/g24 /g1E /g3F/g15\n/g9/g12/gA\n/g22/g24/gD /g24 /g1E/g3F /g35 /g22/g22/gD/g24 /gC/g3F /g35 /g22/g23/gD/g24 /gC/g3F /g35 /g22/g24/gD /g24\n/g3F/g15\n/g9/g13/gA\n/g22/g22/gD/g24 /g1E /g3F/g15\n/g9/g13/gA\n/g22/g23/gD/g24 /g1E /g3F/g15\n/g9/g13/gA\n/g22/g24/gD /g24 /g1E /g3F/g15\n/g9/g12/gA\n/g22/gD/g24 /g1E /g3F/g15\n/g9/g12/gA\n/g23/gD/g24 /g1E /g3F/g15 /g24 /g1E /g3F/g35 /g22/gD/g24 /gC /g3F/g35 /g23/gD/g24\n/g5F/g35 /g22/gD/g24 /g1E /g2/g3 /g5F/g35 /g24 /g1E /g2/g3 /g5F/g35 /g24\n/g3F/g35 /g22/gD/g24\n/g3F/g15\n/g9/g12/gA\n/g22/gD/g24\n/g3F/g35\n/g9/g12/gA\n/g22/g22/gD/g24\n/g3F/g15\n/g9/g13/gA\n/g22/g22/gD/g24",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 723,
      "total_chunks": 873
    }
  },
  {
    "text": "/g5F/g35 /g22/gD/g24 /g1E /g2/g3 /g5F/g35 /g24 /g1E /g2/g3 /g5F/g35 /g24\n/g3F/g35 /g22/gD/g24\n/g3F/g15\n/g9/g12/gA\n/g22/gD/g24\n/g3F/g35\n/g9/g12/gA\n/g22/g22/gD/g24\n/g3F/g15\n/g9/g13/gA\n/g22/g22/gD/g24\n/g5F/g35 /g22/g22/gD/g24 /g1E /g2/g3 /g5F/g35 /g22/gD/g24 /g1E /g2/g3 /g2/g3 /g5F/g35 /g24 /gF\n/g3F/g35 /g22/g22/gD/g24\n/g3F/g15\n/g9/g12/gA\n/g22/g22/gD/g24\n/g3F/g35 /g22/g22/gD/g24\n/g3F/g15\n/g9/g12/gA\n/g22/g22/gD/g24\n/g3F/g35 /g9/g12/gA\n/g22/g22/gD/g24\n/g3F/g15\n/g9/g13/gA\n/g22/g22/gD/g24\n412",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 724,
      "total_chunks": 873
    }
  },
  {
    "text": "Consequently,\nThe other proportions can be obtained similarly.\nOne disadvantage of all top-down approaches, including this one, is that it does not\nproduce unbiased coherent forecasts ( Hyndman, Ahmed, Athanasopoulos, & Shang,\n2011).\nThis approach is implemented in the ıforecast()ı function by setting\nımethod=\"tdfp\" ı, where ıtdfpı stands for “top-down forecast proportions”.\nBibliographyBibliography\nAthanasopoulos, G., Ahmed, R. A., & Hyndman, R. J. (2009). Hierarchical forecasts\nfor Australian domestic tourism. International Journal of Forecasting , 25, 146–\n166. [DOI]\nGross, C. W., & Sohl, J. E. (1990). Disaggregation methods to expedite product line\nforecasting. Journal of Forecasting , 9, 233–254. [DOI]\nHyndman, R. J., Ahmed, R. A., Athanasopoulos, G., & Shang, H. L. (2011). Optimal\ncombination forecasts for hierarchical time series. Computational Statistics\nand Data Analysis , 55(9), 2579–2589. [DOI]\n413",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 725,
      "total_chunks": 873
    }
  },
  {
    "text": "10.510.5  Middle-out approachMiddle-out approach\nThe middle-out approach combines bottom-up and top-down approaches. First, a\n“middle level” is chosen and forecasts are generated for all the series at this level.\nFor the series above the middle level, coherent forecasts are generated using the\nbottom-up approach by aggregating the “middle-level” forecasts upwards. For the\nseries below the “middle level”, coherent forecasts are generated using a top-down\napproach by disaggregating the “middle level” forecasts downwards.\nThis approach is implemented in the ıforecast()ı function by setting ımethod=\"mo\" ı\nand by specifying the appropriate middle level via the ılevelı argument. For the top-\ndown disaggregation below the middle level, the top-down forecast proportions\nmethod is used.\n414",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 726,
      "total_chunks": 873
    }
  },
  {
    "text": "10.610.6  Mapping matricesMapping matrices\nAll of the methods considered so far can be expressed using a common notation.\nSuppose we forecast all series independently, ignoring the aggregation constraints.\nWe call these the base forecastsbase forecasts  and denote them by  where  is the forecast\nhorizon. They are stacked in the same order as the data .\nThen all forecasting approaches for either hierarchical or grouped structures can be\nrepresented as\nwhere  is a matrix that maps the base forecasts into the bottom-level, and the\nsumming matrix  sums these up using the aggregation structure to produce a set of\ncoherent forecasts .\nThe  matrix is defined according to the approach implemented. For example if the\nbottom-up approach is used to forecast the hierarchy of Figure 10.1 , then\nNotice that  contains two partitions. The first three columns zero out the base\nforecasts of the series above the bottom-level, while the -dimensional identity",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 727,
      "total_chunks": 873
    }
  },
  {
    "text": "Notice that  contains two partitions. The first three columns zero out the base\nforecasts of the series above the bottom-level, while the -dimensional identity\nmatrix picks only the base forecasts of the bottom-level. These are then summed by\nthe  matrix.\nIf any of the top-down approaches were used then\nThe first column includes the set of proportions that distribute the base forecasts of\n415",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 728,
      "total_chunks": 873
    }
  },
  {
    "text": "the top-level to the bottom-level. These are then summed up the hierarchy by the \nmatrix. The rest of the columns zero out the base forecasts below the highest level of\naggregation.\nFor a middle out approach, the  matrix will be a combination of the above two.\nUsing a set of proportions, the base forecasts of some pre-chosen level will be\ndisaggregated to the bottom-level, all other base forecasts will be zeroed out, and\nthe bottom-level forecasts will then summed up the hierarchy via the summing\nmatrix.\nForecast reconciliationForecast reconciliation\nWe can rewrite Equation (10.6) as\nwhere  is a “projection” or a “reconciliation matrix”. That is, it takes the\nincoherent base forecasts , and reconciles them to produce coherent forecasts .\nIn the methods discussed so far, no real reconciliation has been done because the\nmethods have been based on forecasts from a single level of the aggregation\nstructure, which have either been aggregated or disaggregated to obtain forecasts at",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 729,
      "total_chunks": 873
    }
  },
  {
    "text": "methods have been based on forecasts from a single level of the aggregation\nstructure, which have either been aggregated or disaggregated to obtain forecasts at\nall other levels. However, in general, we could use other  matrices, and then  will\nbe combining and reconciling all the base forecasts in order to produce coherent\nforecasts.\nIn fact, we can find the optimal  matrix to give the most accurate reconciled\nforecasts.\n416",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 730,
      "total_chunks": 873
    }
  },
  {
    "text": "10.710.7  The optimal reconciliation approachThe optimal reconciliation approach\nOptimal forecast reconciliation will occur if we can find the  matrix which\nminimises the forecast error of the set of coherent forecasts. We present here a\nsimplified summary of the approach. More details are provided in Wickramasuriya et\nal. (2019).\nSuppose we generate coherent forecasts using Equation (10.6), repeated here for\nconvenience:\nFirst we want to make sure we have unbiased forecasts. If the base forecasts  are\nunbiased, then the coherent forecasts  will be unbiased provided \n(Hyndman et al., 2011 ). This provides a constraint on the matrix . Interestingly, no\ntop-down method satisfies this constraint, so all top-down methods are biased.\nNext we need to find the error in our forecasts. Wickramasuriya et al. ( 2019) show\nthat the variance-covariance matrix of the -step-ahead coherent forecast errors is\ngiven by\nwhere  is the variance-covariance matrix of the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 731,
      "total_chunks": 873
    }
  },
  {
    "text": "that the variance-covariance matrix of the -step-ahead coherent forecast errors is\ngiven by\nwhere  is the variance-covariance matrix of the\ncorresponding base forecast errors.\nThe objective is to find a matrix  that minimises the error variances of the\ncoherent forecasts. These error variances are on the diagonal of the matrix , and\nso the sum of all the error variances is given by the trace of the matrix .\nWickramasuriya et al. ( 2019) show that the matrix  which minimises the trace of\n such that , is given by\nTherefore, the optimal reconciled forecasts are given by\nWe refer to this as the “MinT” (or Minimum Trace) estimator.\n417",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 732,
      "total_chunks": 873
    }
  },
  {
    "text": "To use this in practice, we need to estimate , the forecast error variance of the -\nstep-ahead base forecasts. This can be difficult, and so we provide four simplifying\napproximations which have been shown to work well in both simulations and in\npractice.\n1. Set  for all , where .  This is the most simplifying\nassumption to make, and means that  is independent of the data, providing\nsubstantial computational savings. The disadvantage, however, is that this\nspecification does not account for the differences in scale between the levels of\nthe structure, or for relationships between series. This approach is implemented\nin the ıforecast()ı function by setting ımethod = \"comb\"ı and ıweights = \"ols\"ı.\nThe weights here are referred to as OLS (ordinary least squares) because setting\n in (10.8) gives the least squares estimator we introduced in Section\n5.7 with  and .\n2. Set  for all , where ,\nand  is an -dimensional vector of residuals of the models that generated the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 733,
      "total_chunks": 873
    }
  },
  {
    "text": "in (10.8) gives the least squares estimator we introduced in Section\n5.7 with  and .\n2. Set  for all , where ,\nand  is an -dimensional vector of residuals of the models that generated the\nbase forecasts stacked in the same order as the data. The approach is\nimplemented by setting ımethod = \"comb\"ı and ıweights = \"wls\"ı.\nThis specification scales the base forecasts using the variance of the residuals\nand it is therefore referred to as the WLS (weighted least squares) estimator\nusing variance scaling .\n3. Set  for all , where , , and  is a unit vector of\ndimension  (the number of bottom-level series). This specification assumes\nthat the bottom-level base forecast errors each have variance  and are\nuncorrelated between nodes. Hence each element of the diagonal  matrix\ncontains the number of forecast error variances contributing to each node. This\nestimator only depends on the structure of the aggregations, and not on the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 734,
      "total_chunks": 873
    }
  },
  {
    "text": "contains the number of forecast error variances contributing to each node. This\nestimator only depends on the structure of the aggregations, and not on the\nactual data. It is therefore referred to as structural scaling . Applying the\nstructural scaling specification is particularly useful in cases where residuals are\nnot available, and so variance scaling cannot be applied; for example, in cases\nwhere the base forecasts are generated by judgmental forecasting (Chapter 4).\nThis approach is implemented by setting ımethod=\"comb\"ı and ıweights =\n\"nseries\"ı.\n/g19 /g24 /g24\n/g19 /g24 /g1E /g27 /g24 /gB /g24/g27 /g24 /g1F/g11 22\n/g9\n/g19 /g24 /g1E /g27 /g24 /gB\n/g1A /g1E /g15/g35 /g1E/g3F /g35\n/g19 /g24 /g1E /g27 /g24 /g45/g4A/g42/g48/g9 /g3F/g19 /g12 /gA /g24/g27 /g24 /g1F/g11\n/g3F/g19 /g12 /g1E\n/g16\n/g11\n/g30/g1E/g12\n/g21 /g30 /g21 /g9B\n/g30 /gD/g12\n/g16\n/g21 /g30 /g2A\n/g19 /g24 /g1E /g27 /g24 /g83 /g24/g27 /g24 /g1F/g11 /g83 /g1E /g45/g4A/g42/g48/g9/g15 /g12/gA /g12\n/g29\n/g27 /g24\n/g83",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 735,
      "total_chunks": 873
    }
  },
  {
    "text": "/g16\n/g11\n/g30/g1E/g12\n/g21 /g30 /g21 /g9B\n/g30 /gD/g12\n/g16\n/g21 /g30 /g2A\n/g19 /g24 /g1E /g27 /g24 /g83 /g24/g27 /g24 /g1F/g11 /g83 /g1E /g45/g4A/g42/g48/g9/g15 /g12/gA /g12\n/g29\n/g27 /g24\n/g83\n418",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 736,
      "total_chunks": 873
    }
  },
  {
    "text": "4. Set  for all , where . Here we only assume that the error\ncovariance matrices are proportional to each other, and we directly estimate the\nfull one-step covariance matrix . The most obvious and simple way would be\nto use the sample covariance. This is implemented by setting ımethod = \"comb\"ı,\nıweights = \"mint\"ı, and ıcovariance = \"sam\"ı.\nHowever, for cases where the number of bottom-level series  is large\ncompared to the length of the series , this is not a good estimator. Instead we\nuse a shrinkage estimator which shrinks the sample covariance to a diagonal\nmatrix. This is implemented by setting ımethod = \"comb\"ı, ıweights = \"mint\"ı,\nand ıcovariance = \"shr\"ı.\nIn summary, unlike any other existing approach, the optimal reconciliation forecasts\nare generated using all the information available within a hierarchical or a grouped\nstructure. This is important, as particular aggregation levels or groupings may reveal",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 737,
      "total_chunks": 873
    }
  },
  {
    "text": "are generated using all the information available within a hierarchical or a grouped\nstructure. This is important, as particular aggregation levels or groupings may reveal\nfeatures of the data that are of interest to the user and are important to be modelled.\nThese features may be completely hidden or not easily identifiable at other levels.\nFor example, consider the Australian tourism data introduced in Section 10.1, where\nthe hierarchical structure followed the geographic division of a country into states\nand zones. Some coastal areas will be largely summer destinations, while some\nmountain regions may be winter destinations. These differences will be smoothed at\nthe country level due to aggregation.\nExample: Forecasting Australian prison populationExample: Forecasting Australian prison population\nWe compute the forecasts for the Australian prison population, described in Section\n10.2. Using the default arguments for the ıforecast()ı function, we compute",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 738,
      "total_chunks": 873
    }
  },
  {
    "text": "We compute the forecasts for the Australian prison population, described in Section\n10.2. Using the default arguments for the ıforecast()ı function, we compute\ncoherent forecasts by the optimal reconciliation approach with the WLS estimator\nusing variance scaling.\nTo obtain forecasts for each level of aggregation, we can use the ıaggts()ı function.\nFor example, to calculate forecasts for the overall total prison population, and for\nthe one-factor groupings (State, Gender and Legal Status), we use:\nA simple plot is obtained using\nprisonfc <- forecast(prison.gts)\nfcsts <- aggts(prisonfc, levels=0:3)\n419",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 739,
      "total_chunks": 873
    }
  },
  {
    "text": "A nicer plot is available using the following code. The results are shown in Figure\n10.8. The vertical line marks the start of the forecast period.\ngroups <- aggts(prison.gts, levels=0 :3)\nautoplot(fcsts) + autolayer(groups)\nprisonfc <- ts(rbind(groups, fcsts),\n  start=start(groups), frequency=4 )\np1 <- autoplot(prisonfc[,\"Total\"]) +\n  ggtitle(\"Australian prison population\" ) +\n  xlab(\"Year\") + ylab(\"Total number of prisoners ('000)\" ) +\n  geom_vline(xintercept=2017)\ncols <- sample(scales::hue_pal( h=c(15,375),\n          c=100,l=65,h.start=0,direction = 1)(NCOL(groups)))\np2 <- as_tibble(prisonfc[,- 1]) %>%\n  gather(Series) %>%\n  mutate(Date = rep(time(prisonfc), NCOL(prisonfc)-1),\n         Group = str_extract(Series, \"([A-Za-z ]*)\")) %>%\n  ggplot(aes(x=Date, y=value, group=Series, colour=Series)) +\n    geom_line() +\n    xlab(\"Year\") + ylab(\"Number of prisoners ('000)\") +\n    scale_colour_manual(values = cols) +\n    facet_grid(. ~ Group, scales=\"free_y\") +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 740,
      "total_chunks": 873
    }
  },
  {
    "text": "geom_line() +\n    xlab(\"Year\") + ylab(\"Number of prisoners ('000)\") +\n    scale_colour_manual(values = cols) +\n    facet_grid(. ~ Group, scales=\"free_y\") +\n    scale_x_continuous(breaks=seq(2006,2018,by=2)) +\n    theme(axis.text.x = element_text(angle=90, hjust=1)) +\n    geom_vline(xintercept=2017)\ngridExtra::grid.arrange (p1, p2, ncol=1)\n420",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 741,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.8: Coherent forecasts for the total Australian adult prison population and for\nthe population grouped by state, by legal status and by gender.\nSimilar code was used to produce Figure 10.9. The left panel plots the coherent\nforecasts for interactions between states and gender. The right panel shows\nforecasts for the bottom-level series.\n421",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 742,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 10.9: Coherent forecasts for the Australian adult prison population grouped by all\ninteractions of attributes.\nThe ıaccuracy()ı command is useful for evaluating the forecast accuracy across\nhierarchical or grouped structures. The following table summarises the accuracy of\nthe bottom-up and the optimal reconciliation approaches, forecasting 2015 Q1 to\n2016 Q4 as a test period.\nThe results show that the optimal reconciliation approach generates more accurate\nforecasts especially for the top level. In general, we find that as the optimal\nreconciliation approach uses information from all levels in the structure, it\ngenerates more accurate coherent forecasts than the other traditional alternatives\nwhich use limited information.\n422",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 743,
      "total_chunks": 873
    }
  },
  {
    "text": "Table 10.1: Accuracy of Australian prison population forecasts for different groups of\nseries.\nBottom-upBottom-up OptimalOptimal\nMAPEMAPE MASEMASE MAPEMAPE MASEMASE\nTotal 5.32 1.84 3.08 1.06\nState 7.59 1.88 7.62 1.85\nLegal status 6.40 1.76 4.32 1.14\nGender 8.62 2.68 8.72 2.74\nBottom 15.82 2.23 15.25 2.16\nAll series 12.41 2.16 12.02 2.08\nBibliographyBibliography\nHyndman, R. J., Ahmed, R. A., Athanasopoulos, G., & Shang, H. L. (2011). Optimal\ncombination forecasts for hierarchical time series. Computational Statistics\nand Data Analysis , 55(9), 2579–2589. [DOI]\nWickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal\nforecast reconciliation for hierarchical and grouped time series through trace\nminimization. Journal of the American Statistical Association , 114(526), 804–\n819. [DOI]\n22. Note that  is a proportionality constant. It does not need to be estimated or\nspecified here as it gets cancelled out in (10.8).↩ \n423",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 744,
      "total_chunks": 873
    }
  },
  {
    "text": "10.810.8  ExercisesExercises\n1. Write out the  matrices for the Australian tourism hierarchy and the Australian\nprison grouped structure. Use the ısmatrixı command to verify your answers.\n2. Generate 8-step-ahead bottom-up forecasts using ARIMA models for the\nıvisnightsı Australian domestic tourism data. Plot the coherent forecasts by\nlevel and comment on their nature. Are you satisfied with these forecasts?\n3. Model the aggregate series for Australian domestic tourism data ıvisnightsı\nusing an ARIMA model. Comment on the model. Generate and plot 8-step-ahead\nforecasts from the ARIMA model and compare these with the bottom-up\nforecasts generated in question 2 for the aggregate level.\n4. Generate 8-step-ahead optimally reconciled coherent forecasts using ARIMA\nbase forecasts for the ıvisnightsı Australian domestic tourism data. Plot the\ncoherent forecasts by level and comment on their nature. How and why are these\ndifferent to the bottom-up forecasts generated in question 2 above.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 745,
      "total_chunks": 873
    }
  },
  {
    "text": "coherent forecasts by level and comment on their nature. How and why are these\ndifferent to the bottom-up forecasts generated in question 2 above.\n5. Using the last two years of the ıvisnightsı Australian domestic tourism data as a\ntest set, generate bottom-up, top-down and optimally reconciled forecasts for\nthis period and compare their accuracy.\n424",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 746,
      "total_chunks": 873
    }
  },
  {
    "text": "10.910.9  Further readingFurther reading\nThere are no other textbooks which cover hierarchical forecasting in any depth, so\ninterested readers will need to tackle the original research papers for further\ninformation.\nGross & Sohl (1990) provide a good introduction to the top-down approaches.\nThe reconciliation methods were developed in a series of papers, which are best\nread in the following order: Hyndman et al. ( 2011), Athanasopoulos et al. ( 2009),\nHyndman, Lee, & Wang (2016), Wickramasuriya et al. ( 2019).\nAthanasopoulos, Hyndman, Kourentzes, & Petropoulos ( 2017) extends the\nreconciliation approach to deal with temporal hierarchies.\nBibliographyBibliography\nAthanasopoulos, G., Ahmed, R. A., & Hyndman, R. J. (2009). Hierarchical forecasts\nfor Australian domestic tourism. International Journal of Forecasting , 25, 146–\n166. [DOI]\nAthanasopoulos, G., Hyndman, R. J., Kourentzes, N., & Petropoulos, F. (2017).\nForecasting with temporal hierarchies. European Journal of Operational",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 747,
      "total_chunks": 873
    }
  },
  {
    "text": "166. [DOI]\nAthanasopoulos, G., Hyndman, R. J., Kourentzes, N., & Petropoulos, F. (2017).\nForecasting with temporal hierarchies. European Journal of Operational\nResearch, 262(1), 60–74. [DOI]\nGross, C. W., & Sohl, J. E. (1990). Disaggregation methods to expedite product line\nforecasting. Journal of Forecasting , 9, 233–254. [DOI]\nHyndman, R. J., Ahmed, R. A., Athanasopoulos, G., & Shang, H. L. (2011). Optimal\ncombination forecasts for hierarchical time series. Computational Statistics\nand Data Analysis , 55(9), 2579–2589. [DOI]\nHyndman, R. J., Lee, A., & Wang, E. (2016). Fast computation of reconciled\nforecasts for hierarchical and grouped time series. Computational Statistics\nand Data Analysis , 97, 16–32. [DOI]\nWickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal\nforecast reconciliation for hierarchical and grouped time series through trace\nminimization. Journal of the American Statistical Association , 114(526), 804–\n819. [DOI]\n425",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 748,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 11Chapter 11  Advanced forecasting methodsAdvanced forecasting methods\nIn this chapter, we briefly discuss four more advanced forecasting methods that\nbuild on the models discussed in earlier chapters.\n426",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 749,
      "total_chunks": 873
    }
  },
  {
    "text": "11.111.1  Complex seasonalityComplex seasonality\nSo far, we have considered relatively simple seasonal patterns such as quarterly and\nmonthly data. However, higher frequency time series often exhibit more complicated\nseasonal patterns. For example, daily data may have a weekly pattern as well as an\nannual pattern. Hourly data usually has three types of seasonality: a daily pattern, a\nweekly pattern, and an annual pattern. Even weekly data can be challenging to\nforecast as it typically has an annual pattern with seasonal period of\n on average.\nSuch multiple seasonal patterns are becoming more common with high frequency\ndata recording. Further examples where multiple seasonal patterns can occur include\ncall volume in call centres, daily hospital admissions, requests for cash at ATMs,\nelectricity and water usage, and access to computer web sites.\nMost of the methods we have considered so far are unable to deal with these seasonal",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 750,
      "total_chunks": 873
    }
  },
  {
    "text": "electricity and water usage, and access to computer web sites.\nMost of the methods we have considered so far are unable to deal with these seasonal\ncomplexities. Even the ıtsı class in R can only handle one type of seasonality, which\nis usually assumed to take integer values.\nTo deal with such series, we will use the ımstsı class which handles multiple\nseasonality time series. This allows you to specify all of the frequencies that might be\nrelevant. It is also flexible enough to handle non-integer frequencies.\nDespite this flexibility, we don’t necessarily want to include all of these frequencies\n— just the ones that are likely to be present in the data. For example, if we have only\n180 days of data, we may ignore the annual seasonality. If the data are measurements\nof a natural phenomenon (e.g., temperature), we can probably safely ignore any\nweekly seasonality.\nThe top panel of Figure 11.1  shows the number of retail banking call arrivals per 5-",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 751,
      "total_chunks": 873
    }
  },
  {
    "text": "of a natural phenomenon (e.g., temperature), we can probably safely ignore any\nweekly seasonality.\nThe top panel of Figure 11.1  shows the number of retail banking call arrivals per 5-\nminute interval between 7:00am and 9:05pm each weekday over a 33 week period.\nThe bottom panel shows the first three weeks of the same time series. There is a\nstrong daily seasonal pattern with frequency 169 (there are 169 5-minute intervals\nper day), and a weak weekly seasonal pattern with frequency . (Call\nvolumes on Mondays tend to be higher than the rest of the week.) If a longer series of\ndata were available, we may also have observed an annual seasonal pattern.\n427",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 752,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.1: Five-minute call volume handled on weekdays between 7:00am and 9:05pm in\na large North American commercial bank. Top panel shows data from 3 March 2003 for\n164 days.\nSTL with multiple seasonal periodsSTL with multiple seasonal periods\nThe ımstl()ı function is a variation on ıstl()ı designed to deal with multiple\nseasonality. It will return multiple seasonal components, as well as a trend and\nremainder component.\np1 <- autoplot(calls) +\n  ylab(\"Call volume\") + xlab(\"Weeks\") +\n  scale_x_continuous(breaks=seq(1,33,by=2))\np2 <- autoplot(window(calls, end=4)) +\n  ylab(\"Call volume\") + xlab(\"Weeks\") +\n  scale_x_continuous(minor_breaks = seq(1,4,by=0.2))\ngridExtra::grid.arrange(p1,p2)\ncalls %>% mstl() %>%\n  autoplot() + xlab(\"Week\")\n428",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 753,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.2: Multiple STL for the call volume data.\nThere are two seasonal patterns shown, one for the time of day (the third panel), and\none for the time of week (the fourth panel). To properly interpret this graph, it is\nimportant to notice the vertical scales. In this case, the trend and the weekly\nseasonality have relatively narrow ranges compared to the other components,\nbecause there is little trend seen in the data, and the weekly seasonality is weak.\nThe decomposition can also be used in forecasting, with each of the seasonal\ncomponents forecast using a seasonal naïve method, and the seasonally adjusted\ndata forecasting using ETS (or some other user-specified method). The ıstlf()ı\nfunction will do this automatically.\ncalls %>%  stlf() %>%\n  autoplot() + xlab(\"Week\")\n429",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 754,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.3: Multiple STL for the call volume data.\nDynamic harmonic regression with multiple seasonalDynamic harmonic regression with multiple seasonal\nperiodsperiods\nWith multiple seasonalities, we can use Fourier terms as we did in earlier chapters\n(see Sections 5.4 and 9.5). Because there are multiple seasonalities, we need to add\nFourier terms for each seasonal period. In this case, the seasonal periods are 169 and\n845, so the Fourier terms are of the form\nfor . The ıfourier()ı function can generate these for you.\nWe will fit a dynamic harmonic regression model with an ARMA error structure. The\ntotal number of Fourier terms for each seasonal period have been chosen to\nminimise the AICc. We will use a log transformation ( ılambda=0ı) to ensure the\nforecasts and prediction intervals remain positive.\n430",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 755,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.4: Forecasts from a dynamic harmonic regression applied to the call volume\ndata.\nThis is a large model, containing 40 parameters: 4 ARMA coefficients, 20 Fourier\ncoefficients for frequency 169, and 16 Fourier coefficients for frequency 845. We\ndon’t use all the Fourier terms for frequency 845 because there is some overlap with\nthe terms of frequency 169 (since ).\nTBATS modelsTBATS models\nAn alternative approach developed by De Livera, Hyndman, & Snyder ( 2011) uses a\ncombination of Fourier terms with an exponential smoothing state space model and\na Box-Cox transformation, in a completely automated manner. As with any\nautomated modelling framework, there may be cases where it gives poor results, but\nit can be a useful approach in some circumstances.\nfit <- auto.arima(calls, seasonal=FALSE , lambda=0,\n         xreg=fourier(calls, K=c(10,10)))\nfit %>%\n  forecast(xreg=fourier(calls, K=c(10,10), h=2*169)) %>%\n  autoplot(include=5*169) +\n    ylab(\"Call volume\") + xlab(\"Weeks\")",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 756,
      "total_chunks": 873
    }
  },
  {
    "text": "xreg=fourier(calls, K=c(10,10)))\nfit %>%\n  forecast(xreg=fourier(calls, K=c(10,10), h=2*169)) %>%\n  autoplot(include=5*169) +\n    ylab(\"Call volume\") + xlab(\"Weeks\")\n431",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 757,
      "total_chunks": 873
    }
  },
  {
    "text": "A TBATS model differs from dynamic harmonic regression in that the seasonality is\nallowed to change slowly over time in a TBATS model, while harmonic regression\nterms force the seasonal patterns to repeat periodically without changing. One\ndrawback of TBATS models, however, is that they can be slow to estimate, especially\nwith long time series. Hence, we will consider a subset of the ıcallsı data to save\ntime.\nFigure 11.5: Forecasts from a TBATS model applied to the call volume data.\nHere the prediction intervals appear to be much too wide – something that seems to\nhappen quite often with TBATS models unfortunately.\nComplex seasonality with covariatesComplex seasonality with covariates\nTBATS models do not allow for covariates, although they can be included in dynamic\nharmonic regression models. One common application of such models is electricity\ndemand modelling.\ncalls %>%\n  subset(start=length(calls)-2000) %>%\n  tbats() -> fit2\nfc2 <- forecast(fit2, h=2*169)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 758,
      "total_chunks": 873
    }
  },
  {
    "text": "harmonic regression models. One common application of such models is electricity\ndemand modelling.\ncalls %>%\n  subset(start=length(calls)-2000) %>%\n  tbats() -> fit2\nfc2 <- forecast(fit2, h=2*169)\nautoplot(fc2, include=5 *169) +\n  ylab(\"Call volume\") + xlab(\"Weeks\")\n432",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 759,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.6 shows half-hourly electricity demand in Victoria, Australia, during 2014,\nalong with temperatures for the same period for Melbourne (the largest city in\nVictoria).\nFigure 11.6: Half-hourly electricity demand and corresponding temperatures in 2014,\nVictoria, Australia.\nPlotting electricity demand against temperature (Figure 11.7) shows that there is a\nnonlinear relationship between the two, with demand increasing for low\ntemperatures (due to heating) and increasing for high temperatures (due to cooling).\nautoplot(elecdemand[,c (\"Demand\",\"Temperature\")],\n    facet=TRUE) +\n  scale_x_continuous(minor_breaks=NULL,\n    breaks=2014+\n      cumsum(c(0,31,28,31,30,31,30,31,31,30,31,30))/365,\n    labels=month.abb) +\n  xlab(\"Time\") + ylab(\"\")\n433",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 760,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.7: Half-hourly electricity demand for Victoria, plotted against temperatures for\nthe same times in Melbourne, the largest city in Victoria.\nWe will fit a regression model with a piecewise linear function of temperature\n(containing a knot at 18 degrees), and harmonic regression terms to allow for the\ndaily seasonal pattern.\nForecasting with such models is difficult because we require future values of the\npredictor variables. Future values of the Fourier terms are easy to compute, but\nfuture temperatures are, of course, unknown. If we are only interested in forecasting\nup to a week ahead, we could use temperature forecasts obtain from a meteorological\nelecdemand %>%\n  as.data.frame() %>%\n  ggplot(aes(x=Temperature, y=Demand)) +  geom_point() +\n    xlab(\"Temperature (degrees Celsius)\" ) +\n    ylab(\"Demand (GW)\")\ncooling <- pmax(elecdemand[,\"Temperature\"], 18)\nfit <- auto.arima(elecdemand[,\"Demand\"],\n         xreg = cbind(fourier(elecdemand, c (10,10,0)),",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 761,
      "total_chunks": 873
    }
  },
  {
    "text": "ylab(\"Demand (GW)\")\ncooling <- pmax(elecdemand[,\"Temperature\"], 18)\nfit <- auto.arima(elecdemand[,\"Demand\"],\n         xreg = cbind(fourier(elecdemand, c (10,10,0)),\n               heating=elecdemand[,\"Temperature\"],\n               cooling=cooling))\n434",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 762,
      "total_chunks": 873
    }
  },
  {
    "text": "model. Alternatively, we could use scenario forecasting (Section 4.5) and plug in\npossible temperature patterns. In the following example, we have used a repeat of\nthe last two days of temperatures to generate future possible demand values.\nFigure 11.8: Forecasts from a dynamic harmonic regression model applied to half-hourly\nelectricity demand data.\nAlthough the short-term forecasts look reasonable, this is a crude model for a\ncomplicated process. The residuals demonstrate that there is a lot of information\nthat has not been captured with this model.\nFigure 11.9: Residual diagnostics for the dynamic harmonic regression model.\n#> \n#>  Ljung-Box test\n#> \n#> data:  Residuals from Regression with ARIMA(5,1,4) errors\n#> Q* = 738424, df = 3495, p-value <2e-16\n#> \n#> Model df: 9.   Total lags used: 3504\nMore sophisticated versions of this model which provide much better forecasts are\ndescribed in Hyndman & Fan ( 2010) and Fan & Hyndman ( 2012).\nBibliographyBibliography",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 763,
      "total_chunks": 873
    }
  },
  {
    "text": "More sophisticated versions of this model which provide much better forecasts are\ndescribed in Hyndman & Fan ( 2010) and Fan & Hyndman ( 2012).\nBibliographyBibliography\nDe Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series\nwith complex seasonal patterns using exponential smoothing. J American\nStatistical Association , 106(496), 1513–1527. [DOI]\ntemps <- subset(elecdemand[,\"Temperature\"],\n          start=NROW(elecdemand)-2*48+1)\nfc <- forecast(fit,\n        xreg=cbind(fourier(temps, c(10,10,0)),\n          heating=temps, cooling=pmax (temps,18)))\nautoplot(fc, include=14*48)\ncheckresiduals(fc)\n435",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 764,
      "total_chunks": 873
    }
  },
  {
    "text": "Fan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-\nparametric additive model. IEEE Transactions on Power Systems , 27(1), 134–\n141. [DOI]\nHyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak\nelectricity demand. IEEE Transactions on Power Systems , 25(2), 1142–1153.\n[DOI]\n436",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 765,
      "total_chunks": 873
    }
  },
  {
    "text": "11.211.2  Vector autoregressionsVector autoregressions\nOne limitation of the models that we have considered so far is that they impose a\nunidirectional relationship — the forecast variable is influenced by the predictor\nvariables, but not vice versa. However, there are many cases where the reverse\nshould also be allowed for — where all variables affect each other. In Section 9.2, the\nchanges in personal consumption expenditure ( ) were forecast based on the\nchanges in personal disposable income ( ). However, in this case a bi-directional\nrelationship may be more suitable: an increase in  will lead to an increase in  and\nvice versa.\nAn example of such a situation occurred in Australia during the Global Financial\nCrisis of 2008–2009. The Australian government issued stimulus packages that\nincluded cash payments in December 2008, just in time for Christmas spending. As a\nresult, retailers reported strong sales and the economy was stimulated.\nConsequently, incomes increased.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 766,
      "total_chunks": 873
    }
  },
  {
    "text": "included cash payments in December 2008, just in time for Christmas spending. As a\nresult, retailers reported strong sales and the economy was stimulated.\nConsequently, incomes increased.\nSuch feedback relationships are allowed for in the vector autoregressive (VAR)\nframework. In this framework, all variables are treated symmetrically. They are all\nmodelled as if they all influence each other equally. In more formal terminology, all\nvariables are now treated as “endogenous”. To signify this, we now change the\nnotation and write all variables as s:  denotes the th observation of variable ,\n denotes the th observation of variable , and so on.\nA VAR model is a generalisation of the univariate autoregressive model for\nforecasting a vector of time series.  It comprises one equation per variable in the\nsystem. The right hand side of each equation includes a constant and lags of all of the\nvariables in the system. To keep it simple, we will consider a two variable VAR with",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 767,
      "total_chunks": 873
    }
  },
  {
    "text": "system. The right hand side of each equation includes a constant and lags of all of the\nvariables in the system. To keep it simple, we will consider a two variable VAR with\none lag. We write a 2-dimensional VAR(1) as\nwhere  and  are white noise processes that may be contemporaneously\ncorrelated. The coefficient  captures the influence of the th lag of variable  on\nitself, while the coefficient  captures the influence of the th lag of variable  on\n.\n23\n437",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 768,
      "total_chunks": 873
    }
  },
  {
    "text": "If the series are stationary, we forecast them by fitting a VAR to the data directly\n(known as a “VAR in levels”). If the series are non-stationary, we take differences of\nthe data in order to make them stationary, then fit a VAR model (known as a “VAR in\ndifferences”). In both cases, the models are estimated equation by equation using\nthe principle of least squares. For each equation, the parameters are estimated by\nminimising the sum of squared  values.\nThe other possibility, which is beyond the scope of this book and therefore we do not\nexplore here, is that the series may be non-stationary but cointegrated, which\nmeans that there exists a linear combination of them that is stationary. In this case,\na VAR specification that includes an error correction mechanism (usually referred to\nas a vector error correction model) should be included, and alternative estimation\nmethods to least squares estimation should be used.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 769,
      "total_chunks": 873
    }
  },
  {
    "text": "as a vector error correction model) should be included, and alternative estimation\nmethods to least squares estimation should be used.\nForecasts are generated from a VAR in a recursive manner. The VAR generates\nforecasts for each variable included in the system. To illustrate the process, assume\nthat we have fitted the 2-dimensional VAR(1) described in Equations (11.1)– (11.2), for\nall observations up to time . Then the one-step-ahead forecasts are generated by\nThis is the same form as (11.1)– (11.2), except that the errors have been set to zero and\nparameters have been replaced with their estimates. For , the forecasts are\ngiven by\nAgain, this is the same form as (11.1)– (11.2), except that the errors have been set to\nzero, the parameters have been replaced with their estimates, and the unknown\nvalues of  and  have been replaced with their forecasts. The process can be\niterated in this manner for all future time periods.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 770,
      "total_chunks": 873
    }
  },
  {
    "text": "values of  and  have been replaced with their forecasts. The process can be\niterated in this manner for all future time periods.\nThere are two decisions one has to make when using a VAR to forecast, namely how\nmany variables (denoted by ) and how many lags (denoted by ) should be included\nin the system. The number of coefficients to be estimated in a VAR is equal to\n (or  per equation). For example, for a VAR with  variables\n/g21 /g25 /gD/g30\n24\n/g16\n/g3F/g35 /g12/gD /g16 /gC/g12/g5D /g16 /g1E/g3F /g1F /g12 /gC /g3F/g5D /g12/g12/gD/g12 /g35 /g12/gD /g16 /gC /g3F/g5D /g12/g13/gD/g12 /g35 /g13/gD /g16\n/g3F/g35 /g13/gD /g16 /gC/g12/g5D /g16 /g1E/g3F /g1F /g13 /gC /g3F/g5D /g13/g12/gD/g12 /g35 /g12/gD /g16 /gC /g3F/g5D /g13/g13/gD/g12 /g35 /g13/gD /g16 /gF\n/g24 /g1E/g13\n/g3F/g35 /g12/gD /g16 /gC/g13/g5D /g16 /g1E/g3F /g1F /g12 /gC /g3F/g5D /g12/g12/gD/g12 /g3F/g35 /g12/gD /g16 /gC/g12 /gC /g3F/g5D /g12/g13/gD/g12 /g3F/g35 /g13/gD /g16 /gC/g12",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 771,
      "total_chunks": 873
    }
  },
  {
    "text": "/g24 /g1E/g13\n/g3F/g35 /g12/gD /g16 /gC/g13/g5D /g16 /g1E/g3F /g1F /g12 /gC /g3F/g5D /g12/g12/gD/g12 /g3F/g35 /g12/gD /g16 /gC/g12 /gC /g3F/g5D /g12/g13/gD/g12 /g3F/g35 /g13/gD /g16 /gC/g12\n/g3F/g35 /g13/gD /g16 /gC/g13/g5D /g16 /g1E/g3F /g1F /g13 /gC /g3F/g5D /g13/g12/gD/g12 /g3F/g35 /g12/gD /g16 /gC/g12 /gC /g3F/g5D /g13/g13/gD/g12 /g3F/g35 /g13/gD /g16 /gC/g12 /gF\n/g35 /g12 /g35 /g13\n/gD/g2C\n/gD /gC /g2C/gD /g13 /g12/gC/g2C/gD /gD /g1E/g16\n438",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 772,
      "total_chunks": 873
    }
  },
  {
    "text": "and  lags, there are 16 coefficients per equation, giving a total of 80 coefficients\nto be estimated. The more coefficients that need to be estimated, the larger the\nestimation error entering the forecast.\nIn practice, it is usual to keep  small and include only variables that are correlated\nwith each other, and therefore useful in forecasting each other. Information criteria\nare commonly used to select the number of lags to be included.\nVAR models are implemented in the varsvars package in R. It contains a function\nıVARselect()ı for selecting the number of lags  using four different information\ncriteria: AIC, HQ, SC and FPE. We have met the AIC before, and SC is simply another\nname for the BIC (SC stands for Schwarz Criterion, after Gideon Schwarz who\nproposed it). HQ is the Hannan-Quinn criterion, and FPE is the “Final Prediction\nError” criterion.  Care should be taken when using the AIC as it tends to choose",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 773,
      "total_chunks": 873
    }
  },
  {
    "text": "proposed it). HQ is the Hannan-Quinn criterion, and FPE is the “Final Prediction\nError” criterion.  Care should be taken when using the AIC as it tends to choose\nlarge numbers of lags. Instead, for VAR models, we prefer to use the BIC.\nA criticism that VARs face is that they are atheoretical; that is, they are not built on\nsome economic theory that imposes a theoretical structure on the equations. Every\nvariable is assumed to influence every other variable in the system, which makes a\ndirect interpretation of the estimated coefficients difficult. Despite this, VARs are\nuseful in several contexts:\n1. forecasting a collection of related variables where no explicit interpretation is\nrequired;\n2. testing whether one variable is useful in forecasting another (the basis of\nGranger causality tests);\n3. impulse response analysis, where the response of one variable to a sudden but\ntemporary change in another variable is analysed;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 774,
      "total_chunks": 873
    }
  },
  {
    "text": "Granger causality tests);\n3. impulse response analysis, where the response of one variable to a sudden but\ntemporary change in another variable is analysed;\n4. forecast error variance decomposition, where the proportion of the forecast\nvariance of each variable is attributed to the effects of the other variables.\nExample: A VAR model for forecasting US consumptionExample: A VAR model for forecasting US consumption\n25\nlibrary(vars)\nVARselect(uschange[,1:2], lag.max=8,\n  type=\"const\")[[\"selection\" ]]\n#> AIC(n)  HQ(n)  SC(n) FPE(n) \n#>      5      1      1      5\n439",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 775,
      "total_chunks": 873
    }
  },
  {
    "text": "The R output shows the lag length selected by each of the information criteria\navailable in the varsvars package. There is a large discrepancy between the VAR(5)\nselected by the AIC and the VAR(1) selected by the BIC. This is not unusual. As a result\nwe first fit a VAR(1), as selected by the BIC.\nIn similar fashion to the univariate ARIMA methodology, we test that the residuals\nare uncorrelated using a Portmanteau test . Both a VAR(1) and a VAR(2) have some\nresidual serial correlation, and therefore we fit a VAR(3).\nThe residuals for this model pass the test for serial correlation. The forecasts\ngenerated by the VAR(3) are plotted in Figure 11.10 .\nvar1 <- VAR(uschange[,1:2], p=1, type=\"const\")\nserial.test(var1, lags.pt=10, type= \"PT.asymptotic\")\nvar2 <- VAR(uschange[,1:2], p=2, type=\"const\")\nserial.test(var2, lags.pt=10, type= \"PT.asymptotic\")\n26\nvar3 <- VAR(uschange[,1:2], p=3, type=\"const\")\nserial.test(var3, lags.pt=10, type= \"PT.asymptotic\")\n#> \n#>  Portmanteau Test (asymptotic)",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 776,
      "total_chunks": 873
    }
  },
  {
    "text": "serial.test(var2, lags.pt=10, type= \"PT.asymptotic\")\n26\nvar3 <- VAR(uschange[,1:2], p=3, type=\"const\")\nserial.test(var3, lags.pt=10, type= \"PT.asymptotic\")\n#> \n#>  Portmanteau Test (asymptotic)\n#> \n#> data:  Residuals of VAR object var3\n#> Chi-squared = 34, df = 28, p-value = 0.2\nforecast(var3) %>%\n  autoplot() + xlab(\"Year\")\n440",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 777,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.10: Forecasts for US consumption and income generated from a VAR(3).\nBibliographyBibliography\nAthanasopoulos, G., Poskitt, D. S., & Vahid, F. (2012). Two canonical VARMA\nforms: Scalar component models vis-à-vis the echelon form. Econometric\nReviews, 31(1), 60–83. [DOI]\nHamilton, J. D. (1994). Time series analysis . Princeton University Press,\nPrinceton. [Amazon]\nLütkepohl, H. (2005). New introduction to multiple time series analysis . Berlin:\nSpringer-Verlag. [Amazon]\nLütkepohl, H. (2007). General-to-specific or specific-to-general modelling? An\nopinion on current econometric terminology. Journal of Econometrics , 136(1),\n234–319. [DOI]\n441",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 778,
      "total_chunks": 873
    }
  },
  {
    "text": "23. A more flexible generalisation would be a Vector ARMA process. However, the\nrelative simplicity of VARs has led to their dominance in forecasting. Interested\nreaders may refer to Athanasopoulos, Poskitt, & Vahid ( 2012\n).↩\n24. Interested readers should refer to Hamilton ( 1994\n) and Lütkepohl ( 2007).↩\n25. For a detailed comparison of these criteria, see Chapter 4.3 of Lütkepohl\n(2005\n).↩\n26. The tests for serial correlation in the “vars” package are multivariate\ngeneralisations of the tests presented in Section 3.3\n.↩\n442",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 779,
      "total_chunks": 873
    }
  },
  {
    "text": "11.311.3  Neural network modelsNeural network models\nArtificial neural networks are forecasting methods that are based on simple\nmathematical models of the brain. They allow complex nonlinear relationships\nbetween the response variable and its predictors.\nNeural network architectureNeural network architecture\nA neural network can be thought of as a network of “neurons” which are organised\nin layers. The predictors (or inputs) form the bottom layer, and the forecasts (or\noutputs) form the top layer. There may also be intermediate layers containing\n“hidden neurons”.\nThe simplest networks contain no hidden layers and are equivalent to linear\nregressions. Figure 11.11 shows the neural network version of a linear regression with\nfour predictors. The coefficients attached to these predictors are called “weights”.\nThe forecasts are obtained by a linear combination of the inputs. The weights are\nselected in the neural network framework using a “learning algorithm” that",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 780,
      "total_chunks": 873
    }
  },
  {
    "text": "The forecasts are obtained by a linear combination of the inputs. The weights are\nselected in the neural network framework using a “learning algorithm” that\nminimises a “cost function” such as the MSE. Of course, in this simple example, we\ncan use linear regression which is a much more efficient method of training the\nmodel.\nFigure 11.11: A simple neural network equivalent to a linear regression.\n443",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 781,
      "total_chunks": 873
    }
  },
  {
    "text": "Once we add an intermediate layer with hidden neurons, the neural network becomes\nnon-linear. A simple example is shown in Figure 11.12.\nFigure 11.12: A neural network with four inputs and one hidden layer with three hidden\nneurons.\nThis is known as a multilayer feed-forward network , where each layer of nodes\nreceives inputs from the previous layers. The outputs of the nodes in one layer are\ninputs to the next layer. The inputs to each node are combined using a weighted\nlinear combination. The result is then modified by a nonlinear function before being\noutput. For example, the inputs into hidden neuron  in Figure 11.12 are combined\nlinearly to give\nIn the hidden layer, this is then modified using a nonlinear function such as a\nsigmoid,\nto give the input for the next layer. This tends to reduce the effect of extreme input\nvalues, thus making the network somewhat robust to outliers.\nThe parameters  and  are “learned” from the data. The values",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 782,
      "total_chunks": 873
    }
  },
  {
    "text": "values, thus making the network somewhat robust to outliers.\nThe parameters  and  are “learned” from the data. The values\nof the weights are often restricted to prevent them from becoming too large. The\nparameter that restricts the weights is known as the “decay parameter”, and is often\n/g26\n/g36 /g26 /g1E /g1E /g26 /gC\n/g15\n/g11\n/g25 /g1E/g12\n/g33 /g25 /gD/g26 /g34 /g25 /gF\n/g2F/g9/g36 /gA/g1E /gD /g12\n/g12/gC/g21 /gC3 /g36\n/g1E /g12 /gD /g1E /g13 /gD /g1E /g14 /g33 /g12/gD/g12 /gD/g9A/gD/g33 /g15/gD/g14\n444",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 783,
      "total_chunks": 873
    }
  },
  {
    "text": "set to be equal to 0.1.\nThe weights take random values to begin with, and these are then updated using the\nobserved data. Consequently, there is an element of randomness in the predictions\nproduced by a neural network. Therefore, the network is usually trained several\ntimes using different random starting points, and the results are averaged.\nThe number of hidden layers, and the number of nodes in each hidden layer, must be\nspecified in advance. Usually, these would be selected using cross-validation.\nNeural network autoregressionNeural network autoregression\nWith time series data, lagged values of the time series can be used as inputs to a\nneural network, just as we used lagged values in a linear autoregression model\n(Chapter 8). We call this a neural network autoregression or NNAR model.\nIn this book, we only consider feed-forward networks with one hidden layer, and we\nuse the notation NNAR( ) to indicate there are  lagged inputs and  nodes in the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 784,
      "total_chunks": 873
    }
  },
  {
    "text": "In this book, we only consider feed-forward networks with one hidden layer, and we\nuse the notation NNAR( ) to indicate there are  lagged inputs and  nodes in the\nhidden layer. For example, a NNAR(9,5) model is a neural network with the last nine\nobservations ) used as inputs for forecasting the output , and\nwith five neurons in the hidden layer. A NNAR( ) model is equivalent to an ARIMA(\n) model, but without the restrictions on the parameters to ensure stationarity.\nWith seasonal data, it is useful to also add the last observed values from the same\nseason as inputs. For example, an NNAR(3,1,2)  model has inputs , , \nand , and two neurons in the hidden layer. More generally, an NNAR( )\nmodel has inputs  and  neurons in the\nhidden layer. A NNAR( )  model is equivalent to an ARIMA( )( ,0,0)\nmodel but without the restrictions on the parameters that ensure stationarity.\nThe ınnetar()ı function fits an NNAR( )  model. If the values of  and  are",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 785,
      "total_chunks": 873
    }
  },
  {
    "text": "model but without the restrictions on the parameters that ensure stationarity.\nThe ınnetar()ı function fits an NNAR( )  model. If the values of  and  are\nnot specified, they are selected automatically. For non-seasonal time series, the\ndefault is the optimal number of lags (according to the AIC) for a linear AR( ) model.\nFor seasonal time series, the default values are  and  is chosen from the\noptimal linear model fitted to the seasonally adjusted data. If  is not specified, it is\nset to  (rounded to the nearest integer).\nWhen it comes to forecasting, the network is applied iteratively. For forecasting one\nstep ahead, we simply use the available historical inputs. For forecasting two steps\nahead, we use the one-step forecast as an input, along with the historical data. This\nprocess proceeds until we have computed all the required forecasts.\n445",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 786,
      "total_chunks": 873
    }
  },
  {
    "text": "Example: sunspotsExample: sunspots\nThe surface of the sun contains magnetic regions that appear as dark spots. These\naffect the propagation of radio waves, and so telecommunication companies like to\npredict sunspot activity in order to plan for any future difficulties. Sunspots follow a\ncycle of length between 9 and 14 years. In Figure 11.13, forecasts from an NNAR(10,6)\nare shown for the next 30 years. We have set a Box-Cox transformation with\nılambda=0ı to ensure the forecasts stay positive.\nFigure 11.13: Forecasts from a neural network with ten lagged inputs and one hidden layer\ncontaining six neurons.\nHere, the last 10 observations are used as predictors, and there are 6 neurons in the\nhidden layer. The cyclicity in the data has been modelled well. We can also see the\nasymmetry of the cycles has been captured by the model, where the increasing part\nof the cycle is steeper than the decreasing part of the cycle. This is one difference",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 787,
      "total_chunks": 873
    }
  },
  {
    "text": "asymmetry of the cycles has been captured by the model, where the increasing part\nof the cycle is steeper than the decreasing part of the cycle. This is one difference\nbetween a NNAR model and a linear AR model — while linear AR models can model\ncyclicity, the modelled cycles are always symmetric.\nfit <- nnetar(sunspotarea, lambda=0 )\nautoplot(forecast(fit,h=30))\n446",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 788,
      "total_chunks": 873
    }
  },
  {
    "text": "Prediction intervalsPrediction intervals\nUnlike most of the methods considered in this book, neural networks are not based\non a well-defined stochastic model, and so it is not straightforward to derive\nprediction intervals for the resultant forecasts. However, we can still compute\nprediction intervals using simulation where future sample paths are generated using\nbootstrapped residuals (as described in Section 3.5).\nThe neural network fitted to the sunspot data can be written as\nwhere  is a vector containing lagged values of the\nseries, and  is a neural network with 6 hidden nodes in a single layer. The error\nseries  is assumed to be homoscedastic (and possibly also normally distributed).\nWe can simulate future sample paths of this model iteratively, by randomly\ngenerating a value for , either from a normal distribution, or by resampling from\nthe historical values. So if  is a random draw from the distribution of errors at\ntime , then",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 789,
      "total_chunks": 873
    }
  },
  {
    "text": "generating a value for , either from a normal distribution, or by resampling from\nthe historical values. So if  is a random draw from the distribution of errors at\ntime , then\nis one possible draw from the forecast distribution for . Setting\n, we can then repeat the process to get\nIn this way, we can iteratively simulate a future sample path. By repeatedly\nsimulating sample paths, we build up knowledge of the distribution for all future\nvalues based on the fitted neural network.\nHere is a simulation of 9 possible future sample paths for the sunspot data. Each\nsample path covers the next 30 years after the observed data.\nsim <- ts(matrix(0, nrow=30L, ncol=9L),\n  start=end(sunspotarea)[1L]+1L)\nfor(i in seq(9))\n  sim[,i] <- simulate(fit, nsim=30L)\nautoplot(sunspotarea) +  autolayer(sim)\n447",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 790,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.14: Future sample paths for the annual sunspot data.\nIf we do this a few hundred or thousand times, we can get a good picture of the\nforecast distributions. This is how the ıforecast()ı function produces prediction\nintervals for NNAR models:\nfcast <- forecast(fit, PI=TRUE , h=30)\nautoplot(fcast)\n448\n\n\nFigure 11.15: Forecasts with prediction intervals for the annual sunspot data. Prediction\nintervals are computed using simulated future sample paths.\nBecause it is a little slow, ıPI=FALSEı is the default, so prediction intervals are not\ncomputed unless requested. The ınpathsı argument in ıforecast()ı controls how\nmany simulations are done (default 1000). By default, the errors are drawn from a\nnormal distribution. The ıbootstrapı argument allows the errors to be\n“bootstrapped” (i.e., randomly drawn from the historical errors).\n449",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 791,
      "total_chunks": 873
    }
  },
  {
    "text": "11.411.4  Bootstrapping and baggingBootstrapping and bagging\nBootstrapping time seriesBootstrapping time series\nIn the preceding section, and in Section 3.5, we bootstrap the residuals of a time\nseries in order to simulate future values of a series using a model.\nMore generally, we can generate new time series that are similar to our observed\nseries, using another type of bootstrap.\nFirst, the time series is Box-Cox-transformed, and then decomposed into trend,\nseasonal and remainder components using STL. Then we obtain shuffled versions of\nthe remainder component to get bootstrapped remainder series. Because there may\nbe autocorrelation present in an STL remainder series, we cannot simply use the re-\ndraw procedure that was described in Section 3.5. Instead, we use a “blocked\nbootstrap”, where contiguous sections of the time series are selected at random and\njoined together. These bootstrapped remainder series are added to the trend and",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 792,
      "total_chunks": 873
    }
  },
  {
    "text": "bootstrap”, where contiguous sections of the time series are selected at random and\njoined together. These bootstrapped remainder series are added to the trend and\nseasonal components, and the Box-Cox transformation is reversed to give variations\non the original time series.\nSome examples are shown in Figure 11.16 for the monthly expenditure on retail debit\ncards in Iceland, from January 2000 to August 2013.\nbootseries <- bld.mbb.bootstrap(debitcards, 10) %>%\n  as.data.frame() %>% ts(start=2000, frequency=12)\nautoplot(debitcards) +\n  autolayer(bootseries, colour=TRUE ) +\n  autolayer(debitcards, colour=FALSE ) +\n  ylab(\"Bootstrapped series\") +  guides(colour=\"none\")\n450",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 793,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 11.16: Ten bootstrapped versions of monthly expenditure on retail debit cards in\nIceland.\nThis type of bootstrapping can be useful in two ways. First it helps us to get a better\nmeasure of forecast uncertainty, and second it provides a way of improving our point\nforecasts using “bagging”.\nPrediction intervals from bootstrapped seriesPrediction intervals from bootstrapped series\nAlmost all prediction intervals from time series models are too narrow. This is a\nwell-known phenomenon and arises because they do not account for all sources of\nuncertainty. Hyndman, Koehler, Snyder, & Grose ( 2002) measured the size of the\nproblem by computing the actual coverage percentage of the prediction intervals on\ntest data, and found that for ETS models, nominal 95% intervals may only provide\ncoverage between 71% and 87%. The difference is due to missing sources of\nuncertainty.\nThere are at least four sources of uncertainty in forecasting using time series\nmodels:\n1. The random error term;",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 794,
      "total_chunks": 873
    }
  },
  {
    "text": "uncertainty.\nThere are at least four sources of uncertainty in forecasting using time series\nmodels:\n1. The random error term;\n2. The parameter estimates;\n3. The choice of model for the historical data;\n4. The continuation of the historical data generating process into the future.\n451",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 795,
      "total_chunks": 873
    }
  },
  {
    "text": "When we produce prediction intervals for time series models, we generally only take\ninto account the first of these sources of uncertainty. Even if we ignore the model\nuncertainty and the uncertainty due to changing data generating processes (sources\n3 and 4), and we just try to allow for parameter uncertainty as well as the random\nerror term (sources 1 and 2), there are no algebraic solutions apart from some simple\nspecial cases.\nWe can use bootstrapped time series to go some way towards overcoming this\nproblem. We demonstrate the idea using the ıdebitcardsı data. First, we simulate\nmany time series that are similar to the original data, using the block-bootstrap\ndescribed above.\nFor each of these series, we fit an ETS model and simulate one sample path from that\nmodel. A different ETS model may be selected in each case, although it will most\nlikely select the same model because the series are similar. However, the estimated",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 796,
      "total_chunks": 873
    }
  },
  {
    "text": "model. A different ETS model may be selected in each case, although it will most\nlikely select the same model because the series are similar. However, the estimated\nparameters will be different. Therefore the simulated sample paths will allow for\nmodel uncertainty and parameter uncertainty, as well as the uncertainty associated\nwith the random error term. This is a time-consuming process as there are a large\nnumber of time series to model.\nFinally, we take the means and quantiles of these simulated sample paths to form\npoint forecasts and prediction intervals.\nnsim <- 1000L\nsim <- bld.mbb.bootstrap(debitcards, nsim)\nh <- 36L\nfuture <- matrix(0, nrow=nsim, ncol=h)\nfor(i in seq(nsim))\n  future[i,] <- simulate(ets(sim[[i]]), nsim=h)\nstart <- tsp(debitcards)[2]+1/12\nsimfc <- structure(list(\n    mean = ts(colMeans(future), start=start, frequency=12),\n    lower = ts(apply(future, 2, quantile, prob=0.025),\n               start=start, frequency=12),",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 797,
      "total_chunks": 873
    }
  },
  {
    "text": "simfc <- structure(list(\n    mean = ts(colMeans(future), start=start, frequency=12),\n    lower = ts(apply(future, 2, quantile, prob=0.025),\n               start=start, frequency=12),\n    upper = ts(apply(future, 2, quantile, prob=0.975),\n               start=start, frequency=12),\n    level=95),\n  class=\"forecast\")\n452",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 798,
      "total_chunks": 873
    }
  },
  {
    "text": "These prediction intervals will be larger than those obtained from an ETS model\napplied directly to the original data.\nFigure 11.17: Forecasts of Iceland debit card usage using an ETS model with regular\nprediction intervals along with prediction intervals computed using simulations that\nallow for model and parameter uncertainty.\nBagged ETS forecastsBagged ETS forecasts\nAnother use for these bootstrapped time series is to improve forecast accuracy. If we\nproduce forecasts from each of the additional time series, and average the resulting\nforecasts, we get better forecasts than if we simply forecast the original time series\ndirectly. This is called “bagging” which stands for “bbootstrap aggaggregatingg”.\netsfc <- forecast(ets(debitcards), h=h, level=95)\nautoplot(debitcards) +\n  ggtitle(\"Monthly retail debit card usage in Iceland\" ) +\n  xlab(\"Year\") + ylab(\"million ISK\") +\n  autolayer(simfc, series=\"Simulated\") +\n  autolayer(etsfc, series=\"ETS\" )\n453",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 799,
      "total_chunks": 873
    }
  },
  {
    "text": "We could simply average the simulated future sample paths computed earlier.\nHowever, if our interest is only in improving point forecast accuracy, and not in also\nobtaining improved prediction intervals, then it is quicker to average the point\nforecasts from each series. The speed improvement comes about because we do not\nneed to produce so many simulated series.\nWe will use ıets()ı to forecast each of these series. Figure 11.18 shows ten forecasts\nobtained in this way.\nFigure 11.18: Forecasts of the ten bootstrapped series obtained using ETS models.\nsim <- bld.mbb.bootstrap(debitcards, 10) %>%\n  as.data.frame() %>%\n  ts(frequency=12, start=2000 )\nfc <- purrr::map(as.list(sim),\n           function(x){ forecast(ets(x))[[\"mean\"]]}) %>%\n      as.data.frame() %>%\n      ts(frequency=12, start=start)\nautoplot(debitcards) +\n  autolayer(sim, colour=TRUE) +\n  autolayer(fc, colour=TRUE) +\n  autolayer(debitcards, colour=FALSE ) +\n  ylab(\"Bootstrapped series\") +\n  guides(colour=\"none\")\n454",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 800,
      "total_chunks": 873
    }
  },
  {
    "text": "The average of these forecasts gives the bagged forecasts of the original data. The\nwhole procedure can be handled with the ıbaggedETS()ı function. By default, 100\nbootstrapped series are used, and the length of the blocks used for obtaining\nbootstrapped residuals is set to 24 for monthly data. The resulting forecasts are\nshown in Figure 11.19.\nFigure 11.19: Comparing bagged ETS forecasts (the average of 100 bootstrapped forecast)\nand ETS applied directly to the data.\nIn this case, it makes little difference. Bergmeir, Hyndman, & Benítez ( 2016) show\nthat, on average, bagging gives better forecasts than just applying ıets()ı directly.\nOf course, it is slower because a lot more computation is required.\netsfc <- debitcards %>% ets() %>% forecast(h=36)\nbaggedfc <- debitcards %>% baggedETS() %>% forecast(h=36)\nautoplot(debitcards) +\n  autolayer(baggedfc, series=\"BaggedETS\", PI=FALSE) +\n  autolayer(etsfc, series=\"ETS\" , PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecasts\"))\n455",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 801,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nBergmeir, C., Hyndman, R. J., & Benítez, J. M. (2016). Bagging exponential\nsmoothing methods using STL decomposition and Box-Cox transformation.\nInternational Journal of Forecasting , 32(2), 303–312. [DOI]\nHyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space\nframework for automatic forecasting using exponential smoothing methods.\nInternational Journal of Forecasting , 18(3), 439–454. [DOI]\n456",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 802,
      "total_chunks": 873
    }
  },
  {
    "text": "11.511.5  ExercisesExercises\n1. Use the ıtbats()ı function to model your retail time series.\na. Check the residuals and produce forecasts.\nb. Does this completely automated approach work for these data?\nc. Have you saved any degrees of freedom by using Fourier terms rather than\nseasonal differencing?\n2. Consider the weekly data on US finished motor gasoline products supplied\n(millions of barrels per day) (series ıgasolineı):\na. Fit a TBATS model to these data.\nb. Check the residuals and produce forecasts.\nc. Could you model these data using any of the other methods we have\nconsidered in this book?\n3. Experiment with using ınnetar()ı on your retail data and other data we have\nconsidered in previous chapters.\n457",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 803,
      "total_chunks": 873
    }
  },
  {
    "text": "11.611.6  Further readingFurther reading\nDe Livera et al. (2011 ) introduced the TBATS model and discuss the problem of\ncomplex seasonality in general.\nPfaff (2008) provides a book-length overview of VAR modelling and other\nmultivariate time series models.\nNeural networks for individual time series have not tended to produce good\nforecasts. Crone, Hibon, & Nikolopoulos (2011 ) discuss this issue in the context\nof a forecasting competition.\nBootstrapping for time series is discussed in Lahiri (2003).\nBagging for time series forecasting is relatively new. Bergmeir et al. ( 2016) is one\nof the few papers which addresses this topic.\nBibliographyBibliography\nBergmeir, C., Hyndman, R. J., & Benítez, J. M. (2016). Bagging exponential\nsmoothing methods using STL decomposition and Box-Cox transformation.\nInternational Journal of Forecasting , 32(2), 303–312. [DOI]\nCrone, S. F., Hibon, M., & Nikolopoulos, K. (2011). Advances in forecasting with",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 804,
      "total_chunks": 873
    }
  },
  {
    "text": "International Journal of Forecasting , 32(2), 303–312. [DOI]\nCrone, S. F., Hibon, M., & Nikolopoulos, K. (2011). Advances in forecasting with\nneural networks? Empirical evidence from the NN3 competition on time series\nprediction. International Journal of Forecasting , 27(3), 635–660. [DOI]\nDe Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series\nwith complex seasonal patterns using exponential smoothing. J American\nStatistical Association , 106(496), 1513–1527. [DOI]\nLahiri, S. N. (2003). Resampling methods for dependent data . New York, USA:\nSpringer Science & Business Media. [Amazon]\nPfaff, B. (2008). Analysis of integrated and cointegrated time series with R . New\nYork, USA: Springer Science & Business Media. [Amazon]\n458",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 805,
      "total_chunks": 873
    }
  },
  {
    "text": "Chapter 12Chapter 12  Some practical forecastingSome practical forecasting\nissuesissues\nIn this final chapter, we address many practical issues that arise in forecasting, and\ndiscuss some possible solutions. Several of these sections are adapted from\nHyndsight blog posts.\n459",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 806,
      "total_chunks": 873
    }
  },
  {
    "text": "12.112.1  Weekly, daily and sub-daily dataWeekly, daily and sub-daily data\nWeekly, daily and sub-daily data can be challenging for forecasting, although for\ndifferent reasons.\nWeekly dataWeekly data\nWeekly data is difficult to work with because the seasonal period (the number of\nweeks in a year) is both large and non-integer. The average number of weeks in a\nyear is 52.18. Most of the methods we have considered require the seasonal period to\nbe an integer. Even if we approximate it by 52, most of the methods will not handle\nsuch a large seasonal period efficiently.\nThe simplest approach is to use an STL decomposition along with a non-seasonal\nmethod applied to the seasonally adjusted data (as discussed in Chapter 6). Here is\nan example using weekly data on US finished motor gasoline products supplied (in\nmillions of barrels per day) from February 1991 to May 2005.\ngasoline %>% stlf() %>% autoplot()\n460",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 807,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.1: Forecasts for weekly US gasoline production using an STL decomposition with\nan ETS model for the seasonally adjusted data.\nAn alternative approach is to use a dynamic harmonic regression model, as discussed\nin Section 9.5. In the following example, the number of Fourier terms was selected by\nminimising the AICc. The order of the ARIMA model is also selected by minimising\nthe AICc, although that is done within the ıauto.arima()ı function.\nFigure 12.2: Forecasts for weekly US gasoline production using a dynamic harmonic\nregression model.\nbestfit <- list(aicc=Inf)\nfor(K in seq(25)) {\n  fit <- auto.arima(gasoline, xreg= fourier(gasoline, K=K),\n    seasonal=FALSE)\n  if(fit[[\"aicc\"]] < bestfit[[\"aicc\"]]) {\n    bestfit <- fit\n    bestK <- K\n  }\n}\nfc <- forecast(bestfit,\n  xreg=fourier(gasoline, K=bestK, h=104))\nautoplot(fc)\n461",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 808,
      "total_chunks": 873
    }
  },
  {
    "text": "The fitted model has 18 pairs of Fourier terms and can be written as\nwhere  is an ARIMA(4,1,1) process. Because  is non-stationary, the model is\nactually estimated on the differences of the variables on both sides of this equation.\nThere are 36 parameters to capture the seasonality which is rather a lot, but\napparently required according to the AICc selection. The total number of degrees of\nfreedom is 42 (the other six coming from the 4 AR parameters, 1 MA parameter, and\nthe drift parameter).\nA third approach is the TBATS model introduced in Section 11.1. This was the subject\nof Exercise 2 in Section 11.5. In this example, the forecasts are almost identical to the\nprevious two methods.\nThe STL approach or TBATS model is preferable when the seasonality changes over\ntime. The dynamic harmonic regression approach is preferable if there are covariates\nthat are useful predictors as these can be added as additional regressors.\nDaily and sub-daily dataDaily and sub-daily data",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 809,
      "total_chunks": 873
    }
  },
  {
    "text": "that are useful predictors as these can be added as additional regressors.\nDaily and sub-daily dataDaily and sub-daily data\nDaily and sub-daily data are challenging for a different reason — they often involve\nmultiple seasonal patterns, and so we need to use a method that handles such\ncomplex seasonality.\nOf course, if the time series is relatively short so that only one type of seasonality is\npresent, then it will be possible to use one of the single-seasonal methods we have\ndiscussed in previous chapters (e.g., ETS or a seasonal ARIMA model). But when the\ntime series is long enough so that some of the longer seasonal periods become\napparent, it will be necessary to use STL, dynamic harmonic regression or TBATS, as\ndiscussed in Section 11.1.\nHowever, note that even these models only allow for regular seasonality. Capturing\nseasonality associated with moving events such as Easter, Id, or the Chinese New",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 810,
      "total_chunks": 873
    }
  },
  {
    "text": "discussed in Section 11.1.\nHowever, note that even these models only allow for regular seasonality. Capturing\nseasonality associated with moving events such as Easter, Id, or the Chinese New\nYear is more difficult. Even with monthly data, this can be tricky as the festivals can\nfall in either March or April (for Easter), in January or February (for the Chinese New\nYear), or at any time of the year (for Id).\n462",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 811,
      "total_chunks": 873
    }
  },
  {
    "text": "The best way to deal with moving holiday effects is to use dummy variables.\nHowever, neither STL, ETS nor TBATS models allow for covariates. Amongst the\nmodels discussed in this book (and implemented in the forecastforecast package for R), the\nonly choice is a dynamic regression model, where the predictors include any dummy\nholiday effects (and possibly also the seasonality using Fourier terms).\n463",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 812,
      "total_chunks": 873
    }
  },
  {
    "text": "12.212.2  Time series of countsTime series of counts\nAll of the methods discussed in this book assume that the data have a continuous\nsample space. But often data comes in the form of counts. For example, we may wish\nto forecast the number of customers who enter a store each day. We could have 0, 1,\n2, , customers, but we cannot have 3.45693 customers.\nIn practice, this rarely matters provided our counts are sufficiently large. If the\nminimum number of customers is at least 100, then the difference between a\ncontinuous sample space  and the discrete sample space \nhas no perceivable effect on our forecasts. However, if our data contains small\ncounts , then we need to use forecasting methods that are more\nappropriate for a sample space of non-negative integers.\nSuch models are beyond the scope of this book. However, there is one simple method\nwhich gets used in this context, that we would like to mention. It is “Croston’s",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 813,
      "total_chunks": 873
    }
  },
  {
    "text": "Such models are beyond the scope of this book. However, there is one simple method\nwhich gets used in this context, that we would like to mention. It is “Croston’s\nmethod”, named after its British inventor, John Croston, and first described in\nCroston (1972 ). Actually, this method does not properly deal with the count nature of\nthe data either, but it is used so often, that it is worth knowing about it.\nWith Croston’s method, we construct two new series from our original time series by\nnoting which time periods contain zero values, and which periods contain non-zero\nvalues. Let  be the th non-zero quantity, and let  be the time between  and .\nCroston’s method involves separate simple exponential smoothing forecasts on the\ntwo new series  and . Because the method is usually applied to time series of\ndemand for items,  is often called the “demand” and  the “inter-arrival time”.\nIf  and  are the one-step forecasts of the th demand and inter-",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 814,
      "total_chunks": 873
    }
  },
  {
    "text": "demand for items,  is often called the “demand” and  the “inter-arrival time”.\nIf  and  are the one-step forecasts of the th demand and inter-\narrival time respectively, based on data up to demand , then Croston’s method gives\nThe smoothing parameter  takes values between 0 and 1 and is assumed to be the\nsame for both equations. Let  be the time for the last observed positive observation.\nThen the -step ahead forecast for the demand at time , is given by the ratio\n464",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 815,
      "total_chunks": 873
    }
  },
  {
    "text": "There are no algebraic results allowing us to compute prediction intervals for this\nmethod, because the method does not correspond to any statistical model\n(Shenstone & Hyndman, 2005).\nThe ıcroston()ı function produces forecasts using Croston’s method. It simply uses\n by default, and  is set to be equal to the first observation in each of the\nseries. This is consistent with the way Croston envisaged the method being used.\nExample: lubricant salesExample: lubricant sales\nSeveral years ago, we assisted an oil company with forecasts of monthly lubricant\nsales. One of the time series is shown in the table below. The data contain small\ncounts, with many months registering no sales at all, and only small numbers of\nitems sold in other months.\nYearYear JanJan FebFeb MarMar AprApr MayMay JunJun JulJul AugAug SepSep OctOct NovNov DecDec\n10 2 0 1 0 1 1 0 0 00 2 0\n26 3 0 0 0 0 0 7 00 0 0\n30 0 0 3 10 0 1 0 1 0 0\nThere are 11 non-zero demand values in the series, denoted by . The corresponding",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 816,
      "total_chunks": 873
    }
  },
  {
    "text": "10 2 0 1 0 1 1 0 0 00 2 0\n26 3 0 0 0 0 0 7 00 0 0\n30 0 0 3 10 0 1 0 1 0 0\nThere are 11 non-zero demand values in the series, denoted by . The corresponding\narrival series  are also shown in the following table.\n12 34 5 6 789 1 0 1 1\n21 1 126373 1 1 1\n22 2 5 2 1 68 1 3 2\nApplying Croston’s method gives the demand forecast 2.750 and the arrival forecast\n2.793. So the forecast of the original series is . In\npractice, R does these calculations for you:\nproductC %>% croston() %>% autoplot()\n465",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 817,
      "total_chunks": 873
    }
  },
  {
    "text": "An implementation of Croston’s method with more facilities (including parameter\nestimation) is available in the tsintermittenttsintermittent  package for R.\nForecasting models that deal more directly with the count nature of the data are\ndescribed in Christou & Fokianos ( 2015).\nBibliographyBibliography\nChristou, V., & Fokianos, K. (2015). On count time series prediction. Journal of\nStatistical Computation and Simulation , 85(2), 357–373. [DOI]\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands.\nOperational Research Quarterly , 23(3), 289–303. [DOI]\nShenstone, L., & Hyndman, R. J. (2005). Stochastic models underlying Croston’s\nmethod for intermittent demand forecasting. Journal of Forecasting , 24(6),\n389–402. [DOI]\n466",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 818,
      "total_chunks": 873
    }
  },
  {
    "text": "12.312.3  Ensuring forecasts stay within limitsEnsuring forecasts stay within limits\nIt is common to want forecasts to be positive, or to require them to be within some\nspecified range . Both of these situations are relatively easy to handle using\ntransformations.\nPositive forecastsPositive forecasts\nTo impose a positivity constraint, simply work on the log scale, by specifying the\nBox-Cox parameter . For example, consider the real price of a dozen eggs\n(1900-1993; in cents):\nFigure 12.3: Forecasts for the price of a dozen eggs, constrained to be positive using a\nBox-Cox transformation.\neggs %>%\n  ets(model=\"AAN\", damped=FALSE, lambda=0) %>%\n  forecast(h=50, biasadj=TRUE) %>%\n  autoplot()\n467",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 819,
      "total_chunks": 873
    }
  },
  {
    "text": "Because we set ıbiasadj=TRUEı, the forecasts are the means of the forecast\ndistributions.\nForecasts constrained to an intervalForecasts constrained to an interval\nTo see how to handle data constrained to an interval, imagine that the egg prices\nwere constrained to lie within  and . Then we can transform the data\nusing a scaled logit transform which maps  to the whole real line:\nwhere  is on the original scale and  is the transformed data. To reverse the\ntransformation, we will use\nThis is not a built-in transformation, so we will need to do more work.\n    # Bounds\n    a <- 50\n    b <- 400\n    # Transform data and fit model\n    fit <- log((eggs-a)/(b-eggs)) %>%\n      ets(model=\"AAN\", damped=FALSE)\n    fc <- forecast(fit, h=50)\n    # Back-transform forecasts\n    fc[[\"mean\"]] <- (b- a)*exp(fc[[\"mean\"]]) /\n      (1+exp(fc[[\"mean\"]])) + a\n    fc[[\"lower\"]] <- (b -a)*exp(fc[[\"lower\"]]) /\n     (1+exp(fc[[\"lower\"]])) + a\n    fc[[\"upper\"]] <- (b- a)*exp(fc[[\"upper\"]]) /",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 820,
      "total_chunks": 873
    }
  },
  {
    "text": "(1+exp(fc[[\"mean\"]])) + a\n    fc[[\"lower\"]] <- (b -a)*exp(fc[[\"lower\"]]) /\n     (1+exp(fc[[\"lower\"]])) + a\n    fc[[\"upper\"]] <- (b- a)*exp(fc[[\"upper\"]]) /\n     (1+exp(fc[[\"upper\"]])) + a\n    fc[[\"x\"]] <- eggs\n    # Plot result on original scale\n    autoplot(fc)\n468",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 821,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.4: Forecasts for the price of a dozen eggs, constrained to be lie between 50 and\n400.\nNo bias-adjustment has been used here, so the forecasts are the medians of the\nfuture distributions. The prediction intervals from these transformations have the\nsame coverage probability as on the transformed scale, because quantiles are\npreserved under monotonically increasing transformations.\nThe prediction intervals lie above 50 due to the transformation. As a result of this\nartificial (and unrealistic) constraint, the forecast distributions have become\nextremely skewed.\n469",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 822,
      "total_chunks": 873
    }
  },
  {
    "text": "12.412.4  Forecast combinationsForecast combinations\nAn easy way to improve forecast accuracy is to use several different methods on the\nsame time series, and to average the resulting forecasts. Nearly 50 years ago, John\nBates and Clive Granger wrote a famous paper (Bates & Granger, 1969), showing that\ncombining forecasts often leads to better forecast accuracy. Twenty years later,\nClemen (1989) wrote\nWhile there has been considerable research on using weighted averages, or some\nother more complicated combination approach, using a simple average has proven\nhard to beat.\nHere is an example using monthly expenditure on eating out in Australia, from April\n1982 to September 2017. We use forecasts from the following models: ETS, ARIMA,\nSTL-ETS, NNAR, and TBATS; and we compare the results using the last 5 years (60\nmonths) of observations.\nThe results have been virtually unanimous: combining multiple forecasts leads\nto increased forecast accuracy. In many cases one can make dramatic",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 823,
      "total_chunks": 873
    }
  },
  {
    "text": "months) of observations.\nThe results have been virtually unanimous: combining multiple forecasts leads\nto increased forecast accuracy. In many cases one can make dramatic\nperformance improvements by simply averaging the forecasts.\ntrain <- window(auscafe, end=c(2012,9))\nh <- length(auscafe) - length(train)\nETS <- forecast(ets(train), h=h)\nARIMA <- forecast(auto.arima(train, lambda=0 , biasadj=TRUE),\n  h=h)\nSTL <- stlf(train, lambda=0, h=h, biasadj=TRUE)\nNNAR <- forecast(nnetar(train), h=h)\nTBATS <- forecast(tbats(train, biasadj=TRUE), h=h)\nCombination <- (ETS[[\"mean\"]] +  ARIMA[[\"mean\"]] +\n  STL[[\"mean\"]] + NNAR[[\"mean\"]] + TBATS[[\"mean\"]])/5\n470",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 824,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.5: Point forecasts from various methods applied to Australian monthly\nexpenditure on eating out.\nautoplot(auscafe) +\n  autolayer(ETS, series=\"ETS\" , PI=FALSE) +\n  autolayer(ARIMA, series=\"ARIMA\", PI=FALSE ) +\n  autolayer(STL, series=\"STL\" , PI=FALSE) +\n  autolayer(NNAR, series=\"NNAR\", PI=FALSE ) +\n  autolayer(TBATS, series=\"TBATS\", PI= FALSE) +\n  autolayer(Combination, series=\"Combination\") +\n  xlab(\"Year\") + ylab(\"$ billion\") +\n  ggtitle(\"Australian monthly expenditure on eating out\" )\nc(ETS = accuracy(ETS, auscafe)[\"Test set\", \"RMSE\"],\n  ARIMA = accuracy(ARIMA, auscafe)[\"Test set\", \"RMSE\"],\n  `STL-ETS` = accuracy(STL, auscafe)[\"Test set\", \"RMSE\"],\n  NNAR = accuracy(NNAR, auscafe)[\"Test set\", \"RMSE\"],\n  TBATS = accuracy(TBATS, auscafe)[\"Test set\", \"RMSE\"],\n  Combination =\n    accuracy(Combination, auscafe)[ \"Test set\",\"RMSE\"])\n#>         ETS       ARIMA     STL-ETS        NNAR       TBATS Combinatio\n#>     0.13700     0.15920     0.19310     0.31769     0.09406     0.0716",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 825,
      "total_chunks": 873
    }
  },
  {
    "text": "#>         ETS       ARIMA     STL-ETS        NNAR       TBATS Combinatio\n#>     0.13700     0.15920     0.19310     0.31769     0.09406     0.0716\n471",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 826,
      "total_chunks": 873
    }
  },
  {
    "text": "TBATS does particularly well with this series, but the combination approach is even\nbetter. For other data, TBATS may be quite poor, while the combination approach is\nalmost always close to, or better than, the best component method.\nBibliographyBibliography\nBates, J. M., & Granger, C. W. J. (1969). The combination of forecasts. Operational\nResearch Quarterly , 20(4), 451–468. [DOI]\nClemen, R. (1989). Combining forecasts: A review and annotated bibliography.\nInternational Journal of Forecasting , 5(4), 559–583. [DOI]\n472",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 827,
      "total_chunks": 873
    }
  },
  {
    "text": "12.512.5  Prediction intervals for aggregatesPrediction intervals for aggregates\nA common problem is to forecast the aggregate of several time periods of data, using\na model fitted to the disaggregated data. For example, we may have monthly data but\nwish to forecast the total for the next year. Or we may have weekly data, and want to\nforecast the total for the next four weeks.\nIf the point forecasts are means, then adding them up will give a good estimate of\nthe total. But prediction intervals are more tricky due to the correlations between\nforecast errors.\nA general solution is to use simulations. Here is an example using ETS models\napplied to Australian monthly gas production data, assuming we wish to forecast the\naggregate gas demand in the next six months.\nThe mean of the simulations is close to the sum of the individual forecasts:\nPrediction intervals are also easy to obtain:\n# First fit a model to the data\nfit <- ets(gas/1000)\n# Forecast six months ahead",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 828,
      "total_chunks": 873
    }
  },
  {
    "text": "Prediction intervals are also easy to obtain:\n# First fit a model to the data\nfit <- ets(gas/1000)\n# Forecast six months ahead\nfc <- forecast(fit, h=6)\n# Simulate 10000 future sample paths\nnsim <- 10000\nh <- 6\nsim <- numeric(nsim)\nfor(i in seq_len(nsim))\n  sim[i] <- sum(simulate(fit, future=TRUE , nsim=h))\nmeanagg <- mean(sim)\nsum(fc[[\"mean\"]][1:6])\n#> [1] 281.8\nmeanagg\n#> [1] 281.7\n473",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 829,
      "total_chunks": 873
    }
  },
  {
    "text": "#80% interval:\nquantile(sim, prob=c(0.1, 0.9))\n#> 10% 90% \n#> 263 301\n#95% interval:\nquantile(sim, prob=c(0.025, 0.975))\n#>  2.5% 97.5% \n#> 254.1 311.4\n474",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 830,
      "total_chunks": 873
    }
  },
  {
    "text": "12.612.6  BackcastingBackcasting\nSometimes it is useful to “backcast” a time series — that is, forecast in reverse time.\nAlthough there are no in-built R functions to do this, it is easy to implement. The\nfollowing functions reverse a ıtsı object and a ıforecastı object.\nThen we can apply these functions to backcast any time series. Here is an example\napplied to quarterly retail trade in the Euro area. The data are from 1996 to 2011. We\nbackcast to predict the years 1994-1995.\n# Function to reverse time\nreverse_ts <- function(y)\n{\n  ts(rev(y), start=tsp(y)[1L], frequency=frequency(y))\n}\n# Function to reverse a forecast\nreverse_forecast <- function(object)\n{\n  h <- length(object[[\"mean\"]])\n  f <- frequency(object[[\"mean\"]])\n  object[[\"x\"]] <- reverse_ts(object[[\"x\"]])\n  object[[\"mean\"]] <- ts(rev(object[[\"mean\"]]),\n    end=tsp(object[[\"x\"]])[1L]-1/f, frequency=f)\n  object[[\"lower\"]] <- object[[\"lower\"]][h :1L,]\n  object[[\"upper\"]] <- object[[\"upper\"]][h :1L,]\n  return(object)\n}\n475",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 831,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.6: Backcasts for quarterly retail trade in the Euro area using an ARIMA model.\n# Backcast example\neuretail %>%\n  reverse_ts() %>%\n  auto.arima() %>%\n  forecast() %>%\n  reverse_forecast() -> bc\nautoplot(bc) +\n  ggtitle(paste(\"Backcasts from\",bc[[ \"method\"]]))\n476",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 832,
      "total_chunks": 873
    }
  },
  {
    "text": "12.712.7  Very long and very short time seriesVery long and very short time series\nForecasting very short time seriesForecasting very short time series\nWe often get asked howÖ fewÖdata points can be used to fit a time series model. As\nwith almost all sample size questions, there is no easy answer. It depends on the\nnumber of model parameters to be estimated and the amount of randomness in the\ndata. The sample size required increases with the number of parameters to be\nestimated, and the amount of noise in the data.\nSome textbooks provide rules-of-thumb giving minimum sample sizes for various\ntime series models. These are misleading and unsubstantiated in theory or practice.\nFurther, they ignore the underlying variability of the data and often overlook the\nnumber of parameters to be estimated as well. There is, for example, no justification\nwhatever for the magic number of 30 often given as a minimum for ARIMA",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 833,
      "total_chunks": 873
    }
  },
  {
    "text": "number of parameters to be estimated as well. There is, for example, no justification\nwhatever for the magic number of 30 often given as a minimum for ARIMA\nmodelling. The only theoretical limit is that we need more observations than there\nare parameters in our forecasting model. However, in practice, we usually need\nsubstantially more observations than that.\nIdeally, we would test if our chosen model performs well out-of-sample compared\nto some simpler approaches. However, with short series, there is not enough data to\nallow some observations to be withheld for testing purposes, and even time series\ncross validation can be difficult to apply. The AICc is particularly useful here, because\nit is a proxy for the one-step forecast out-of-sample MSE. Choosing the model with\nthe minimum AICc value allows both the number of parameters and the amount of\nnoise to be taken into account.\nWhat tends to happen with short series is that the AIC suggests simple models",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 834,
      "total_chunks": 873
    }
  },
  {
    "text": "the minimum AICc value allows both the number of parameters and the amount of\nnoise to be taken into account.\nWhat tends to happen with short series is that the AIC suggests simple models\nbecause anything with more than one or two parameters will produce poor forecasts\ndue to the estimation error. ÖWe applied the ıauto.arima()ı function to all the series\nfrom the M-competition with fewer than 20 observations. There were a total of 144\nseries, of which 54 had models with zero parameters (white noise and random\nwalks), 73 had models with one parameter, 15 had models with two parameters, and\n2 series had models with three parameters. Interested readers can carry out the same\nexercise using the following code.\n477",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 835,
      "total_chunks": 873
    }
  },
  {
    "text": "Forecasting very long time seriesForecasting very long time series\nMost time series models do not work well for very long time series. The problem is\nthat real data do not come from the models we use. When the number of\nobservations is not large (say up to about 200) the models often work well as an\napproximation to whatever process generated the data. But eventually we will have\nenough data that the difference between the true process and the model starts to\nbecome more obvious. An additional problem is that the optimisation of the\nparameters becomes more time consuming because of the number of observations\ninvolved.\nWhat to do about these issues depends on the purpose of the model. A more flexible\nand complicated model could be used, but this still assumes that the model structure\nwill work over the whole period of the data. A better approach is usually to allow the\nmodel itself to change over time. ETS models are designed to handle this situation by",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 836,
      "total_chunks": 873
    }
  },
  {
    "text": "will work over the whole period of the data. A better approach is usually to allow the\nmodel itself to change over time. ETS models are designed to handle this situation by\nallowing the trend and seasonal terms to evolve over time. ARIMA models with\ndifferencing have a similar property. But dynamic regression models do not allow\nany evolution of model components.\nIf we are only interested in forecasting the next few observations, one simple\napproach is to throw away the earliest observations and only fit a model to the most\nrecent observations. Then an inflexible model can work well because there is not\nenough time for the relationships to change substantially.\nFor example, we fitted a dynamic harmonic regression model to 26 years of weekly\ngasoline production in Section 12.1. It is, perhaps, unrealistic to assume that the\nseasonal pattern remains the same over nearly three decades. So we could simply fit\nlibrary(Mcomp)\nlibrary(purrr)\nn <- map_int(M1, function(x) {length(x[[\"x\"]])})",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 837,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonal pattern remains the same over nearly three decades. So we could simply fit\nlibrary(Mcomp)\nlibrary(purrr)\nn <- map_int(M1, function(x) {length(x[[\"x\"]])})\nM1[n < 20] %>%\n  map_int(function(u) {\n    u[[\"x\"]] %>%\n      auto.arima() %>%\n      coefficients() %>%\n      length()\n  }) %>%\n  table()\n478",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 838,
      "total_chunks": 873
    }
  },
  {
    "text": "a model to the most recent years instead.\n479",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 839,
      "total_chunks": 873
    }
  },
  {
    "text": "12.812.8  Forecasting on training and test setsForecasting on training and test sets\nTypically, we compute one-step forecasts on the training data (the “fitted values”)\nand multi-step forecasts on the test data. However, occasionally we may wish to\ncompute multi-step forecasts on the training data, or one-step forecasts on the test\ndata.\nMulti-step forecasts on training dataMulti-step forecasts on training data\nWe normally define fitted values to be one-step forecasts on the training set (see\nSection 3.3), but a similar idea can be used for multi-step forecasts. We will illustrate\nthe method using an ARIMA(2,1,1)(0,1,2)  model for the Australian eating-out\nexpenditure. The last five years are used for a test set, and the forecasts are plotted\nin Figure 12.7.\ntraining <- subset(auscafe, end=length(auscafe)-61)\ntest <- subset(auscafe, start=length(auscafe)- 60)\ncafe.train <- Arima(training, order=c(2,1,1),\n  seasonal=c(0,1,2), lambda=0)\ncafe.train %>%\n  forecast(h=60) %>%",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 840,
      "total_chunks": 873
    }
  },
  {
    "text": "test <- subset(auscafe, start=length(auscafe)- 60)\ncafe.train <- Arima(training, order=c(2,1,1),\n  seasonal=c(0,1,2), lambda=0)\ncafe.train %>%\n  forecast(h=60) %>%\n  autoplot() + autolayer(test)\n480",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 841,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.7: Forecasts from an ARIMA model fitted to the Australian café training data.\nThe ıfitted()ı function has an ıhı argument to allow for -step “fitted values” on\nthe training set. Figure 12.8 is a plot of 12-step (one year) forecasts on the training\nset. Because the model involves both seasonal (lag 12) and first (lag 1) differencing, it\nis not possible to compute these forecasts for the first few observations.\n/g24\nautoplot(training, series=\"Training data\") +\n  autolayer(fitted(cafe.train, h=12),\n    series=\"12-step fitted values\" )\n481",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 842,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.8: Twelve-step fitted values from an ARIMA model fitted to the Australian café\ntraining data.\nOne-step forecasts on test dataOne-step forecasts on test data\nIt is common practice to fit a model using training data, and then to evaluate its\nperformance on a test data set. The way this is usually done means the comparisons\non the test data use different forecast horizons. In the above example, we have used\nthe last sixty observations for the test data, and estimated our forecasting model on\nthe training data. Then the forecast errors will be for 1-step, 2-steps, …, 60-steps\nahead. The forecast variance usually increases with the forecast horizon, so if we are\nsimply averaging the absolute or squared errors from the test set, we are combining\nresults with different variances.\nOne solution to this issue is to obtain 1-step errors on the test data. That is, we still\nuse the training data to estimate any parameters, but when we compute forecasts on",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 843,
      "total_chunks": 873
    }
  },
  {
    "text": "One solution to this issue is to obtain 1-step errors on the test data. That is, we still\nuse the training data to estimate any parameters, but when we compute forecasts on\nthe test data, we use all of the data preceding each observation (both training and\ntest data). So our training data are for times . We estimate the model\non these data, but then compute , for . Because the\ntest data are not used to estimate the parameters, this still gives us a “fair” forecast.\nFor the ıets()ı, ıArima()ı, ıtbats()ı and ınnetar()ı functions, these calculations are\neasily carried out using the ımodelı argument.\n482",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 844,
      "total_chunks": 873
    }
  },
  {
    "text": "Using the same ARIMA model used above, we now apply the model to the test data.\nNote that ıArima()ı does not re-estimate in this case. Instead, the model obtained\npreviously (and stored as ıcafe.trainı) is applied to the test data. Because the model\nwas not re-estimated, the “residuals” obtained here are actually one-step forecast\nerrors. Consequently, the results produced from the ıaccuracy()ı command are\nactually on the test set (despite the output saying “Training set”).\ncafe.test <- Arima(test, model=cafe.train)\naccuracy(cafe.test)\n#>                     ME    RMSE     MAE      MPE  MAPE   MASE     ACF1\n#> Training set -0.002622 0.04591 0.03413 -0.07301 1.002 0.1899 -0.05704\n483",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 845,
      "total_chunks": 873
    }
  },
  {
    "text": "12.912.9  Dealing with missing values and outliersDealing with missing values and outliers\nReal data often contains missing values, outlying observations, and other messy\nfeatures. Dealing with them can sometimes be troublesome.\nMissing valuesMissing values\nMissing data can arise for many reasons, and it is worth considering whether the\nmissingness will induce bias in the forecasting model. For example, suppose we are\nstudying sales data for a store, and missing values occur on public holidays when the\nstore is closed. The following day may have increased sales as a result. If we fail to\nallow for this in our forecasting model, we will most likely under-estimate sales on\nthe first day after the public holiday, but over-estimate sales on the days after that.\nOne way to deal with this kind of situation is to use a dynamic regression model, with\ndummy variables indicating if the day is a public holiday or the day after a public",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 846,
      "total_chunks": 873
    }
  },
  {
    "text": "One way to deal with this kind of situation is to use a dynamic regression model, with\ndummy variables indicating if the day is a public holiday or the day after a public\nholiday. No automated method can handle such effects as they depend on the specific\nforecasting context.\nIn other situations, the missingness may be essentially random. For example,\nsomeone may have forgotten to record the sales figures, or the data recording device\nmay have malfunctioned. If the timing of the missing data is not informative for the\nforecasting problem, then the missing values can be handled more easily.\nSome methods allow for missing values without any problems. For example, the\nnaïve forecasting method continues to work, with the most recent non-missing\nvalue providing the forecast for the future time periods. Similarly, the other\nbenchmark methods introduced in Section 3.1 will all produce forecasts when there\nare missing values present in the historical data. The R functions for ARIMA models,",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 847,
      "total_chunks": 873
    }
  },
  {
    "text": "benchmark methods introduced in Section 3.1 will all produce forecasts when there\nare missing values present in the historical data. The R functions for ARIMA models,\ndynamic regression models and NNAR models will also work correctly without\ncausing errors. However, other modelling functions do not handle missing values\nincluding ıets()ı, ıstlf()ı, and ıtbats()ı.\nWhen missing values cause errors, there are at least two ways to handle the problem.\nFirst, we could just take the section of data after the last missing value, assuming\nthere is a long enough series of observations to produce meaningful forecasts.\n484",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 848,
      "total_chunks": 873
    }
  },
  {
    "text": "Alternatively, we could replace the missing values with estimates. The ına.interp()ı\nfunction is designed for this purpose.\nThe ıgoldı data contains daily morning gold prices from 1 January 1985 to 31 March\n1989. This series was provided to us as part of a consulting project; it contains 34\nmissing values as well as one apparently incorrect value. Figure 12.9 shows estimates\nof the missing observations in red.\nFigure 12.9: Daily morning gold prices for 1108 consecutive trading days beginning on 1\nJanuary 1985 and ending on 31 March 1989.\nFor non-seasonal data like this, simple linear interpolation is used to fill in the\nmissing sections. For seasonal data, an STL decomposition is used to estimate the\nseasonal component, and the seasonally adjusted series are linear interpolated. More\nsophisticated missing value interpolation is provided in the imputeTSimputeTS package.\ngold2 <- na.interp(gold)\nautoplot(gold2, series=\"Interpolated\") +\n  autolayer(gold, series=\"Original\") +",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 849,
      "total_chunks": 873
    }
  },
  {
    "text": "sophisticated missing value interpolation is provided in the imputeTSimputeTS package.\ngold2 <- na.interp(gold)\nautoplot(gold2, series=\"Interpolated\") +\n  autolayer(gold, series=\"Original\") +\n  scale_colour_manual (\n    values=c(`Interpolated`=\"red\",`Original`=\"gray\"))\n485",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 850,
      "total_chunks": 873
    }
  },
  {
    "text": "OutliersOutliers\nOutliers are observations that are very different from the majority of the\nobservations in the time series. They may be errors, or they may simply be unusual.\n(See Section 5.3 for a discussion of outliers in a regression context.) All of the\nmethods we have considered in this book will not work well if there are extreme\noutliers in the data. In this case, we may wish to replace them with missing values, or\nwith an estimate that is more consistent with the majority of the data.\nSimply replacing outliers without thinking about why they have occurred is a\ndangerous practice. They may provide useful information about the process that\nproduced the data, and which should be taken into account when forecasting.\nHowever, if we are willing to assume that the outliers are genuinely errors, or that\nthey won’t occur in the forecasting period, then replacing them can make the\nforecasting task easier.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 851,
      "total_chunks": 873
    }
  },
  {
    "text": "However, if we are willing to assume that the outliers are genuinely errors, or that\nthey won’t occur in the forecasting period, then replacing them can make the\nforecasting task easier.\nThe ıtsoutliers()ı function is designed to identify outliers, and to suggest potential\nreplacement values. In the ıgoldı data shown in Figure 12.9 , there is an apparently\noutlier on day 770:\nCloser inspection reveals that the neighbouring observations are close to $100 less\nthan the apparent outlier.\nMost likely, this was a transcription error, and the correct value should have been\n$493.70.\nAnother useful function is ıtsclean()ı which identifies and replaces outliers, and\nalso replaces missing values. Obviously this should be used with some caution, but it\ndoes allow us to use forecasting models that are sensitive to outliers, or which do not\nhandle missing values. For example, we could use the ıets()ı function on the ıgoldı\nseries, after applying ıtsclean()ı.\ntsoutliers(gold)\n#> $index\n#> [1] 770",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 852,
      "total_chunks": 873
    }
  },
  {
    "text": "handle missing values. For example, we could use the ıets()ı function on the ıgoldı\nseries, after applying ıtsclean()ı.\ntsoutliers(gold)\n#> $index\n#> [1] 770\n#> \n#> $replacements\n#> [1] 494.9\ngold[768:772]\n#> [1] 495.00 502.75 593.70 487.05 487.75\n486",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 853,
      "total_chunks": 873
    }
  },
  {
    "text": "Figure 12.10: Forecasts from an ETS model for the gold price data after removing an\noutlier.\nNotice that the outlier and missing values have been replaced with estimates.\ngold %>%\n  tsclean() %>%\n  ets() %>%\n  forecast(h=50) %>%\n  autoplot()\n487\n\n\n12.1012.10  Further readingFurther reading\nSo many diverse topics are discussed in this chapter, that it is not possible to point to\nspecific references on all of them. The last chapter in Ord et al. ( 2017) also covers\n“Forecasting in practice” and discusses other issues that might be of interest to\nreaders.\nBibliographyBibliography\nOrd, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\n488",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 854,
      "total_chunks": 873
    }
  },
  {
    "text": "Appendix: Using RAppendix: Using R\nThis book uses R and is designed to be used with R. R is free, available on almost\nevery operating system, and there are thousands of add-on packages to do almost\nanything you could ever want to do. We recommend you use R with RStudio.\nInstalling R and RStudioInstalling R and RStudio\n1. Download and install R.\n2. Download and install RStudio.\n3. Run RStudio. On the “Packages” tab, click on “Install packages” and install the\npackage fpp2fpp2 (make sure “install dependencies” is checked).\nThat’s it! You should now be ready to go.\nR examples in this bookR examples in this book\nWe provide R code for most examples in shaded boxes like this:\nThese examples assume that you have the fpp2fpp2 package loaded (and that you are\nusing at least v2.3 of the package). So you should use the command ılibrary(fpp2)ı\nbefore you try any examples provided here. (This needs to be done at the start of",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 855,
      "total_chunks": 873
    }
  },
  {
    "text": "using at least v2.3 of the package). So you should use the command ılibrary(fpp2)ı\nbefore you try any examples provided here. (This needs to be done at the start of\nevery R session.) Sometimes we also assume that the R code that appears earlier in\nthe same section of the book has also been run; so it is best to work through the R\ncode in the order provided within each section.\nGetting started with RGetting started with R\nIf you have never previously used R, please work through the first section (chapters\n1-8) of “R for Data Science” by Garrett Grolemund and Hadley Wickham. While this\ndoes not cover time series or forecasting, it will get you used to the basics of the R\nlanguage. The Coursera R Programming  course is also highly recommended.\nautoplot(a10)\nh02 %>% ets() %>% forecast() %>% summary()\n489",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 856,
      "total_chunks": 873
    }
  },
  {
    "text": "You will learn how to use R for forecasting using the exercises in this book.\n490",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 857,
      "total_chunks": 873
    }
  },
  {
    "text": "BibliographyBibliography\nArmstrong, J. S. (1978). Long-range forecasting: From crystal ball to computer .\nJohn Wiley & Sons. [Amazon]\nArmstrong, J. S. (Ed.). (2001). Principles of forecasting: A handbook for\nresearchers and practitioners . Kluwer Academic Publishers. [Amazon]\nAthanasopoulos, G., Ahmed, R. A., & Hyndman, R. J. (2009). Hierarchical forecasts\nfor Australian domestic tourism. International Journal of Forecasting , 25, 146–\n166. [DOI]\nAthanasopoulos, G., & Hyndman, R. J. (2008). Modelling and forecasting\nAustralian domestic tourism. Tourism Management , 29(1), 19–31. [DOI]\nAthanasopoulos, G., Hyndman, R. J., Kourentzes, N., & Petropoulos, F. (2017).\nForecasting with temporal hierarchies. European Journal of Operational\nResearch, 262(1), 60–74. [DOI]\nAthanasopoulos, G., Poskitt, D. S., & Vahid, F. (2012). Two canonical VARMA\nforms: Scalar component models vis-à-vis the echelon form. Econometric\nReviews, 31(1), 60–83. [DOI]",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 858,
      "total_chunks": 873
    }
  },
  {
    "text": "Athanasopoulos, G., Poskitt, D. S., & Vahid, F. (2012). Two canonical VARMA\nforms: Scalar component models vis-à-vis the echelon form. Econometric\nReviews, 31(1), 60–83. [DOI]\nBates, J. M., & Granger, C. W. J. (1969). The combination of forecasts. Operational\nResearch Quarterly , 20(4), 451–468. [DOI]\nBergmeir, C., Hyndman, R. J., & Benítez, J. M. (2016). Bagging exponential\nsmoothing methods using STL decomposition and Box-Cox transformation.\nInternational Journal of Forecasting , 32(2), 303–312. [DOI]\nBergmeir, C., Hyndman, R. J., & Koo, B. (2018). A note on the validity of cross-\nvalidation for evaluating autoregressive time series prediction. Computational\nStatistics and Data Analysis , 120, 70–83. [DOI]\nBickel, P. J., & Doksum, K. A. (1981). An analysis of transformations revisited.\nJournal of the American Statistical Association , 76(374), 296–311.\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 859,
      "total_chunks": 873
    }
  },
  {
    "text": "Journal of the American Statistical Association , 76(374), 296–311.\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the\nRoyal Statistical Society. Series B, Statistical Methodology , 26(2), 211–252.\n[DOI]\nBox, G. E. P., & Jenkins, G. M. (1970). Time series analysis: Forecasting and\ncontrol. San Francisco: Holden-Day.\nBox, G. E. P., Jenkins, G. M., Reinsel, G. C., & Ljung, G. M. (2015). Time series\nanalysis: Forecasting and control  (5th ed). Hoboken, New Jersey: John Wiley &\n496",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 860,
      "total_chunks": 873
    }
  },
  {
    "text": "Sons. [Amazon]\nBrockwell, P. J., & Davis, R. A. (2016). Introduction to time series and forecasting\n(3rd ed). New York, USA: Springer. [Amazon]\nBrown, R. G. (1959). Statistical forecasting for inventory control . McGraw/Hill.\nBuehler, R., Messervey, D., & Griffin, D. (2005). Collaborative planning and\nprediction: Does group discussion affect optimistic biases in time estimation?\nOrganizational Behavior and Human Decision Processes , 97(1), 47–63. [DOI]\nChristou, V., & Fokianos, K. (2015). On count time series prediction. Journal of\nStatistical Computation and Simulation , 85(2), 357–373. [DOI]\nClemen, R. (1989). Combining forecasts: A review and annotated bibliography.\nInternational Journal of Forecasting , 5(4), 559–583. [DOI]\nCleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. J. (1990). STL: A\nseasonal-trend decomposition procedure based on loess. Journal of Official\nStatistics , 6(1), 3–33. http://bit.ly/stl1990",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 861,
      "total_chunks": 873
    }
  },
  {
    "text": "seasonal-trend decomposition procedure based on loess. Journal of Official\nStatistics , 6(1), 3–33. http://bit.ly/stl1990\nCleveland, W. S. (1993). Visualizing data . Hobart Press. [Amazon]\nCrone, S. F., Hibon, M., & Nikolopoulos, K. (2011). Advances in forecasting with\nneural networks? Empirical evidence from the NN3 competition on time series\nprediction. International Journal of Forecasting , 27(3), 635–660. [DOI]\nCroston, J. D. (1972). Forecasting and stock control for intermittent demands.\nOperational Research Quarterly , 23(3), 289–303. [DOI]\nDagum, E. B., & Bianconcini, S. (2016). Seasonal adjustment methods and real\ntime trend-cycle estimation . Springer. [Amazon]\nDe Livera, A. M., Hyndman, R. J., & Snyder, R. D. (2011). Forecasting time series\nwith complex seasonal patterns using exponential smoothing. J American\nStatistical Association , 106(496), 1513–1527. [DOI]\nEroglu, C., & Croxton, K. L. (2010). Biases in judgmental adjustments of statistical",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 862,
      "total_chunks": 873
    }
  },
  {
    "text": "Statistical Association , 106(496), 1513–1527. [DOI]\nEroglu, C., & Croxton, K. L. (2010). Biases in judgmental adjustments of statistical\nforecasts: The role of individual differences. International Journal of\nForecasting , 26(1), 116–133. [DOI]\nFan, S., & Hyndman, R. J. (2012). Short-term load forecasting based on a semi-\nparametric additive model. IEEE Transactions on Power Systems , 27(1), 134–\n141. [DOI]\nFildes, R., & Goodwin, P. (2007a). Against your better judgment? How\norganizations can improve their use of management judgment in forecasting.\nInterfaces , 37(6), 570–576. [DOI]\nFildes, R., & Goodwin, P. (2007b). Good and bad judgment in forecasting: Lessons\nfrom four companies. Foresight: The International Journal of Applied\nForecasting , (8), 5–10.\n497",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 863,
      "total_chunks": 873
    }
  },
  {
    "text": "Franses, P. H., & Legerstee, R. (2013). Do statistical forecasting models for SKU-\nlevel data benefit from including past expert knowledge? International Journal\nof Forecasting , 29(1), 80–87. [DOI]\nGardner, E. S. (1985). Exponential smoothing: The state of the art. Journal of\nForecasting , 4(1), 1–28. [DOI]\nGardner, E. S. (2006). Exponential smoothing: The state of the art — Part II.\nInternational Journal of Forecasting , 22, 637–666. [DOI]\nGardner, E. S., & McKenzie, E. (1985). Forecasting trends in time series.\nManagement Science , 31(10), 1237–1246. [DOI]\nGoodwin, P., & Wright, G. (2009). Decision analysis for management judgment\n(4th ed). Chichester: John Wiley & Sons. [Amazon]\nGreen, K. C., & Armstrong, J. S. (2007). Structured analogies for forecasting.\nInternational Journal of Forecasting , 23(3), 365–376. [DOI]\nGross, C. W., & Sohl, J. E. (1990). Disaggregation methods to expedite product line\nforecasting. Journal of Forecasting , 9, 233–254. [DOI]",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 864,
      "total_chunks": 873
    }
  },
  {
    "text": "Gross, C. W., & Sohl, J. E. (1990). Disaggregation methods to expedite product line\nforecasting. Journal of Forecasting , 9, 233–254. [DOI]\nGroves, R. M., Fowler, F. J., Couper, M. P., Lepkowski, J. M., Singer, E., &\nTourangeau, R. (2009). Survey methodology  (2nd ed). John Wiley & Sons.\n[Amazon]\nHamilton, J. D. (1994). Time series analysis . Princeton University Press,\nPrinceton. [Amazon]\nHarrell, F. E. (2015). Regression modeling strategies: With applications to linear\nmodels, logistic and ordinal regression, and survival analysis  (2nd ed). New\nYork, USA: Springer. [Amazon]\nHarris, R., & Sollis, R. (2003). Applied time series modelling and forecasting .\nChichester, UK: John Wiley & Sons. [Amazon]\nHarvey, N. (2001). Improving judgment in forecasting. In J. S. Armstrong (Ed.),\nPrinciples of forecasting: A handbook for researchers and practitioners  (pp.\n59–80). Boston, MA: Kluwer Academic Publishers. [DOI]\nHolt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 865,
      "total_chunks": 873
    }
  },
  {
    "text": "59–80). Boston, MA: Kluwer Academic Publishers. [DOI]\nHolt, C. C. (1957). Forecasting seasonals and trends by exponentially weighted\naverages (O.N.R. Memorandum No. 52). Carnegie Institute of Technology,\nPittsburgh USA. [DOI]\nHyndman, R. J., Ahmed, R. A., Athanasopoulos, G., & Shang, H. L. (2011). Optimal\ncombination forecasts for hierarchical time series. Computational Statistics\nand Data Analysis , 55(9), 2579–2589. [DOI]\nHyndman, R. J., & Fan, S. (2010). Density forecasting for long-term peak\nelectricity demand. IEEE Transactions on Power Systems, 25(2), 1142–1153.\n498\nHyndman, R. J.\u0001\u0007 Athanasopoulos, G. (20\u00131).\u0001'PSFDBTUJOH\u001b\u00011SJODJQMFT\u0001BOE\u00011SBDUJDF\u0001\t\u0014SE\u0001FE",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 866,
      "total_chunks": 873
    }
  },
  {
    "text": "Hyndman, R. J., & Khandakar, Y. (2008). Automatic time series forecasting: The\nforecast package for R. Journal of Statistical Software , 27(1), 1–22. [DOI]\nHyndman, R. J., & Koehler, A. B. (2006). Another look at measures of forecast\naccuracy. International Journal of Forecasting , 22(4), 679–688. [DOI]\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2008). Forecasting with\nexponential smoothing: The state space approach . Berlin: Springer-Verlag.\nhttp://www.exponentialsmoothing.net\nHyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002). A state space\nframework for automatic forecasting using exponential smoothing methods.\nInternational Journal of Forecasting , 18(3), 439–454. [DOI]\nHyndman, R. J., Lee, A., & Wang, E. (2016). Fast computation of reconciled\nforecasts for hierarchical and grouped time series. Computational Statistics\nand Data Analysis , 97, 16–32. [DOI]\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2014). An introduction to",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 867,
      "total_chunks": 873
    }
  },
  {
    "text": "forecasts for hierarchical and grouped time series. Computational Statistics\nand Data Analysis , 97, 16–32. [DOI]\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2014). An introduction to\nstatistical learning: With applications in R . New York: Springer. [Amazon]\nKahn, K. B. (2006). New product forecasting: An applied approach . M.E. Sharp.\n[Amazon]\nKahneman, D., & Lovallo, D. (1993). Timid choices and bold forecasts: A cognitive\nperspective on risk taking. Management Science , 39(1), 17–31. [DOI]\nKwiatkowski, D., Phillips, P. C. B., Schmidt, P., & Shin, Y. (1992). Testing the null\nhypothesis of stationarity against the alternative of a unit root: How sure are\nwe that economic time series have a unit root? Journal of Econometrics , 54(1-\n3), 159–178. [DOI]\nLahiri, S. N. (2003). Resampling methods for dependent data . New York, USA:\nSpringer Science & Business Media. [Amazon]\nLawrence, M., Goodwin, P., O’Connor, M., & Önkal, D. (2006). Judgmental",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 868,
      "total_chunks": 873
    }
  },
  {
    "text": "Lahiri, S. N. (2003). Resampling methods for dependent data . New York, USA:\nSpringer Science & Business Media. [Amazon]\nLawrence, M., Goodwin, P., O’Connor, M., & Önkal, D. (2006). Judgmental\nforecasting: A review of progress over the last 25 years. International Journal of\nForecasting , 22(3), 493–518. [DOI]\nLütkepohl, H. (2005). New introduction to multiple time series analysis . Berlin:\nSpringer-Verlag. [Amazon]\nLütkepohl, H. (2007). General-to-specific or specific-to-general modelling? An\nopinion on current econometric terminology. Journal of Econometrics , 136(1),\n234–319. [DOI]\nMorwitz, V. G., Steckel, J. H., & Gupta, A. (2007). When do purchase intentions\npredict sales? International Journal of Forecasting , 23(3), 347–364. [DOI]\nÖnkal, D., Sayı m, K. Z., & Gönül, M. S. (2013). Scenarios as channels of forecast\nadvice. Technological Forecasting and Social Change , 80(4), 772–788. [DOI]\n499",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 869,
      "total_chunks": 873
    }
  },
  {
    "text": "Ord, J. K., Fildes, R., & Kourentzes, N. (2017). Principles of business forecasting\n(2nd ed.). Wessex Press Publishing Co. [Amazon]\nPankratz, A. E. (1991). Forecasting with dynamic regression models . New York,\nUSA: John Wiley & Sons. [Amazon]\nPegels, C. C. (1969). Exponential forecasting: Some new variations. Management\nScience, 15(5), 311–315. [DOI]\nPeña, D., Tiao, G. C., & Tsay, R. S. (Eds.). (2001). A course in time series analysis .\nNew York, USA: John Wiley & Sons. [Amazon]\nPfaff, B. (2008). Analysis of integrated and cointegrated time series with R . New\nYork, USA: Springer Science & Business Media. [Amazon]\nRandall, D. M., & Wolff, J. A. (1994). The time interval in the intention-behaviour\nrelationship: Meta-analysis. British Journal of Social Psychology , 33(4), 405–\n418. [DOI]\nRowe, G. (2007). A guide to Delphi. Foresight: The International Journal of\nApplied Forecasting , (8), 11–16.\nRowe, G., & Wright, G. (1999). The Delphi technique as a forecasting tool: Issues",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 870,
      "total_chunks": 873
    }
  },
  {
    "text": "Rowe, G. (2007). A guide to Delphi. Foresight: The International Journal of\nApplied Forecasting , (8), 11–16.\nRowe, G., & Wright, G. (1999). The Delphi technique as a forecasting tool: Issues\nand analysis. International Journal of Forecasting , 15(4), 353–375. [DOI]\nSanders, N., Goodwin, P., Önkal, D., Gönül, M. S., Harvey, N., Lee, A., & Kjolso, L.\n(2005). When and how should statistical forecasts be judgmentally adjusted?\nForesight: The International Journal of Applied Forecasting , 1(1), 5–23.\nhttp://www.forecastpro.com/Trends/pdf/Nada%20Sanders%20Judgmental%2\n0Adjustments%20to%20Statistical%20Forecasts%20July%202008.pdf\nSheather, S. J. (2009). A modern approach to regression with R . New York, USA:\nSpringer. [Amazon]\nShenstone, L., & Hyndman, R. J. (2005). Stochastic models underlying Croston’s\nmethod for intermittent demand forecasting. Journal of Forecasting , 24(6),\n389–402. [DOI]\nTaylor, J. W. (2003). Exponential smoothing with a damped multiplicative trend.",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 871,
      "total_chunks": 873
    }
  },
  {
    "text": "method for intermittent demand forecasting. Journal of Forecasting , 24(6),\n389–402. [DOI]\nTaylor, J. W. (2003). Exponential smoothing with a damped multiplicative trend.\nInternational Journal of Forecasting , 19(4), 715–725. [DOI]\nTheodosiou, M. (2011). Forecasting monthly and quarterly time series using STL\ndecomposition. International Journal of Forecasting , 27(4), 1178–1195. [DOI]\nUnwin, A. (2015). Graphical data analysis with R . Chapman; Hall/CRC. [Amazon]\nWang, X., Smith, K. A., & Hyndman, R. J. (2006). Characteristic-based clustering\nfor time series data. Data Mining and Knowledge Discovery , 13(3), 335–364.\n[DOI]\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis  (2nd ed).\nSpringer. [Amazon]\n500",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 872,
      "total_chunks": 873
    }
  },
  {
    "text": "Wickramasuriya, S. L., Athanasopoulos, G., & Hyndman, R. J. (2019). Optimal\nforecast reconciliation for hierarchical and grouped time series through trace\nminimization. Journal of the American Statistical Association , 114(526), 804–\n819. [DOI]\nWinters, P. R. (1960). Forecasting sales by exponentially weighted moving\naverages. Management Science , 6(3), 324–342. [DOI]\nYoung, P. C., Pedregal, D. J., & Tych, W. (1999). Dynamic harmonic regression.\nJournal of Forecasting , 18, 369–394. [DOI]\n501",
    "metadata": {
      "source": "0061 Zetheta Ebook Forecasting with R language and Excel.pdf",
      "chunk": 873,
      "total_chunks": 873
    }
  }
]